CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 12:04:41
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by philly on 34e58dd0283f
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
07/14/2016 12:34:01: -------------------------------------------------------------------
07/14/2016 12:34:01: Build info: 

07/14/2016 12:34:01: 		Built time: Jul 14 2016 12:04:41
07/14/2016 12:34:01: 		Last modified date: Tue Jul 12 04:28:35 2016
07/14/2016 12:34:01: 		Build type: release
07/14/2016 12:34:01: 		Build target: GPU
07/14/2016 12:34:01: 		With 1bit-SGD: no
07/14/2016 12:34:01: 		Math lib: mkl
07/14/2016 12:34:01: 		CUDA_PATH: /usr/local/cuda-7.5
07/14/2016 12:34:01: 		CUB_PATH: /usr/local/cub-1.4.1
07/14/2016 12:34:01: 		CUDNN_PATH: /usr/local/cudnn-4.0
07/14/2016 12:34:01: 		Build Branch: HEAD
07/14/2016 12:34:01: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
07/14/2016 12:34:01: 		Built by philly on 34e58dd0283f
07/14/2016 12:34:01: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
07/14/2016 12:34:01: -------------------------------------------------------------------
07/14/2016 12:34:02: -------------------------------------------------------------------
07/14/2016 12:34:02: GPU info:

07/14/2016 12:34:02: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:02: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:02: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:02: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:02: -------------------------------------------------------------------

07/14/2016 12:34:02: Running on localhost at 2016/07/14 12:34:02
07/14/2016 12:34:02: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple  OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu  DeviceId=0  timestamping=true



07/14/2016 12:34:02: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/14/2016 12:34:02: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=$RunDir$/models/simple.dnn
deviceId=$DeviceNumber$
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=$RunDir$/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DeviceId=0
timestamping=true

07/14/2016 12:34:02: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/14/2016 12:34:02: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/14/2016 12:34:02: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn
deviceId=-1
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DeviceId=0
timestamping=true

07/14/2016 12:34:02: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/14/2016 12:34:02: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.cntk:command=Simple_Demo:Simple_Demo_Output
configparameters: cntk.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
configparameters: cntk.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:deviceId=0
configparameters: cntk.cntk:DeviceNumber=-1
configparameters: cntk.cntk:modelPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn
configparameters: cntk.cntk:OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
configparameters: cntk.cntk:outputNodeNames=ScaledLogLikelihood
configparameters: cntk.cntk:precision=float
configparameters: cntk.cntk:RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
configparameters: cntk.cntk:Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]

configparameters: cntk.cntk:Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/SimpleOutput    
]

configparameters: cntk.cntk:timestamping=true
configparameters: cntk.cntk:traceLevel=1
07/14/2016 12:34:02: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/14/2016 12:34:02: Commands: Simple_Demo Simple_Demo_Output
07/14/2016 12:34:02: Precision = "float"
07/14/2016 12:34:02: CNTKModelPath: /tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn
07/14/2016 12:34:02: CNTKCommandTrainInfo: Simple_Demo : 50
07/14/2016 12:34:02: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 50

07/14/2016 12:34:02: ##############################################################################
07/14/2016 12:34:02: #                                                                            #
07/14/2016 12:34:02: # Action "train"                                                             #
07/14/2016 12:34:02: #                                                                            #
07/14/2016 12:34:02: ##############################################################################

07/14/2016 12:34:02: CNTKCommandTrainBegin: Simple_Demo
SimpleNetworkBuilder Using GPU 0

07/14/2016 12:34:02: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *]
Validating --> MeanOfFeatures = Mean (features) : [2 x *] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *], [2], [2] -> [2 x *]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *] -> [50 x *]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *], [50 x 1] -> [50 x 1 x *]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *] -> [50 x 1 x *]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *] -> [50 x 1 x *]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *], [50 x 1] -> [50 x 1 x *]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *] -> [50 x 1 x *]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *] -> [2 x 1 x *]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *], [2 x 1] -> [2 x 1 x *]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *], [2 x 1 x *] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *], [2 x 1 x *] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *] -> [2 x 1 x *]
Validating --> Prior = Mean (labels) : [2 x *] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *], [2] -> [2 x 1 x *]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 12:34:02: Created model with 25 nodes on GPU 0.

07/14/2016 12:34:02: Training criterion node(s):
07/14/2016 12:34:02: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax

07/14/2016 12:34:02: Evaluation criterion node(s):

07/14/2016 12:34:02: 	EvalErrorPrediction = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[EvalErrorPrediction Gradient[1]] [InvStdOfFeatures Gradient[2]] [LogOfPrior Gradient[2]] [MVNormalizedFeatures Gradient[2 x *]] [MeanOfFeatures Gradient[2]] [PosteriorProb Gradient[2 x 1 x *]] [PosteriorProb Value[2 x 1 x *]] [Prior Gradient[2]] [ScaledLogLikelihood Gradient[2 x 1 x *]] [features Gradient[2 x *]] [labels Gradient[2 x *]] }
0x2efc4c8: {[features Value[2 x *]] }
0x7fd191664ea8: {[B1 Value[50 x 1]] }
0x7fd1916661d8: {[W2 Value[2 x 50]] }
0x7fd191666b08: {[B2 Value[2 x 1]] }
0x7fd191667ae8: {[labels Value[2 x *]] }
0x7fd191668d28: {[Prior Value[2]] }
0x7fd19166e858: {[EvalErrorPrediction Value[1]] }
0x7fd19166e9b8: {[ScaledLogLikelihood Value[2 x 1 x *]] }
0x7fd19166eb78: {[CrossEntropyWithSoftmax Value[1]] }
0x7fd19166f0f8: {[LogOfPrior Value[2]] }
0x7fd1916702e8: {[MVNormalizedFeatures Value[2 x *]] }
0x7fd1916709e8: {[W0*features Value[50 x *]] }
0x7fd191670f18: {[W0 Gradient[50 x 2]] [W0*features+B0 Value[50 x 1 x *]] }
0x7fd191671078: {[H1 Value[50 x 1 x *]] [W0*features Gradient[50 x *]] }
0x7fd191671238: {[W0*features+B0 Gradient[50 x 1 x *]] [W1*H1 Value[50 x 1 x *]] }
0x7fd1916713f8: {[W1 Gradient[50 x 50]] [W1*H1+B1 Value[50 x 1 x *]] }
0x7fd1916715b8: {[H2 Value[50 x 1 x *]] [W1*H1 Gradient[50 x 1 x *]] }
0x7fd191671778: {[B0 Gradient[50 x 1]] [H1 Gradient[50 x 1 x *]] [W1*H1+B1 Gradient[50 x 1 x *]] [W2*H1 Value[2 x 1 x *]] }
0x7fd191671938: {[HLast Value[2 x 1 x *]] [W2 Gradient[2 x 50]] }
0x7fd191672498: {[CrossEntropyWithSoftmax Gradient[1]] }
0x7fd191672658: {[B1 Gradient[50 x 1]] [H2 Gradient[50 x 1 x *]] [HLast Gradient[2 x 1 x *]] }
0x7fd191672818: {[W2*H1 Gradient[2 x 1 x *]] }
0x7fd1916729d8: {[B2 Gradient[2 x 1]] }
0x7fd193955f08: {[MeanOfFeatures Value[2]] }
0x7fd1939563d8: {[InvStdOfFeatures Value[2]] }
0x7fd193957108: {[W0 Value[50 x 2]] }
0x7fd1939a8e48: {[B0 Value[50 x 1]] }
0x7fd1939ab108: {[W1 Value[50 x 50]] }


07/14/2016 12:34:02: Precomputing --> 3 PreCompute nodes found.

07/14/2016 12:34:02: 	MeanOfFeatures = Mean()
07/14/2016 12:34:02: 	InvStdOfFeatures = InvStdDev()
07/14/2016 12:34:02: 	Prior = Mean()
BlockRandomizer::StartEpoch: epoch 0: frames [0..10000] (first sequence at sample 0), data subset 0 of 1

07/14/2016 12:34:02: Precomputing --> Completed.


07/14/2016 12:34:02: Starting Epoch 1: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 0: frames [0..10000] (first sequence at sample 0), data subset 0 of 1

07/14/2016 12:34:02: Starting minibatch loop.
07/14/2016 12:34:02:  Epoch[ 1 of 50]-Minibatch[   1-  10]: CrossEntropyWithSoftmax = 0.81617508 * 1280; EvalErrorPrediction = 0.51796875 * 1280; time = 0.0078s; samplesPerSecond = 163223.7
07/14/2016 12:34:02:  Epoch[ 1 of 50]-Minibatch[  11-  20]: CrossEntropyWithSoftmax = 0.73377447 * 1280; EvalErrorPrediction = 0.51796875 * 1280; time = 0.0067s; samplesPerSecond = 192048.0
07/14/2016 12:34:02:  Epoch[ 1 of 50]-Minibatch[  21-  30]: CrossEntropyWithSoftmax = 0.71287031 * 1280; EvalErrorPrediction = 0.48906250 * 1280; time = 0.0083s; samplesPerSecond = 155001.2
07/14/2016 12:34:02:  Epoch[ 1 of 50]-Minibatch[  31-  40]: CrossEntropyWithSoftmax = 0.69614067 * 1280; EvalErrorPrediction = 0.48203125 * 1280; time = 0.0064s; samplesPerSecond = 200187.7
07/14/2016 12:34:02:  Epoch[ 1 of 50]-Minibatch[  41-  50]: CrossEntropyWithSoftmax = 0.72144070 * 1280; EvalErrorPrediction = 0.47968750 * 1280; time = 0.0064s; samplesPerSecond = 199345.9
07/14/2016 12:34:02:  Epoch[ 1 of 50]-Minibatch[  51-  60]: CrossEntropyWithSoftmax = 0.74117203 * 1280; EvalErrorPrediction = 0.45781250 * 1280; time = 0.0065s; samplesPerSecond = 196499.8
07/14/2016 12:34:02:  Epoch[ 1 of 50]-Minibatch[  61-  70]: CrossEntropyWithSoftmax = 0.73851471 * 1280; EvalErrorPrediction = 0.45468750 * 1280; time = 0.0067s; samplesPerSecond = 190873.8
07/14/2016 12:34:02: Finished Epoch[ 1 of 50]: [Training] CrossEntropyWithSoftmax = 0.73239038 * 10000; EvalErrorPrediction = 0.48410000 * 10000; totalSamplesSeen = 10000; learningRatePerSample = 0.1; epochTime=0.056878s
07/14/2016 12:34:02: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.1'

07/14/2016 12:34:02: Starting Epoch 2: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 1: frames [10000..20000] (first sequence at sample 10000), data subset 0 of 1

07/14/2016 12:34:02: Starting minibatch loop.
07/14/2016 12:34:02:  Epoch[ 2 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.49843268 * 1280; EvalErrorPrediction = 0.27500000 * 1280; time = 0.0064s; samplesPerSecond = 201352.8
07/14/2016 12:34:02:  Epoch[ 2 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.23827281 * 1280; EvalErrorPrediction = 0.10078125 * 1280; time = 0.0061s; samplesPerSecond = 210284.2
07/14/2016 12:34:02:  Epoch[ 2 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.21230659 * 1280; EvalErrorPrediction = 0.08828125 * 1280; time = 0.0063s; samplesPerSecond = 202307.6
07/14/2016 12:34:02:  Epoch[ 2 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.26566648 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0062s; samplesPerSecond = 205226.9
07/14/2016 12:34:02:  Epoch[ 2 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.23675966 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0062s; samplesPerSecond = 206318.5
07/14/2016 12:34:02:  Epoch[ 2 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.22643671 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0062s; samplesPerSecond = 205887.1
07/14/2016 12:34:02:  Epoch[ 2 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16871567 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0063s; samplesPerSecond = 204505.5
07/14/2016 12:34:02: Finished Epoch[ 2 of 50]: [Training] CrossEntropyWithSoftmax = 0.25624265 * 10000; EvalErrorPrediction = 0.10820000 * 10000; totalSamplesSeen = 20000; learningRatePerSample = 0.1; epochTime=0.050415s
07/14/2016 12:34:02: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.2'

07/14/2016 12:34:02: Starting Epoch 3: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 2: frames [20000..30000] (first sequence at sample 20000), data subset 0 of 1

07/14/2016 12:34:02: Starting minibatch loop.
07/14/2016 12:34:02:  Epoch[ 3 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15800086 * 1280; EvalErrorPrediction = 0.06171875 * 1280; time = 0.0062s; samplesPerSecond = 205194.0
07/14/2016 12:34:02:  Epoch[ 3 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16581289 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0062s; samplesPerSecond = 207961.0
07/14/2016 12:34:02:  Epoch[ 3 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.19208786 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0061s; samplesPerSecond = 209252.9
07/14/2016 12:34:02:  Epoch[ 3 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16155334 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0062s; samplesPerSecond = 208028.6
07/14/2016 12:34:02:  Epoch[ 3 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.20824003 * 1280; EvalErrorPrediction = 0.10546875 * 1280; time = 0.0062s; samplesPerSecond = 208062.4
07/14/2016 12:34:02:  Epoch[ 3 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15917091 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0061s; samplesPerSecond = 209767.3
07/14/2016 12:34:02:  Epoch[ 3 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16028433 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0061s; samplesPerSecond = 210630.2
07/14/2016 12:34:02: Finished Epoch[ 3 of 50]: [Training] CrossEntropyWithSoftmax = 0.17245027 * 10000; EvalErrorPrediction = 0.07720000 * 10000; totalSamplesSeen = 30000; learningRatePerSample = 0.1; epochTime=0.049654s
07/14/2016 12:34:02: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.3'

07/14/2016 12:34:02: Starting Epoch 4: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 3: frames [30000..40000] (first sequence at sample 30000), data subset 0 of 1

07/14/2016 12:34:02: Starting minibatch loop.
07/14/2016 12:34:02:  Epoch[ 4 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.18641645 * 1280; EvalErrorPrediction = 0.08984375 * 1280; time = 0.0063s; samplesPerSecond = 203951.6
07/14/2016 12:34:02:  Epoch[ 4 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17437385 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0060s; samplesPerSecond = 212483.4
07/14/2016 12:34:02:  Epoch[ 4 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14729838 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0061s; samplesPerSecond = 211012.2
07/14/2016 12:34:02:  Epoch[ 4 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16334481 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0061s; samplesPerSecond = 211535.3
07/14/2016 12:34:02:  Epoch[ 4 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14964414 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0067s; samplesPerSecond = 189742.1
07/14/2016 12:34:02:  Epoch[ 4 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.18884563 * 1280; EvalErrorPrediction = 0.09375000 * 1280; time = 0.0060s; samplesPerSecond = 215053.8
07/14/2016 12:34:02:  Epoch[ 4 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16661625 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0060s; samplesPerSecond = 213226.7
07/14/2016 12:34:02: Finished Epoch[ 4 of 50]: [Training] CrossEntropyWithSoftmax = 0.16787476 * 10000; EvalErrorPrediction = 0.07800000 * 10000; totalSamplesSeen = 40000; learningRatePerSample = 0.1; epochTime=0.049597s
07/14/2016 12:34:02: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.4'

07/14/2016 12:34:02: Starting Epoch 5: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 4: frames [40000..50000] (first sequence at sample 40000), data subset 0 of 1

07/14/2016 12:34:02: Starting minibatch loop.
07/14/2016 12:34:02:  Epoch[ 5 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.14795867 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0061s; samplesPerSecond = 210215.1
07/14/2016 12:34:02:  Epoch[ 5 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18887879 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0060s; samplesPerSecond = 212978.4
07/14/2016 12:34:02:  Epoch[ 5 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.19039090 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0061s; samplesPerSecond = 211012.2
07/14/2016 12:34:02:  Epoch[ 5 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16367989 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0060s; samplesPerSecond = 214621.1
07/14/2016 12:34:02:  Epoch[ 5 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14825420 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0060s; samplesPerSecond = 211955.6
07/14/2016 12:34:02:  Epoch[ 5 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17402649 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0060s; samplesPerSecond = 214945.4
07/14/2016 12:34:02:  Epoch[ 5 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18850470 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0060s; samplesPerSecond = 214477.2
07/14/2016 12:34:03: Finished Epoch[ 5 of 50]: [Training] CrossEntropyWithSoftmax = 0.16812375 * 10000; EvalErrorPrediction = 0.07640000 * 10000; totalSamplesSeen = 50000; learningRatePerSample = 0.1; epochTime=0.048584s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.5'

07/14/2016 12:34:03: Starting Epoch 6: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 5: frames [50000..60000] (first sequence at sample 50000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[ 6 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16417043 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0061s; samplesPerSecond = 209321.3
07/14/2016 12:34:03:  Epoch[ 6 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15806546 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0060s; samplesPerSecond = 213404.5
07/14/2016 12:34:03:  Epoch[ 6 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15316274 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0061s; samplesPerSecond = 208775.1
07/14/2016 12:34:03:  Epoch[ 6 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16868696 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0060s; samplesPerSecond = 212377.6
07/14/2016 12:34:03:  Epoch[ 6 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16524391 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0060s; samplesPerSecond = 214477.2
07/14/2016 12:34:03:  Epoch[ 6 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16858387 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0061s; samplesPerSecond = 210318.8
07/14/2016 12:34:03:  Epoch[ 6 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15710440 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0062s; samplesPerSecond = 205589.5
07/14/2016 12:34:03: Finished Epoch[ 6 of 50]: [Training] CrossEntropyWithSoftmax = 0.16169860 * 10000; EvalErrorPrediction = 0.07590000 * 10000; totalSamplesSeen = 60000; learningRatePerSample = 0.1; epochTime=0.049275s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.6'

07/14/2016 12:34:03: Starting Epoch 7: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 6: frames [60000..70000] (first sequence at sample 60000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[ 7 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.18160713 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0067s; samplesPerSecond = 191731.6
07/14/2016 12:34:03:  Epoch[ 7 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.19635506 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0064s; samplesPerSecond = 199937.5
07/14/2016 12:34:03:  Epoch[ 7 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17201478 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0063s; samplesPerSecond = 203013.5
07/14/2016 12:34:03:  Epoch[ 7 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16116476 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0061s; samplesPerSecond = 210422.5
07/14/2016 12:34:03:  Epoch[ 7 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14690967 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0060s; samplesPerSecond = 212730.6
07/14/2016 12:34:03:  Epoch[ 7 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17914429 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0060s; samplesPerSecond = 211745.2
07/14/2016 12:34:03:  Epoch[ 7 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15242701 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0067s; samplesPerSecond = 191588.1
07/14/2016 12:34:03: Finished Epoch[ 7 of 50]: [Training] CrossEntropyWithSoftmax = 0.17052689 * 10000; EvalErrorPrediction = 0.07780000 * 10000; totalSamplesSeen = 70000; learningRatePerSample = 0.1; epochTime=0.050807s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.7'

07/14/2016 12:34:03: Starting Epoch 8: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 7: frames [70000..80000] (first sequence at sample 70000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[ 8 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.19076351 * 1280; EvalErrorPrediction = 0.08906250 * 1280; time = 0.0063s; samplesPerSecond = 203368.3
07/14/2016 12:34:03:  Epoch[ 8 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15753578 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0061s; samplesPerSecond = 210630.2
07/14/2016 12:34:03:  Epoch[ 8 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16156237 * 1280; EvalErrorPrediction = 0.06640625 * 1280; time = 0.0061s; samplesPerSecond = 208639.0
07/14/2016 12:34:03:  Epoch[ 8 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15250692 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0060s; samplesPerSecond = 213618.2
07/14/2016 12:34:03:  Epoch[ 8 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16825066 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0061s; samplesPerSecond = 210630.2
07/14/2016 12:34:03:  Epoch[ 8 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.18632565 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0061s; samplesPerSecond = 211186.3
07/14/2016 12:34:03:  Epoch[ 8 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14869471 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0062s; samplesPerSecond = 207724.8
07/14/2016 12:34:03: Finished Epoch[ 8 of 50]: [Training] CrossEntropyWithSoftmax = 0.16726311 * 10000; EvalErrorPrediction = 0.07660000 * 10000; totalSamplesSeen = 80000; learningRatePerSample = 0.1; epochTime=0.049392s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.8'

07/14/2016 12:34:03: Starting Epoch 9: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 8: frames [80000..90000] (first sequence at sample 80000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[ 9 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16929054 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0062s; samplesPerSecond = 205854.0
07/14/2016 12:34:03:  Epoch[ 9 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17313080 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0061s; samplesPerSecond = 208299.4
07/14/2016 12:34:03:  Epoch[ 9 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16014233 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0061s; samplesPerSecond = 209184.5
07/14/2016 12:34:03:  Epoch[ 9 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15436630 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0061s; samplesPerSecond = 209836.1
07/14/2016 12:34:03:  Epoch[ 9 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.13862023 * 1280; EvalErrorPrediction = 0.05703125 * 1280; time = 0.0061s; samplesPerSecond = 210769.0
07/14/2016 12:34:03:  Epoch[ 9 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17191691 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0060s; samplesPerSecond = 212872.1
07/14/2016 12:34:03:  Epoch[ 9 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14892521 * 1280; EvalErrorPrediction = 0.06406250 * 1280; time = 0.0060s; samplesPerSecond = 212448.1
07/14/2016 12:34:03: Finished Epoch[ 9 of 50]: [Training] CrossEntropyWithSoftmax = 0.16023846 * 10000; EvalErrorPrediction = 0.07450000 * 10000; totalSamplesSeen = 90000; learningRatePerSample = 0.1; epochTime=0.049195s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.9'

07/14/2016 12:34:03: Starting Epoch 10: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 9: frames [90000..100000] (first sequence at sample 90000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[10 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15953543 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0062s; samplesPerSecond = 205062.5
07/14/2016 12:34:03:  Epoch[10 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16755600 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0061s; samplesPerSecond = 211360.6
07/14/2016 12:34:03:  Epoch[10 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14238112 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0061s; samplesPerSecond = 211151.4
07/14/2016 12:34:03:  Epoch[10 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17691965 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0060s; samplesPerSecond = 212096.1
07/14/2016 12:34:03:  Epoch[10 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16566796 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0061s; samplesPerSecond = 209595.5
07/14/2016 12:34:03:  Epoch[10 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16127262 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0060s; samplesPerSecond = 211675.2
07/14/2016 12:34:03:  Epoch[10 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14459457 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0061s; samplesPerSecond = 209321.3
07/14/2016 12:34:03: Finished Epoch[10 of 50]: [Training] CrossEntropyWithSoftmax = 0.15843966 * 10000; EvalErrorPrediction = 0.07470000 * 10000; totalSamplesSeen = 100000; learningRatePerSample = 0.1; epochTime=0.049387s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.10'

07/14/2016 12:34:03: Starting Epoch 11: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 10: frames [100000..110000] (first sequence at sample 100000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[11 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15813996 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0062s; samplesPerSecond = 205953.3
07/14/2016 12:34:03:  Epoch[11 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15605491 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0062s; samplesPerSecond = 205062.5
07/14/2016 12:34:03:  Epoch[11 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15871444 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0064s; samplesPerSecond = 199283.8
07/14/2016 12:34:03:  Epoch[11 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.18369932 * 1280; EvalErrorPrediction = 0.09140625 * 1280; time = 0.0063s; samplesPerSecond = 203400.6
07/14/2016 12:34:03:  Epoch[11 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16956115 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0063s; samplesPerSecond = 204114.2
07/14/2016 12:34:03:  Epoch[11 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16034946 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0064s; samplesPerSecond = 200595.5
07/14/2016 12:34:03:  Epoch[11 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15532827 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0062s; samplesPerSecond = 205754.7
07/14/2016 12:34:03: Finished Epoch[11 of 50]: [Training] CrossEntropyWithSoftmax = 0.16192614 * 10000; EvalErrorPrediction = 0.07640000 * 10000; totalSamplesSeen = 110000; learningRatePerSample = 0.1; epochTime=0.050772s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.11'

07/14/2016 12:34:03: Starting Epoch 12: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 11: frames [110000..120000] (first sequence at sample 110000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[12 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.14863362 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0071s; samplesPerSecond = 179825.8
07/14/2016 12:34:03:  Epoch[12 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16842790 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0066s; samplesPerSecond = 192713.0
07/14/2016 12:34:03:  Epoch[12 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17496700 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0065s; samplesPerSecond = 196499.8
07/14/2016 12:34:03:  Epoch[12 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16903710 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0066s; samplesPerSecond = 195241.0
07/14/2016 12:34:03:  Epoch[12 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.19188952 * 1280; EvalErrorPrediction = 0.08828125 * 1280; time = 0.0066s; samplesPerSecond = 194086.4
07/14/2016 12:34:03:  Epoch[12 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14883680 * 1280; EvalErrorPrediction = 0.06640625 * 1280; time = 0.0066s; samplesPerSecond = 193675.3
07/14/2016 12:34:03:  Epoch[12 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16941414 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0065s; samplesPerSecond = 195628.9
07/14/2016 12:34:03: Finished Epoch[12 of 50]: [Training] CrossEntropyWithSoftmax = 0.16702577 * 10000; EvalErrorPrediction = 0.07700000 * 10000; totalSamplesSeen = 120000; learningRatePerSample = 0.1; epochTime=0.053747s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.12'

07/14/2016 12:34:03: Starting Epoch 13: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 12: frames [120000..130000] (first sequence at sample 120000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[13 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15751876 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0068s; samplesPerSecond = 187600.8
07/14/2016 12:34:03:  Epoch[13 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15110375 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0066s; samplesPerSecond = 193939.4
07/14/2016 12:34:03:  Epoch[13 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16237354 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0066s; samplesPerSecond = 193704.6
07/14/2016 12:34:03:  Epoch[13 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16139441 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0066s; samplesPerSecond = 192945.4
07/14/2016 12:34:03:  Epoch[13 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18865657 * 1280; EvalErrorPrediction = 0.09218750 * 1280; time = 0.0066s; samplesPerSecond = 194440.2
07/14/2016 12:34:03:  Epoch[13 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16029387 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0066s; samplesPerSecond = 193733.9
07/14/2016 12:34:03:  Epoch[13 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18121386 * 1280; EvalErrorPrediction = 0.08906250 * 1280; time = 0.0066s; samplesPerSecond = 193646.0
07/14/2016 12:34:03: Finished Epoch[13 of 50]: [Training] CrossEntropyWithSoftmax = 0.16550040 * 10000; EvalErrorPrediction = 0.07720000 * 10000; totalSamplesSeen = 130000; learningRatePerSample = 0.1; epochTime=0.053577s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.13'

07/14/2016 12:34:03: Starting Epoch 14: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 13: frames [130000..140000] (first sequence at sample 130000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[14 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16697839 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0066s; samplesPerSecond = 192916.4
07/14/2016 12:34:03:  Epoch[14 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16038507 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0064s; samplesPerSecond = 200847.3
07/14/2016 12:34:03:  Epoch[14 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16519830 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0065s; samplesPerSecond = 197469.9
07/14/2016 12:34:03:  Epoch[14 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.14744382 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0064s; samplesPerSecond = 200313.0
07/14/2016 12:34:03:  Epoch[14 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15347919 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0064s; samplesPerSecond = 199781.5
07/14/2016 12:34:03:  Epoch[14 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16583862 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0063s; samplesPerSecond = 203013.5
07/14/2016 12:34:03:  Epoch[14 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16769333 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0063s; samplesPerSecond = 201670.1
07/14/2016 12:34:03: Finished Epoch[14 of 50]: [Training] CrossEntropyWithSoftmax = 0.16058110 * 10000; EvalErrorPrediction = 0.07530000 * 10000; totalSamplesSeen = 140000; learningRatePerSample = 0.1; epochTime=0.051825s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.14'

07/14/2016 12:34:03: Starting Epoch 15: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 14: frames [140000..150000] (first sequence at sample 140000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[15 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17504191 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0067s; samplesPerSecond = 191817.8
07/14/2016 12:34:03:  Epoch[15 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17398829 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0064s; samplesPerSecond = 200407.1
07/14/2016 12:34:03:  Epoch[15 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16190443 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0064s; samplesPerSecond = 201068.2
07/14/2016 12:34:03:  Epoch[15 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17221560 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0073s; samplesPerSecond = 176454.4
07/14/2016 12:34:03:  Epoch[15 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17401280 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0063s; samplesPerSecond = 203174.6
07/14/2016 12:34:03:  Epoch[15 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17121601 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0064s; samplesPerSecond = 201005.0
07/14/2016 12:34:03:  Epoch[15 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17040396 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0064s; samplesPerSecond = 201511.3
07/14/2016 12:34:03: Finished Epoch[15 of 50]: [Training] CrossEntropyWithSoftmax = 0.17091884 * 10000; EvalErrorPrediction = 0.07830000 * 10000; totalSamplesSeen = 150000; learningRatePerSample = 0.1; epochTime=0.052789s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.15'

07/14/2016 12:34:03: Starting Epoch 16: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 15: frames [150000..160000] (first sequence at sample 150000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[16 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.14499781 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0068s; samplesPerSecond = 189377.1
07/14/2016 12:34:03:  Epoch[16 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17145011 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0065s; samplesPerSecond = 196258.8
07/14/2016 12:34:03:  Epoch[16 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16693316 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0064s; samplesPerSecond = 201289.5
07/14/2016 12:34:03:  Epoch[16 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.14426079 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0064s; samplesPerSecond = 199221.8
07/14/2016 12:34:03:  Epoch[16 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15098538 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0064s; samplesPerSecond = 201416.2
07/14/2016 12:34:03:  Epoch[16 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15499597 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0063s; samplesPerSecond = 202371.5
07/14/2016 12:34:03:  Epoch[16 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15641975 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0064s; samplesPerSecond = 201257.9
07/14/2016 12:34:03: Finished Epoch[16 of 50]: [Training] CrossEntropyWithSoftmax = 0.15964780 * 10000; EvalErrorPrediction = 0.07560000 * 10000; totalSamplesSeen = 160000; learningRatePerSample = 0.1; epochTime=0.052009s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.16'

07/14/2016 12:34:03: Starting Epoch 17: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 16: frames [160000..170000] (first sequence at sample 160000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[17 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17199061 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0068s; samplesPerSecond = 188790.6
07/14/2016 12:34:03:  Epoch[17 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16243212 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0063s; samplesPerSecond = 202531.6
07/14/2016 12:34:03:  Epoch[17 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16780829 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0061s; samplesPerSecond = 210249.7
07/14/2016 12:34:03:  Epoch[17 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15655594 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0062s; samplesPerSecond = 206451.6
07/14/2016 12:34:03:  Epoch[17 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15865726 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 208503.0
07/14/2016 12:34:03:  Epoch[17 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14919052 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0061s; samplesPerSecond = 209801.7
07/14/2016 12:34:03:  Epoch[17 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15324907 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 208333.3
07/14/2016 12:34:03: Finished Epoch[17 of 50]: [Training] CrossEntropyWithSoftmax = 0.15978616 * 10000; EvalErrorPrediction = 0.07410000 * 10000; totalSamplesSeen = 170000; learningRatePerSample = 0.1; epochTime=0.050427s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.17'

07/14/2016 12:34:03: Starting Epoch 18: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 17: frames [170000..180000] (first sequence at sample 170000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[18 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16007935 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0063s; samplesPerSecond = 202756.2
07/14/2016 12:34:03:  Epoch[18 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15282840 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0062s; samplesPerSecond = 207792.2
07/14/2016 12:34:03:  Epoch[18 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17086225 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0061s; samplesPerSecond = 210977.4
07/14/2016 12:34:03:  Epoch[18 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15249200 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0061s; samplesPerSecond = 209218.7
07/14/2016 12:34:03:  Epoch[18 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18579068 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0062s; samplesPerSecond = 207556.3
07/14/2016 12:34:03:  Epoch[18 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16916323 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0062s; samplesPerSecond = 206818.5
07/14/2016 12:34:03:  Epoch[18 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15036688 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0062s; samplesPerSecond = 208028.6
07/14/2016 12:34:03: Finished Epoch[18 of 50]: [Training] CrossEntropyWithSoftmax = 0.16232883 * 10000; EvalErrorPrediction = 0.07540000 * 10000; totalSamplesSeen = 180000; learningRatePerSample = 0.1; epochTime=0.049954s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.18'

07/14/2016 12:34:03: Starting Epoch 19: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 18: frames [180000..190000] (first sequence at sample 180000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[19 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.14655702 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0064s; samplesPerSecond = 200438.5
07/14/2016 12:34:03:  Epoch[19 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15965455 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0061s; samplesPerSecond = 209389.8
07/14/2016 12:34:03:  Epoch[19 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16810975 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0061s; samplesPerSecond = 208571.0
07/14/2016 12:34:03:  Epoch[19 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17923827 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0062s; samplesPerSecond = 207927.2
07/14/2016 12:34:03:  Epoch[19 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18138485 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0061s; samplesPerSecond = 208639.0
07/14/2016 12:34:03:  Epoch[19 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.19831581 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0060s; samplesPerSecond = 211815.3
07/14/2016 12:34:03:  Epoch[19 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15901518 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0063s; samplesPerSecond = 204767.2
07/14/2016 12:34:03: Finished Epoch[19 of 50]: [Training] CrossEntropyWithSoftmax = 0.16842527 * 10000; EvalErrorPrediction = 0.07800000 * 10000; totalSamplesSeen = 190000; learningRatePerSample = 0.1; epochTime=0.050074s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.19'

07/14/2016 12:34:03: Starting Epoch 20: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 19: frames [190000..200000] (first sequence at sample 190000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[20 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16536105 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0065s; samplesPerSecond = 195658.8
07/14/2016 12:34:03:  Epoch[20 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18725352 * 1280; EvalErrorPrediction = 0.09609375 * 1280; time = 0.0062s; samplesPerSecond = 206685.0
07/14/2016 12:34:03:  Epoch[20 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17445228 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0085s; samplesPerSecond = 149882.9
07/14/2016 12:34:03:  Epoch[20 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15546021 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0062s; samplesPerSecond = 207522.7
07/14/2016 12:34:03:  Epoch[20 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16204348 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0060s; samplesPerSecond = 214477.2
07/14/2016 12:34:03:  Epoch[20 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.18671112 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0060s; samplesPerSecond = 212061.0
07/14/2016 12:34:03:  Epoch[20 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16987848 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0060s; samplesPerSecond = 213760.9
07/14/2016 12:34:03: Finished Epoch[20 of 50]: [Training] CrossEntropyWithSoftmax = 0.16896514 * 10000; EvalErrorPrediction = 0.07840000 * 10000; totalSamplesSeen = 200000; learningRatePerSample = 0.1; epochTime=0.052576s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.20'

07/14/2016 12:34:03: Starting Epoch 21: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 20: frames [200000..210000] (first sequence at sample 200000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[21 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16469028 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0063s; samplesPerSecond = 202084.0
07/14/2016 12:34:03:  Epoch[21 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15381479 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0060s; samplesPerSecond = 212307.2
07/14/2016 12:34:03:  Epoch[21 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15882838 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0060s; samplesPerSecond = 211990.7
07/14/2016 12:34:03:  Epoch[21 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16333494 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0061s; samplesPerSecond = 208945.5
07/14/2016 12:34:03:  Epoch[21 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17477932 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0061s; samplesPerSecond = 211256.0
07/14/2016 12:34:03:  Epoch[21 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15844307 * 1280; EvalErrorPrediction = 0.06250000 * 1280; time = 0.0060s; samplesPerSecond = 212942.9
07/14/2016 12:34:03:  Epoch[21 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18186140 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0060s; samplesPerSecond = 212766.0
07/14/2016 12:34:03: Finished Epoch[21 of 50]: [Training] CrossEntropyWithSoftmax = 0.16533986 * 10000; EvalErrorPrediction = 0.07570000 * 10000; totalSamplesSeen = 210000; learningRatePerSample = 0.1; epochTime=0.049164s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.21'

07/14/2016 12:34:03: Starting Epoch 22: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 21: frames [210000..220000] (first sequence at sample 210000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[22 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.20349460 * 1280; EvalErrorPrediction = 0.09765625 * 1280; time = 0.0061s; samplesPerSecond = 208299.4
07/14/2016 12:34:03:  Epoch[22 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17329526 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0060s; samplesPerSecond = 212096.1
07/14/2016 12:34:03:  Epoch[22 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16117549 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0060s; samplesPerSecond = 212766.0
07/14/2016 12:34:03:  Epoch[22 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15503263 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0060s; samplesPerSecond = 213368.9
07/14/2016 12:34:03:  Epoch[22 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16139951 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0060s; samplesPerSecond = 213368.9
07/14/2016 12:34:03:  Epoch[22 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16247711 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0061s; samplesPerSecond = 210560.9
07/14/2016 12:34:03:  Epoch[22 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17946978 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0060s; samplesPerSecond = 212624.6
07/14/2016 12:34:03: Finished Epoch[22 of 50]: [Training] CrossEntropyWithSoftmax = 0.16675659 * 10000; EvalErrorPrediction = 0.07770000 * 10000; totalSamplesSeen = 220000; learningRatePerSample = 0.1; epochTime=0.04887s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.22'

07/14/2016 12:34:03: Starting Epoch 23: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 22: frames [220000..230000] (first sequence at sample 220000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[23 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17159755 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0063s; samplesPerSecond = 204767.2
07/14/2016 12:34:03:  Epoch[23 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17114067 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0060s; samplesPerSecond = 213475.7
07/14/2016 12:34:03:  Epoch[23 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15244133 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0061s; samplesPerSecond = 210215.1
07/14/2016 12:34:03:  Epoch[23 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15626545 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 209252.9
07/14/2016 12:34:03:  Epoch[23 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16120472 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0061s; samplesPerSecond = 211151.4
07/14/2016 12:34:03:  Epoch[23 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14013500 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0060s; samplesPerSecond = 211745.2
07/14/2016 12:34:03:  Epoch[23 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16929522 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 210249.7
07/14/2016 12:34:03: Finished Epoch[23 of 50]: [Training] CrossEntropyWithSoftmax = 0.15960048 * 10000; EvalErrorPrediction = 0.07490000 * 10000; totalSamplesSeen = 230000; learningRatePerSample = 0.1; epochTime=0.049237s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.23'

07/14/2016 12:34:03: Starting Epoch 24: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 23: frames [230000..240000] (first sequence at sample 230000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[24 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17182534 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0063s; samplesPerSecond = 202052.1
07/14/2016 12:34:03:  Epoch[24 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15354176 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0062s; samplesPerSecond = 208130.1
07/14/2016 12:34:03:  Epoch[24 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16197512 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0063s; samplesPerSecond = 204472.8
07/14/2016 12:34:03:  Epoch[24 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16589231 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0063s; samplesPerSecond = 202307.6
07/14/2016 12:34:03:  Epoch[24 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16722522 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0064s; samplesPerSecond = 200375.7
07/14/2016 12:34:03:  Epoch[24 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17564621 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0062s; samplesPerSecond = 205622.5
07/14/2016 12:34:03:  Epoch[24 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16375513 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0063s; samplesPerSecond = 203497.6
07/14/2016 12:34:03: Finished Epoch[24 of 50]: [Training] CrossEntropyWithSoftmax = 0.16714675 * 10000; EvalErrorPrediction = 0.07680000 * 10000; totalSamplesSeen = 240000; learningRatePerSample = 0.1; epochTime=0.050773s
07/14/2016 12:34:03: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.24'

07/14/2016 12:34:03: Starting Epoch 25: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 24: frames [240000..250000] (first sequence at sample 240000), data subset 0 of 1

07/14/2016 12:34:03: Starting minibatch loop.
07/14/2016 12:34:03:  Epoch[25 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17995143 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0072s; samplesPerSecond = 177236.2
07/14/2016 12:34:03:  Epoch[25 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.18855677 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0064s; samplesPerSecond = 200878.8
07/14/2016 12:34:04:  Epoch[25 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16692853 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0064s; samplesPerSecond = 199843.9
07/14/2016 12:34:04:  Epoch[25 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17160010 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0065s; samplesPerSecond = 198111.7
07/14/2016 12:34:04:  Epoch[25 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15487237 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0064s; samplesPerSecond = 199283.8
07/14/2016 12:34:04:  Epoch[25 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14489584 * 1280; EvalErrorPrediction = 0.06406250 * 1280; time = 0.0063s; samplesPerSecond = 201606.6
07/14/2016 12:34:04:  Epoch[25 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16311197 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0064s; samplesPerSecond = 200093.8
07/14/2016 12:34:04: Finished Epoch[25 of 50]: [Training] CrossEntropyWithSoftmax = 0.16767448 * 10000; EvalErrorPrediction = 0.07770000 * 10000; totalSamplesSeen = 250000; learningRatePerSample = 0.1; epochTime=0.052581s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.25'

07/14/2016 12:34:04: Starting Epoch 26: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 25: frames [250000..260000] (first sequence at sample 250000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[26 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16878678 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0069s; samplesPerSecond = 184278.7
07/14/2016 12:34:04:  Epoch[26 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15142335 * 1280; EvalErrorPrediction = 0.06640625 * 1280; time = 0.0065s; samplesPerSecond = 198295.9
07/14/2016 12:34:04:  Epoch[26 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15124254 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0064s; samplesPerSecond = 201036.6
07/14/2016 12:34:04:  Epoch[26 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15940919 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0063s; samplesPerSecond = 203206.9
07/14/2016 12:34:04:  Epoch[26 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17065420 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0064s; samplesPerSecond = 200752.8
07/14/2016 12:34:04:  Epoch[26 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14535627 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0063s; samplesPerSecond = 201988.3
07/14/2016 12:34:04:  Epoch[26 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17644186 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0063s; samplesPerSecond = 203465.3
07/14/2016 12:34:04: Finished Epoch[26 of 50]: [Training] CrossEntropyWithSoftmax = 0.15937396 * 10000; EvalErrorPrediction = 0.07440000 * 10000; totalSamplesSeen = 260000; learningRatePerSample = 0.1; epochTime=0.051971s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.26'

07/14/2016 12:34:04: Starting Epoch 27: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 26: frames [260000..270000] (first sequence at sample 260000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[27 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15709606 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0068s; samplesPerSecond = 187134.5
07/14/2016 12:34:04:  Epoch[27 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16543273 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0064s; samplesPerSecond = 199159.8
07/14/2016 12:34:04:  Epoch[27 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15277243 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0064s; samplesPerSecond = 199190.8
07/14/2016 12:34:04:  Epoch[27 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16587291 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0063s; samplesPerSecond = 201574.8
07/14/2016 12:34:04:  Epoch[27 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15222979 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0064s; samplesPerSecond = 199843.9
07/14/2016 12:34:04:  Epoch[27 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15051155 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0063s; samplesPerSecond = 202084.0
07/14/2016 12:34:04:  Epoch[27 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18173809 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0064s; samplesPerSecond = 198542.0
07/14/2016 12:34:04: Finished Epoch[27 of 50]: [Training] CrossEntropyWithSoftmax = 0.15830378 * 10000; EvalErrorPrediction = 0.07530000 * 10000; totalSamplesSeen = 270000; learningRatePerSample = 0.1; epochTime=0.05215s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.27'

07/14/2016 12:34:04: Starting Epoch 28: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 27: frames [270000..280000] (first sequence at sample 270000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[28 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16907721 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0069s; samplesPerSecond = 184384.9
07/14/2016 12:34:04:  Epoch[28 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14707675 * 1280; EvalErrorPrediction = 0.06328125 * 1280; time = 0.0087s; samplesPerSecond = 147840.1
07/14/2016 12:34:04:  Epoch[28 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14986830 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0069s; samplesPerSecond = 186425.9
07/14/2016 12:34:04:  Epoch[28 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16217022 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0065s; samplesPerSecond = 195928.4
07/14/2016 12:34:04:  Epoch[28 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18148379 * 1280; EvalErrorPrediction = 0.08906250 * 1280; time = 0.0064s; samplesPerSecond = 200313.0
07/14/2016 12:34:04:  Epoch[28 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15600395 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0063s; samplesPerSecond = 203951.6
07/14/2016 12:34:04:  Epoch[28 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16555967 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0063s; samplesPerSecond = 203271.4
07/14/2016 12:34:04: Finished Epoch[28 of 50]: [Training] CrossEntropyWithSoftmax = 0.16259102 * 10000; EvalErrorPrediction = 0.07560000 * 10000; totalSamplesSeen = 280000; learningRatePerSample = 0.1; epochTime=0.055145s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.28'

07/14/2016 12:34:04: Starting Epoch 29: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 28: frames [280000..290000] (first sequence at sample 280000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[29 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16562409 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0068s; samplesPerSecond = 189405.1
07/14/2016 12:34:04:  Epoch[29 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15941446 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0063s; samplesPerSecond = 201765.4
07/14/2016 12:34:04:  Epoch[29 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16366003 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0061s; samplesPerSecond = 210699.6
07/14/2016 12:34:04:  Epoch[29 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15629354 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0062s; samplesPerSecond = 206918.8
07/14/2016 12:34:04:  Epoch[29 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14837217 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0062s; samplesPerSecond = 206918.8
07/14/2016 12:34:04:  Epoch[29 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15770688 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0061s; samplesPerSecond = 209355.6
07/14/2016 12:34:04:  Epoch[29 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15916605 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0061s; samplesPerSecond = 209870.5
07/14/2016 12:34:04: Finished Epoch[29 of 50]: [Training] CrossEntropyWithSoftmax = 0.15779647 * 10000; EvalErrorPrediction = 0.07340000 * 10000; totalSamplesSeen = 290000; learningRatePerSample = 0.1; epochTime=0.050526s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.29'

07/14/2016 12:34:04: Starting Epoch 30: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 29: frames [290000..300000] (first sequence at sample 290000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[30 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.14879410 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0065s; samplesPerSecond = 196983.7
07/14/2016 12:34:04:  Epoch[30 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15640090 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0061s; samplesPerSecond = 208605.0
07/14/2016 12:34:04:  Epoch[30 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.18904819 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0062s; samplesPerSecond = 205986.5
07/14/2016 12:34:04:  Epoch[30 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.14199157 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0062s; samplesPerSecond = 206451.6
07/14/2016 12:34:04:  Epoch[30 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14786630 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0061s; samplesPerSecond = 208163.9
07/14/2016 12:34:04:  Epoch[30 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16597714 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0062s; samplesPerSecond = 207893.5
07/14/2016 12:34:04:  Epoch[30 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16080675 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0061s; samplesPerSecond = 209836.1
07/14/2016 12:34:04: Finished Epoch[30 of 50]: [Training] CrossEntropyWithSoftmax = 0.15906808 * 10000; EvalErrorPrediction = 0.07570000 * 10000; totalSamplesSeen = 300000; learningRatePerSample = 0.1; epochTime=0.050122s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.30'

07/14/2016 12:34:04: Starting Epoch 31: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 30: frames [300000..310000] (first sequence at sample 300000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[31 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16275744 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0064s; samplesPerSecond = 199408.0
07/14/2016 12:34:04:  Epoch[31 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17017746 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0062s; samplesPerSecond = 207354.6
07/14/2016 12:34:04:  Epoch[31 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14321699 * 1280; EvalErrorPrediction = 0.06328125 * 1280; time = 0.0061s; samplesPerSecond = 210595.6
07/14/2016 12:34:04:  Epoch[31 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15120192 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0062s; samplesPerSecond = 207220.3
07/14/2016 12:34:04:  Epoch[31 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15285535 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0061s; samplesPerSecond = 209184.5
07/14/2016 12:34:04:  Epoch[31 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.13534746 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0063s; samplesPerSecond = 204538.2
07/14/2016 12:34:04:  Epoch[31 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17624168 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0062s; samplesPerSecond = 206185.6
07/14/2016 12:34:04: Finished Epoch[31 of 50]: [Training] CrossEntropyWithSoftmax = 0.15980447 * 10000; EvalErrorPrediction = 0.07370000 * 10000; totalSamplesSeen = 310000; learningRatePerSample = 0.1; epochTime=0.050077s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.31'

07/14/2016 12:34:04: Starting Epoch 32: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 31: frames [310000..320000] (first sequence at sample 310000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[32 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16198599 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0063s; samplesPerSecond = 204767.2
07/14/2016 12:34:04:  Epoch[32 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16215127 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0060s; samplesPerSecond = 212342.4
07/14/2016 12:34:04:  Epoch[32 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.13372493 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0062s; samplesPerSecond = 207792.2
07/14/2016 12:34:04:  Epoch[32 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16426897 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0062s; samplesPerSecond = 207792.2
07/14/2016 12:34:04:  Epoch[32 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16832576 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 210630.2
07/14/2016 12:34:04:  Epoch[32 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16835799 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0061s; samplesPerSecond = 208707.0
07/14/2016 12:34:04:  Epoch[32 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14903240 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0061s; samplesPerSecond = 208197.8
07/14/2016 12:34:04: Finished Epoch[32 of 50]: [Training] CrossEntropyWithSoftmax = 0.16206068 * 10000; EvalErrorPrediction = 0.07470000 * 10000; totalSamplesSeen = 320000; learningRatePerSample = 0.1; epochTime=0.04981s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.32'

07/14/2016 12:34:04: Starting Epoch 33: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 32: frames [320000..330000] (first sequence at sample 320000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[33 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17260238 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0068s; samplesPerSecond = 186970.5
07/14/2016 12:34:04:  Epoch[33 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15036043 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0065s; samplesPerSecond = 196530.0
07/14/2016 12:34:04:  Epoch[33 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.14416060 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0065s; samplesPerSecond = 197652.9
07/14/2016 12:34:04:  Epoch[33 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16257315 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0063s; samplesPerSecond = 204081.6
07/14/2016 12:34:04:  Epoch[33 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16877375 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0061s; samplesPerSecond = 208741.0
07/14/2016 12:34:04:  Epoch[33 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15273428 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0062s; samplesPerSecond = 206351.8
07/14/2016 12:34:04:  Epoch[33 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17888365 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0062s; samplesPerSecond = 207489.1
07/14/2016 12:34:04: Finished Epoch[33 of 50]: [Training] CrossEntropyWithSoftmax = 0.16180492 * 10000; EvalErrorPrediction = 0.07540000 * 10000; totalSamplesSeen = 330000; learningRatePerSample = 0.1; epochTime=0.051265s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.33'

07/14/2016 12:34:04: Starting Epoch 34: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 33: frames [330000..340000] (first sequence at sample 330000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[34 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17212042 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0073s; samplesPerSecond = 176235.7
07/14/2016 12:34:04:  Epoch[34 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16251820 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0065s; samplesPerSecond = 198111.7
07/14/2016 12:34:04:  Epoch[34 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16373565 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0064s; samplesPerSecond = 201416.2
07/14/2016 12:34:04:  Epoch[34 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15235853 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0064s; samplesPerSecond = 199439.1
07/14/2016 12:34:04:  Epoch[34 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14773169 * 1280; EvalErrorPrediction = 0.06484375 * 1280; time = 0.0063s; samplesPerSecond = 202115.9
07/14/2016 12:34:04:  Epoch[34 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15427608 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0064s; samplesPerSecond = 199345.9
07/14/2016 12:34:04:  Epoch[34 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15895214 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0064s; samplesPerSecond = 200313.0
07/14/2016 12:34:04: Finished Epoch[34 of 50]: [Training] CrossEntropyWithSoftmax = 0.15999507 * 10000; EvalErrorPrediction = 0.07510000 * 10000; totalSamplesSeen = 340000; learningRatePerSample = 0.1; epochTime=0.05266s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.34'

07/14/2016 12:34:04: Starting Epoch 35: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 34: frames [340000..350000] (first sequence at sample 340000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[35 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16768730 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0068s; samplesPerSecond = 189013.6
07/14/2016 12:34:04:  Epoch[35 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15872076 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0064s; samplesPerSecond = 201479.6
07/14/2016 12:34:04:  Epoch[35 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16511321 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0061s; samplesPerSecond = 208877.3
07/14/2016 12:34:04:  Epoch[35 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15483818 * 1280; EvalErrorPrediction = 0.07109375 * 1280; time = 0.0061s; samplesPerSecond = 209355.6
07/14/2016 12:34:04:  Epoch[35 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14570861 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0061s; samplesPerSecond = 208469.1
07/14/2016 12:34:04:  Epoch[35 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15019670 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 208401.2
07/14/2016 12:34:04:  Epoch[35 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18149843 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0061s; samplesPerSecond = 209150.3
07/14/2016 12:34:04: Finished Epoch[35 of 50]: [Training] CrossEntropyWithSoftmax = 0.16064935 * 10000; EvalErrorPrediction = 0.07610000 * 10000; totalSamplesSeen = 350000; learningRatePerSample = 0.1; epochTime=0.050423s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.35'

07/14/2016 12:34:04: Starting Epoch 36: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 35: frames [350000..360000] (first sequence at sample 350000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[36 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17114042 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0063s; samplesPerSecond = 202531.6
07/14/2016 12:34:04:  Epoch[36 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15853363 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0061s; samplesPerSecond = 208231.7
07/14/2016 12:34:04:  Epoch[36 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16492014 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0062s; samplesPerSecond = 205226.9
07/14/2016 12:34:04:  Epoch[36 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16080680 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 208537.0
07/14/2016 12:34:04:  Epoch[36 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15284743 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0061s; samplesPerSecond = 208605.0
07/14/2016 12:34:04:  Epoch[36 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15469093 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0061s; samplesPerSecond = 208741.0
07/14/2016 12:34:04:  Epoch[36 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18092909 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0061s; samplesPerSecond = 208911.4
07/14/2016 12:34:04: Finished Epoch[36 of 50]: [Training] CrossEntropyWithSoftmax = 0.16307070 * 10000; EvalErrorPrediction = 0.07690000 * 10000; totalSamplesSeen = 360000; learningRatePerSample = 0.1; epochTime=0.04992s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.36'

07/14/2016 12:34:04: Starting Epoch 37: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 36: frames [360000..370000] (first sequence at sample 360000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[37 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15121933 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0064s; samplesPerSecond = 199470.2
07/14/2016 12:34:04:  Epoch[37 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15874637 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0063s; samplesPerSecond = 203789.2
07/14/2016 12:34:04:  Epoch[37 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17776804 * 1280; EvalErrorPrediction = 0.08671875 * 1280; time = 0.0063s; samplesPerSecond = 201638.3
07/14/2016 12:34:04:  Epoch[37 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15243282 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0063s; samplesPerSecond = 203659.5
07/14/2016 12:34:04:  Epoch[37 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15466056 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0063s; samplesPerSecond = 202788.3
07/14/2016 12:34:04:  Epoch[37 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14456706 * 1280; EvalErrorPrediction = 0.06796875 * 1280; time = 0.0064s; samplesPerSecond = 200752.8
07/14/2016 12:34:04:  Epoch[37 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16275949 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0062s; samplesPerSecond = 205820.9
07/14/2016 12:34:04: Finished Epoch[37 of 50]: [Training] CrossEntropyWithSoftmax = 0.15827069 * 10000; EvalErrorPrediction = 0.07470000 * 10000; totalSamplesSeen = 370000; learningRatePerSample = 0.1; epochTime=0.051004s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.37'

07/14/2016 12:34:04: Starting Epoch 38: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 37: frames [370000..380000] (first sequence at sample 370000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[38 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.16772083 * 1280; EvalErrorPrediction = 0.07968750 * 1280; time = 0.0064s; samplesPerSecond = 200093.8
07/14/2016 12:34:04:  Epoch[38 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15543102 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0062s; samplesPerSecond = 207086.2
07/14/2016 12:34:04:  Epoch[38 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15747359 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0062s; samplesPerSecond = 206584.9
07/14/2016 12:34:04:  Epoch[38 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.18036132 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0061s; samplesPerSecond = 208741.0
07/14/2016 12:34:04:  Epoch[38 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16553798 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0061s; samplesPerSecond = 209321.3
07/14/2016 12:34:04:  Epoch[38 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16498775 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0062s; samplesPerSecond = 208062.4
07/14/2016 12:34:04:  Epoch[38 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14760447 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0062s; samplesPerSecond = 208028.6
07/14/2016 12:34:04: Finished Epoch[38 of 50]: [Training] CrossEntropyWithSoftmax = 0.16437140 * 10000; EvalErrorPrediction = 0.07690000 * 10000; totalSamplesSeen = 380000; learningRatePerSample = 0.1; epochTime=0.050082s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.38'

07/14/2016 12:34:04: Starting Epoch 39: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 38: frames [380000..390000] (first sequence at sample 380000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[39 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15486622 * 1280; EvalErrorPrediction = 0.06406250 * 1280; time = 0.0064s; samplesPerSecond = 200658.4
07/14/2016 12:34:04:  Epoch[39 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17947733 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0062s; samplesPerSecond = 206451.6
07/14/2016 12:34:04:  Epoch[39 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17070038 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0061s; samplesPerSecond = 208401.2
07/14/2016 12:34:04:  Epoch[39 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17012653 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0061s; samplesPerSecond = 209150.3
07/14/2016 12:34:04:  Epoch[39 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14479184 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0061s; samplesPerSecond = 209801.7
07/14/2016 12:34:04:  Epoch[39 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.13912868 * 1280; EvalErrorPrediction = 0.06171875 * 1280; time = 0.0062s; samplesPerSecond = 205358.6
07/14/2016 12:34:04:  Epoch[39 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15967226 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0062s; samplesPerSecond = 207893.5
07/14/2016 12:34:04: Finished Epoch[39 of 50]: [Training] CrossEntropyWithSoftmax = 0.16386038 * 10000; EvalErrorPrediction = 0.07550000 * 10000; totalSamplesSeen = 390000; learningRatePerSample = 0.1; epochTime=0.050027s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.39'

07/14/2016 12:34:04: Starting Epoch 40: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 39: frames [390000..400000] (first sequence at sample 390000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[40 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17116028 * 1280; EvalErrorPrediction = 0.08046875 * 1280; time = 0.0071s; samplesPerSecond = 179548.3
07/14/2016 12:34:04:  Epoch[40 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15966750 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0066s; samplesPerSecond = 195241.0
07/14/2016 12:34:04:  Epoch[40 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15952737 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0064s; samplesPerSecond = 200595.5
07/14/2016 12:34:04:  Epoch[40 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15408745 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0065s; samplesPerSecond = 196409.4
07/14/2016 12:34:04:  Epoch[40 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15742240 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0064s; samplesPerSecond = 200564.1
07/14/2016 12:34:04:  Epoch[40 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16165428 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0063s; samplesPerSecond = 202020.2
07/14/2016 12:34:04:  Epoch[40 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18436918 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0067s; samplesPerSecond = 191187.5
07/14/2016 12:34:04: Finished Epoch[40 of 50]: [Training] CrossEntropyWithSoftmax = 0.16481310 * 10000; EvalErrorPrediction = 0.07570000 * 10000; totalSamplesSeen = 400000; learningRatePerSample = 0.1; epochTime=0.053089s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.40'

07/14/2016 12:34:04: Starting Epoch 41: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 40: frames [400000..410000] (first sequence at sample 400000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[41 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17456944 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0067s; samplesPerSecond = 189657.7
07/14/2016 12:34:04:  Epoch[41 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.14992547 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0065s; samplesPerSecond = 196681.0
07/14/2016 12:34:04:  Epoch[41 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16370704 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0065s; samplesPerSecond = 195928.4
07/14/2016 12:34:04:  Epoch[41 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.16085882 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0066s; samplesPerSecond = 193968.8
07/14/2016 12:34:04:  Epoch[41 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15773911 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0065s; samplesPerSecond = 197439.5
07/14/2016 12:34:04:  Epoch[41 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14436550 * 1280; EvalErrorPrediction = 0.06171875 * 1280; time = 0.0065s; samplesPerSecond = 196138.5
07/14/2016 12:34:04:  Epoch[41 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15339108 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0065s; samplesPerSecond = 195419.8
07/14/2016 12:34:04: Finished Epoch[41 of 50]: [Training] CrossEntropyWithSoftmax = 0.15843987 * 10000; EvalErrorPrediction = 0.07380000 * 10000; totalSamplesSeen = 410000; learningRatePerSample = 0.1; epochTime=0.052855s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.41'

07/14/2016 12:34:04: Starting Epoch 42: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 41: frames [410000..420000] (first sequence at sample 410000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[42 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17765868 * 1280; EvalErrorPrediction = 0.09062500 * 1280; time = 0.0069s; samplesPerSecond = 186779.5
07/14/2016 12:34:04:  Epoch[42 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15345802 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0067s; samplesPerSecond = 190902.3
07/14/2016 12:34:04:  Epoch[42 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15648363 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0066s; samplesPerSecond = 194528.9
07/14/2016 12:34:04:  Epoch[42 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15764861 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0065s; samplesPerSecond = 197500.4
07/14/2016 12:34:04:  Epoch[42 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.16663527 * 1280; EvalErrorPrediction = 0.07812500 * 1280; time = 0.0066s; samplesPerSecond = 194736.0
07/14/2016 12:34:04:  Epoch[42 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14366312 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0065s; samplesPerSecond = 196319.0
07/14/2016 12:34:04:  Epoch[42 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.15641041 * 1280; EvalErrorPrediction = 0.07656250 * 1280; time = 0.0065s; samplesPerSecond = 196018.4
07/14/2016 12:34:04: Finished Epoch[42 of 50]: [Training] CrossEntropyWithSoftmax = 0.15860306 * 10000; EvalErrorPrediction = 0.07620000 * 10000; totalSamplesSeen = 420000; learningRatePerSample = 0.1; epochTime=0.053211s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.42'

07/14/2016 12:34:04: Starting Epoch 43: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 42: frames [420000..430000] (first sequence at sample 420000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[43 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.15629972 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0066s; samplesPerSecond = 192916.4
07/14/2016 12:34:04:  Epoch[43 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15308679 * 1280; EvalErrorPrediction = 0.07578125 * 1280; time = 0.0065s; samplesPerSecond = 196078.4
07/14/2016 12:34:04:  Epoch[43 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.13974707 * 1280; EvalErrorPrediction = 0.06171875 * 1280; time = 0.0065s; samplesPerSecond = 197897.3
07/14/2016 12:34:04:  Epoch[43 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15581870 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0065s; samplesPerSecond = 196681.0
07/14/2016 12:34:04:  Epoch[43 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.14598870 * 1280; EvalErrorPrediction = 0.06562500 * 1280; time = 0.0065s; samplesPerSecond = 196138.5
07/14/2016 12:34:04:  Epoch[43 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.18108988 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0065s; samplesPerSecond = 196439.5
07/14/2016 12:34:04:  Epoch[43 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.17168808 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0065s; samplesPerSecond = 196198.7
07/14/2016 12:34:04: Finished Epoch[43 of 50]: [Training] CrossEntropyWithSoftmax = 0.15814004 * 10000; EvalErrorPrediction = 0.07360000 * 10000; totalSamplesSeen = 430000; learningRatePerSample = 0.1; epochTime=0.052836s
07/14/2016 12:34:04: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.43'

07/14/2016 12:34:04: Starting Epoch 44: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 43: frames [430000..440000] (first sequence at sample 430000), data subset 0 of 1

07/14/2016 12:34:04: Starting minibatch loop.
07/14/2016 12:34:04:  Epoch[44 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17231209 * 1280; EvalErrorPrediction = 0.08828125 * 1280; time = 0.0068s; samplesPerSecond = 189629.6
07/14/2016 12:34:04:  Epoch[44 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.13793182 * 1280; EvalErrorPrediction = 0.06171875 * 1280; time = 0.0066s; samplesPerSecond = 195062.5
07/14/2016 12:34:04:  Epoch[44 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16530783 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0065s; samplesPerSecond = 196771.7
07/14/2016 12:34:05:  Epoch[44 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.19532485 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0065s; samplesPerSecond = 195718.7
07/14/2016 12:34:05:  Epoch[44 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17846689 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0066s; samplesPerSecond = 192829.2
07/14/2016 12:34:05:  Epoch[44 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.21160450 * 1280; EvalErrorPrediction = 0.09375000 * 1280; time = 0.0065s; samplesPerSecond = 197439.5
07/14/2016 12:34:05:  Epoch[44 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18801451 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0065s; samplesPerSecond = 198203.8
07/14/2016 12:34:05: Finished Epoch[44 of 50]: [Training] CrossEntropyWithSoftmax = 0.17975632 * 10000; EvalErrorPrediction = 0.08190000 * 10000; totalSamplesSeen = 440000; learningRatePerSample = 0.1; epochTime=0.052953s
07/14/2016 12:34:05: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.44'

07/14/2016 12:34:05: Starting Epoch 45: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 44: frames [440000..450000] (first sequence at sample 440000), data subset 0 of 1

07/14/2016 12:34:05: Starting minibatch loop.
07/14/2016 12:34:05:  Epoch[45 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.19800919 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0067s; samplesPerSecond = 190306.3
07/14/2016 12:34:05:  Epoch[45 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15382904 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0061s; samplesPerSecond = 210146.1
07/14/2016 12:34:05:  Epoch[45 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16011503 * 1280; EvalErrorPrediction = 0.06953125 * 1280; time = 0.0060s; samplesPerSecond = 213760.9
07/14/2016 12:34:05:  Epoch[45 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17285686 * 1280; EvalErrorPrediction = 0.08515625 * 1280; time = 0.0060s; samplesPerSecond = 212872.1
07/14/2016 12:34:05:  Epoch[45 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.12871919 * 1280; EvalErrorPrediction = 0.05546875 * 1280; time = 0.0060s; samplesPerSecond = 212624.6
07/14/2016 12:34:05:  Epoch[45 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.19098673 * 1280; EvalErrorPrediction = 0.09218750 * 1280; time = 0.0060s; samplesPerSecond = 212342.4
07/14/2016 12:34:05:  Epoch[45 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.18352175 * 1280; EvalErrorPrediction = 0.08750000 * 1280; time = 0.0063s; samplesPerSecond = 203594.7
07/14/2016 12:34:05: Finished Epoch[45 of 50]: [Training] CrossEntropyWithSoftmax = 0.16838304 * 10000; EvalErrorPrediction = 0.07700000 * 10000; totalSamplesSeen = 450000; learningRatePerSample = 0.1; epochTime=0.049967s
07/14/2016 12:34:05: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.45'

07/14/2016 12:34:05: Starting Epoch 46: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 45: frames [450000..460000] (first sequence at sample 450000), data subset 0 of 1

07/14/2016 12:34:05: Starting minibatch loop.
07/14/2016 12:34:05:  Epoch[46 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17409599 * 1280; EvalErrorPrediction = 0.08281250 * 1280; time = 0.0063s; samplesPerSecond = 202179.8
07/14/2016 12:34:05:  Epoch[46 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.15440207 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0063s; samplesPerSecond = 201924.6
07/14/2016 12:34:05:  Epoch[46 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16372373 * 1280; EvalErrorPrediction = 0.07421875 * 1280; time = 0.0063s; samplesPerSecond = 204570.9
07/14/2016 12:34:05:  Epoch[46 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17626615 * 1280; EvalErrorPrediction = 0.07734375 * 1280; time = 0.0063s; samplesPerSecond = 204636.3
07/14/2016 12:34:05:  Epoch[46 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.19316645 * 1280; EvalErrorPrediction = 0.08984375 * 1280; time = 0.0060s; samplesPerSecond = 212801.3
07/14/2016 12:34:05:  Epoch[46 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.16905041 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0060s; samplesPerSecond = 213689.5
07/14/2016 12:34:05:  Epoch[46 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16094189 * 1280; EvalErrorPrediction = 0.07500000 * 1280; time = 0.0061s; samplesPerSecond = 210630.2
07/14/2016 12:34:05: Finished Epoch[46 of 50]: [Training] CrossEntropyWithSoftmax = 0.17035063 * 10000; EvalErrorPrediction = 0.08000000 * 10000; totalSamplesSeen = 460000; learningRatePerSample = 0.1; epochTime=0.049842s
07/14/2016 12:34:05: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.46'

07/14/2016 12:34:05: Starting Epoch 47: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 46: frames [460000..470000] (first sequence at sample 460000), data subset 0 of 1

07/14/2016 12:34:05: Starting minibatch loop.
07/14/2016 12:34:05:  Epoch[47 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.19717933 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0063s; samplesPerSecond = 203497.6
07/14/2016 12:34:05:  Epoch[47 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.17599429 * 1280; EvalErrorPrediction = 0.08437500 * 1280; time = 0.0062s; samplesPerSecond = 205325.6
07/14/2016 12:34:05:  Epoch[47 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16111016 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0061s; samplesPerSecond = 210526.3
07/14/2016 12:34:05:  Epoch[47 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.17828107 * 1280; EvalErrorPrediction = 0.07890625 * 1280; time = 0.0062s; samplesPerSecond = 207287.4
07/14/2016 12:34:05:  Epoch[47 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.17307177 * 1280; EvalErrorPrediction = 0.07343750 * 1280; time = 0.0062s; samplesPerSecond = 207657.4
07/14/2016 12:34:05:  Epoch[47 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.15202827 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0061s; samplesPerSecond = 208163.9
07/14/2016 12:34:05:  Epoch[47 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14770594 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0062s; samplesPerSecond = 206618.2
07/14/2016 12:34:05: Finished Epoch[47 of 50]: [Training] CrossEntropyWithSoftmax = 0.16891559 * 10000; EvalErrorPrediction = 0.07650000 * 10000; totalSamplesSeen = 470000; learningRatePerSample = 0.1; epochTime=0.050016s
07/14/2016 12:34:05: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.47'

07/14/2016 12:34:05: Starting Epoch 48: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 47: frames [470000..480000] (first sequence at sample 470000), data subset 0 of 1

07/14/2016 12:34:05: Starting minibatch loop.
07/14/2016 12:34:05:  Epoch[48 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.18937275 * 1280; EvalErrorPrediction = 0.08828125 * 1280; time = 0.0070s; samplesPerSecond = 183381.1
07/14/2016 12:34:05:  Epoch[48 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16697199 * 1280; EvalErrorPrediction = 0.07265625 * 1280; time = 0.0066s; samplesPerSecond = 192713.0
07/14/2016 12:34:05:  Epoch[48 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.16790347 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0064s; samplesPerSecond = 200910.4
07/14/2016 12:34:05:  Epoch[48 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15334325 * 1280; EvalErrorPrediction = 0.06718750 * 1280; time = 0.0064s; samplesPerSecond = 199252.8
07/14/2016 12:34:05:  Epoch[48 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.13439155 * 1280; EvalErrorPrediction = 0.06015625 * 1280; time = 0.0063s; samplesPerSecond = 201860.9
07/14/2016 12:34:05:  Epoch[48 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.17633085 * 1280; EvalErrorPrediction = 0.09062500 * 1280; time = 0.0065s; samplesPerSecond = 198173.1
07/14/2016 12:34:05:  Epoch[48 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16360836 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0064s; samplesPerSecond = 200250.3
07/14/2016 12:34:05: Finished Epoch[48 of 50]: [Training] CrossEntropyWithSoftmax = 0.16220248 * 10000; EvalErrorPrediction = 0.07660000 * 10000; totalSamplesSeen = 480000; learningRatePerSample = 0.1; epochTime=0.052434s
07/14/2016 12:34:05: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.48'

07/14/2016 12:34:05: Starting Epoch 49: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 48: frames [480000..490000] (first sequence at sample 480000), data subset 0 of 1

07/14/2016 12:34:05: Starting minibatch loop.
07/14/2016 12:34:05:  Epoch[49 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17351888 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0065s; samplesPerSecond = 195898.4
07/14/2016 12:34:05:  Epoch[49 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16539623 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0063s; samplesPerSecond = 202115.9
07/14/2016 12:34:05:  Epoch[49 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.17113481 * 1280; EvalErrorPrediction = 0.08203125 * 1280; time = 0.0064s; samplesPerSecond = 201226.2
07/14/2016 12:34:05:  Epoch[49 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15255375 * 1280; EvalErrorPrediction = 0.06250000 * 1280; time = 0.0063s; samplesPerSecond = 201701.9
07/14/2016 12:34:05:  Epoch[49 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.18179188 * 1280; EvalErrorPrediction = 0.08593750 * 1280; time = 0.0064s; samplesPerSecond = 200658.4
07/14/2016 12:34:05:  Epoch[49 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14414072 * 1280; EvalErrorPrediction = 0.05937500 * 1280; time = 0.0063s; samplesPerSecond = 202531.6
07/14/2016 12:34:05:  Epoch[49 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.16890459 * 1280; EvalErrorPrediction = 0.08125000 * 1280; time = 0.0064s; samplesPerSecond = 198542.0
07/14/2016 12:34:05: Finished Epoch[49 of 50]: [Training] CrossEntropyWithSoftmax = 0.16371610 * 10000; EvalErrorPrediction = 0.07560000 * 10000; totalSamplesSeen = 490000; learningRatePerSample = 0.1; epochTime=0.051611s
07/14/2016 12:34:05: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.49'

07/14/2016 12:34:05: Starting Epoch 50: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 49: frames [490000..500000] (first sequence at sample 490000), data subset 0 of 1

07/14/2016 12:34:05: Starting minibatch loop.
07/14/2016 12:34:05:  Epoch[50 of 50]-Minibatch[   1-  10, 14.29%]: CrossEntropyWithSoftmax = 0.17541753 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.0066s; samplesPerSecond = 192887.3
07/14/2016 12:34:05:  Epoch[50 of 50]-Minibatch[  11-  20, 28.57%]: CrossEntropyWithSoftmax = 0.16571552 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0062s; samplesPerSecond = 205424.5
07/14/2016 12:34:05:  Epoch[50 of 50]-Minibatch[  21-  30, 42.86%]: CrossEntropyWithSoftmax = 0.15297840 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0063s; samplesPerSecond = 204244.5
07/14/2016 12:34:05:  Epoch[50 of 50]-Minibatch[  31-  40, 57.14%]: CrossEntropyWithSoftmax = 0.15474615 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0064s; samplesPerSecond = 201479.6
07/14/2016 12:34:05:  Epoch[50 of 50]-Minibatch[  41-  50, 71.43%]: CrossEntropyWithSoftmax = 0.15424118 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0065s; samplesPerSecond = 197317.7
07/14/2016 12:34:05:  Epoch[50 of 50]-Minibatch[  51-  60, 85.71%]: CrossEntropyWithSoftmax = 0.14947748 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0066s; samplesPerSecond = 194884.3
07/14/2016 12:34:05:  Epoch[50 of 50]-Minibatch[  61-  70, 100.00%]: CrossEntropyWithSoftmax = 0.14548969 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0067s; samplesPerSecond = 191158.9
07/14/2016 12:34:05: Finished Epoch[50 of 50]: [Training] CrossEntropyWithSoftmax = 0.15954734 * 10000; EvalErrorPrediction = 0.07370000 * 10000; totalSamplesSeen = 500000; learningRatePerSample = 0.1; epochTime=0.052186s
07/14/2016 12:34:05: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn'
07/14/2016 12:34:05: CNTKCommandTrainEnd: Simple_Demo

07/14/2016 12:34:05: Action "train" complete.


07/14/2016 12:34:05: ##############################################################################
07/14/2016 12:34:05: #                                                                            #
07/14/2016 12:34:05: # Action "write"                                                             #
07/14/2016 12:34:05: #                                                                            #
07/14/2016 12:34:05: ##############################################################################


Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *1]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *1]
Validating --> MeanOfFeatures = Mean (features) : [2 x *1] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *1] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *1], [2], [2] -> [2 x *1]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *1] -> [50 x *1]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *1] -> [2 x 1 x *1]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *1], [2 x 1] -> [2 x 1 x *1]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *1] -> [2 x 1 x *1]
Validating --> Prior = Mean (labels) : [2 x *1] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *1], [2] -> [2 x 1 x *1]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.



Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[B0 Gradient[50 x 1]] [B1 Gradient[50 x 1]] [B2 Gradient[2 x 1]] [CrossEntropyWithSoftmax Gradient[1]] [CrossEntropyWithSoftmax Value[1]] [EvalErrorPrediction Gradient[1]] [EvalErrorPrediction Value[1]] [H1 Gradient[50 x 1 x *1]] [H2 Gradient[50 x 1 x *1]] [HLast Gradient[2 x 1 x *1]] [InvStdOfFeatures Gradient[2]] [LogOfPrior Gradient[2]] [MVNormalizedFeatures Gradient[2 x *1]] [MeanOfFeatures Gradient[2]] [PosteriorProb Gradient[2 x 1 x *1]] [PosteriorProb Value[2 x 1 x *1]] [Prior Gradient[2]] [ScaledLogLikelihood Gradient[2 x 1 x *1]] [W0 Gradient[50 x 2]] [W0*features Gradient[50 x *1]] [W0*features+B0 Gradient[50 x 1 x *1]] [W1 Gradient[50 x 50]] [W1*H1 Gradient[50 x 1 x *1]] [W1*H1+B1 Gradient[50 x 1 x *1]] [W2 Gradient[2 x 50]] [W2*H1 Gradient[2 x 1 x *1]] [features Gradient[2 x *1]] [labels Gradient[2 x *1]] }
0x7fd18b2735f8: {[InvStdOfFeatures Value[2]] }
0x7fd18b274468: {[labels Value[2 x *1]] }
0x7fd18b274ff8: {[MeanOfFeatures Value[2]] }
0x7fd18b276068: {[Prior Value[2]] }
0x7fd18b276788: {[W0 Value[50 x 2]] }
0x7fd18b276b58: {[W1 Value[50 x 50]] }
0x7fd18b2784d8: {[W2 Value[2 x 50]] }
0x7fd18b27e648: {[ScaledLogLikelihood Value[2 x 1 x *1]] }
0x7fd18b27e8d8: {[MVNormalizedFeatures Value[2 x *1]] }
0x7fd18b27ea18: {[LogOfPrior Value[2]] }
0x7fd18b2803c8: {[W0*features Value[50 x *1]] }
0x7fd18b2807d8: {[W0*features+B0 Value[50 x 1 x *1]] }
0x7fd18b280998: {[H1 Value[50 x 1 x *1]] }
0x7fd18b280b58: {[W1*H1 Value[50 x 1 x *1]] }
0x7fd18b280d18: {[W1*H1+B1 Value[50 x 1 x *1]] }
0x7fd18b280ed8: {[H2 Value[50 x 1 x *1]] }
0x7fd18b281098: {[W2*H1 Value[2 x 1 x *1]] }
0x7fd18b281258: {[HLast Value[2 x 1 x *1]] }
0x7fd18b4a0f88: {[B0 Value[50 x 1]] }
0x7fd18b4a1c08: {[B1 Value[50 x 1]] }
0x7fd18b4a22b8: {[features Value[2 x *1]] }
0x7fd18b4a2c68: {[B2 Value[2 x 1]] }

Minibatch[0]: ActualMBSize = 603
Written to /tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/SimpleOutput*
Total Samples Evaluated = 603

07/14/2016 12:34:05: Action "write" complete.

07/14/2016 12:34:05: __COMPLETED__
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu DeviceId=0 timestamping=true makeMode=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 12:04:41
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by philly on 34e58dd0283f
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
07/14/2016 12:34:06: -------------------------------------------------------------------
07/14/2016 12:34:06: Build info: 

07/14/2016 12:34:06: 		Built time: Jul 14 2016 12:04:41
07/14/2016 12:34:06: 		Last modified date: Tue Jul 12 04:28:35 2016
07/14/2016 12:34:06: 		Build type: release
07/14/2016 12:34:06: 		Build target: GPU
07/14/2016 12:34:06: 		With 1bit-SGD: no
07/14/2016 12:34:06: 		Math lib: mkl
07/14/2016 12:34:06: 		CUDA_PATH: /usr/local/cuda-7.5
07/14/2016 12:34:06: 		CUB_PATH: /usr/local/cub-1.4.1
07/14/2016 12:34:06: 		CUDNN_PATH: /usr/local/cudnn-4.0
07/14/2016 12:34:06: 		Build Branch: HEAD
07/14/2016 12:34:06: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
07/14/2016 12:34:06: 		Built by philly on 34e58dd0283f
07/14/2016 12:34:06: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
07/14/2016 12:34:06: -------------------------------------------------------------------
07/14/2016 12:34:07: -------------------------------------------------------------------
07/14/2016 12:34:07: GPU info:

07/14/2016 12:34:07: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:07: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:07: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:07: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
07/14/2016 12:34:07: -------------------------------------------------------------------

07/14/2016 12:34:07: Running on localhost at 2016/07/14 12:34:07
07/14/2016 12:34:07: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple/cntk.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple  OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu  DeviceId=0  timestamping=true  makeMode=true



07/14/2016 12:34:07: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/14/2016 12:34:07: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=$RunDir$/models/simple.dnn
deviceId=$DeviceNumber$
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=$DataDir$/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=$RunDir$/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DeviceId=0
timestamping=true
makeMode=true

07/14/2016 12:34:07: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/14/2016 12:34:07: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/14/2016 12:34:07: command=Simple_Demo:Simple_Demo_Output
DeviceNumber=-1
precision=float
modelPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn
deviceId=-1
outputNodeNames=ScaledLogLikelihood
traceLevel=1
Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]
Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/SimpleOutput    
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
DeviceId=0
timestamping=true
makeMode=true

07/14/2016 12:34:07: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/14/2016 12:34:07: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.cntk:command=Simple_Demo:Simple_Demo_Output
configparameters: cntk.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Simple
configparameters: cntk.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk.cntk:deviceId=0
configparameters: cntk.cntk:DeviceNumber=-1
configparameters: cntk.cntk:makeMode=true
configparameters: cntk.cntk:modelPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn
configparameters: cntk.cntk:OutputDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
configparameters: cntk.cntk:outputNodeNames=ScaledLogLikelihood
configparameters: cntk.cntk:precision=float
configparameters: cntk.cntk:RunDir=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu
configparameters: cntk.cntk:Simple_Demo=[
    action=train
    SimpleNetworkBuilder=[
        layerSizes=2:50*2:2
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes=Sigmoid
        initValueScale=1.0
        applyMeanVarNorm=true
        uniformInit=true
        needPrior=true
    ]
    SGD=[
        epochSize=0 
        minibatchSize=128
        learningRatesPerSample=0.1
        momentumAsTimeConstant=2500
        dropoutRate=0.0
        maxEpochs=50
        keepCheckPointFiles = true
    ]
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTrain_cntk_text.txt
        input = [
            features=[
dim = 2      
                format = "dense"
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
]

configparameters: cntk.cntk:Simple_Demo_Output=[
    action=write
    reader=[
        readerType=CNTKTextFormatReader
        file=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/SimpleDataTest_cntk_text.txt
        input = [
            features=[
dim = 2 
                format = "dense" 
            ]
            labels=[
dim = 2 
                format = "dense"
            ]
        ]
    ]
outputPath=/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/SimpleOutput    
]

configparameters: cntk.cntk:timestamping=true
configparameters: cntk.cntk:traceLevel=1
07/14/2016 12:34:07: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/14/2016 12:34:07: Commands: Simple_Demo Simple_Demo_Output
07/14/2016 12:34:07: Precision = "float"
07/14/2016 12:34:07: CNTKModelPath: /tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn
07/14/2016 12:34:07: CNTKCommandTrainInfo: Simple_Demo : 50
07/14/2016 12:34:07: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 50

07/14/2016 12:34:07: ##############################################################################
07/14/2016 12:34:07: #                                                                            #
07/14/2016 12:34:07: # Action "train"                                                             #
07/14/2016 12:34:07: #                                                                            #
07/14/2016 12:34:07: ##############################################################################

07/14/2016 12:34:07: CNTKCommandTrainBegin: Simple_Demo
SimpleNetworkBuilder Using GPU 0

07/14/2016 12:34:07: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn.49'.

Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *1]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *1]
Validating --> MeanOfFeatures = Mean (features) : [2 x *1] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *1] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *1], [2], [2] -> [2 x *1]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *1] -> [50 x *1]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *1], [50 x 1] -> [50 x 1 x *1]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *1] -> [50 x 1 x *1]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *1] -> [2 x 1 x *1]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *1], [2 x 1] -> [2 x 1 x *1]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *1], [2 x 1 x *1] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *1] -> [2 x 1 x *1]
Validating --> Prior = Mean (labels) : [2 x *1] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *1], [2] -> [2 x 1 x *1]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/14/2016 12:34:07: Loaded model with 25 nodes on GPU 0.

07/14/2016 12:34:07: Training criterion node(s):
07/14/2016 12:34:07: 	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax

07/14/2016 12:34:07: Evaluation criterion node(s):

07/14/2016 12:34:07: 	EvalErrorPrediction = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[EvalErrorPrediction Gradient[1]] [InvStdOfFeatures Gradient[2]] [LogOfPrior Gradient[2]] [MVNormalizedFeatures Gradient[2 x *1]] [MeanOfFeatures Gradient[2]] [PosteriorProb Gradient[2 x 1 x *1]] [PosteriorProb Value[2 x 1 x *1]] [Prior Gradient[2]] [ScaledLogLikelihood Gradient[2 x 1 x *1]] [features Gradient[2 x *1]] [labels Gradient[2 x *1]] }
0x7fc50a001838: {[EvalErrorPrediction Value[1]] }
0x7fc50a001c58: {[ScaledLogLikelihood Value[2 x 1 x *1]] }
0x7fc50a001e18: {[CrossEntropyWithSoftmax Value[1]] }
0x7fc50a002358: {[W0 Gradient[50 x 2]] [W0*features+B0 Value[50 x 1 x *1]] }
0x7fc50a0024c8: {[LogOfPrior Value[2]] }
0x7fc50a0040a8: {[MVNormalizedFeatures Value[2 x *1]] }
0x7fc50a004868: {[W0*features Value[50 x *1]] }
0x7fc50a004a78: {[H1 Value[50 x 1 x *1]] [W0*features Gradient[50 x *1]] }
0x7fc50a004bd8: {[W0*features+B0 Gradient[50 x 1 x *1]] [W1*H1 Value[50 x 1 x *1]] }
0x7fc50a004d38: {[W1 Gradient[50 x 50]] [W1*H1+B1 Value[50 x 1 x *1]] }
0x7fc50a004e98: {[H2 Value[50 x 1 x *1]] [W1*H1 Gradient[50 x 1 x *1]] }
0x7fc50a005058: {[B0 Gradient[50 x 1]] [H1 Gradient[50 x 1 x *1]] [W1*H1+B1 Gradient[50 x 1 x *1]] [W2*H1 Value[2 x 1 x *1]] }
0x7fc50a005218: {[HLast Value[2 x 1 x *1]] [W2 Gradient[2 x 50]] }
0x7fc50a005d78: {[CrossEntropyWithSoftmax Gradient[1]] }
0x7fc50a005f38: {[B1 Gradient[50 x 1]] [H2 Gradient[50 x 1 x *1]] [HLast Gradient[2 x 1 x *1]] }
0x7fc50a0060f8: {[W2*H1 Gradient[2 x 1 x *1]] }
0x7fc50a0062b8: {[B2 Gradient[2 x 1]] }
0x7fc50a219098: {[features Value[2 x *1]] }
0x7fc50a21a2e8: {[InvStdOfFeatures Value[2]] }
0x7fc50a21b228: {[labels Value[2 x *1]] }
0x7fc50a21bf38: {[MeanOfFeatures Value[2]] }
0x7fc50a21cf38: {[Prior Value[2]] }
0x7fc50a21dd48: {[W0 Value[50 x 2]] }
0x7fc50a21dff8: {[W1 Value[50 x 50]] }
0x7fc50a226aa8: {[B2 Value[2 x 1]] }
0x7fc50a2fbca8: {[W2 Value[2 x 50]] }
0x7fc50e73f748: {[B1 Value[50 x 1]] }
0x7fc50f9bc468: {[B0 Value[50 x 1]] }

07/14/2016 12:34:07: No PreCompute nodes found, skipping PreCompute step.

07/14/2016 12:34:07: Starting Epoch 50: learning rate per sample = 0.100000  effective momentum = 0.950085  momentum as time constant = 2499.8 samples
BlockRandomizer::StartEpoch: epoch 49: frames [490000..500000] (first sequence at sample 490000), data subset 0 of 1

07/14/2016 12:34:07: Starting minibatch loop.
07/14/2016 12:34:07:  Epoch[50 of 50]-Minibatch[   1-  10, 0.00000000000003%]: CrossEntropyWithSoftmax = 0.17541753 * 1280; EvalErrorPrediction = 0.08359375 * 1280; time = 0.1221s; samplesPerSecond = 10482.6
07/14/2016 12:34:07:  Epoch[50 of 50]-Minibatch[  11-  20, 0.00000000000006%]: CrossEntropyWithSoftmax = 0.16571552 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0066s; samplesPerSecond = 194765.7
07/14/2016 12:34:07:  Epoch[50 of 50]-Minibatch[  21-  30, 0.00000000000008%]: CrossEntropyWithSoftmax = 0.15297840 * 1280; EvalErrorPrediction = 0.06875000 * 1280; time = 0.0065s; samplesPerSecond = 195898.4
07/14/2016 12:34:07:  Epoch[50 of 50]-Minibatch[  31-  40, 0.00000000000011%]: CrossEntropyWithSoftmax = 0.15474615 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0064s; samplesPerSecond = 200973.5
07/14/2016 12:34:07:  Epoch[50 of 50]-Minibatch[  41-  50, 0.00000000000014%]: CrossEntropyWithSoftmax = 0.15424118 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0064s; samplesPerSecond = 199843.9
07/14/2016 12:34:07:  Epoch[50 of 50]-Minibatch[  51-  60, 0.00000000000017%]: CrossEntropyWithSoftmax = 0.14947748 * 1280; EvalErrorPrediction = 0.07031250 * 1280; time = 0.0064s; samplesPerSecond = 201574.8
07/14/2016 12:34:07:  Epoch[50 of 50]-Minibatch[  61-  70, 0.00000000000019%]: CrossEntropyWithSoftmax = 0.14548969 * 1280; EvalErrorPrediction = 0.07187500 * 1280; time = 0.0064s; samplesPerSecond = 200000.0
07/14/2016 12:34:07: Finished Epoch[50 of 50]: [Training] CrossEntropyWithSoftmax = 0.15954734 * 10000; EvalErrorPrediction = 0.07370000 * 10000; totalSamplesSeen = 500000; learningRatePerSample = 0.1; epochTime=0.169013s
07/14/2016 12:34:07: SGD: Saving checkpoint model '/tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/models/simple.dnn'
07/14/2016 12:34:07: CNTKCommandTrainEnd: Simple_Demo

07/14/2016 12:34:07: Action "train" complete.


07/14/2016 12:34:07: ##############################################################################
07/14/2016 12:34:07: #                                                                            #
07/14/2016 12:34:07: # Action "write"                                                             #
07/14/2016 12:34:07: #                                                                            #
07/14/2016 12:34:07: ##############################################################################


Post-processing network...

7 roots:
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax()
	EvalErrorPrediction = ErrorPrediction()
	InvStdOfFeatures = InvStdDev()
	MeanOfFeatures = Mean()
	PosteriorProb = Softmax()
	Prior = Mean()
	ScaledLogLikelihood = Minus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [2 x *2]
Validating --> W2 = LearnableParameter() :  -> [2 x 50]
Validating --> W1 = LearnableParameter() :  -> [50 x 50]
Validating --> W0 = LearnableParameter() :  -> [50 x 2]
Validating --> features = InputValue() :  -> [2 x *2]
Validating --> MeanOfFeatures = Mean (features) : [2 x *2] -> [2]
Validating --> InvStdOfFeatures = InvStdDev (features) : [2 x *2] -> [2]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization (features, MeanOfFeatures, InvStdOfFeatures) : [2 x *2], [2], [2] -> [2 x *2]
Validating --> W0*features = Times (W0, MVNormalizedFeatures) : [50 x 2], [2 x *2] -> [50 x *2]
Validating --> B0 = LearnableParameter() :  -> [50 x 1]
Validating --> W0*features+B0 = Plus (W0*features, B0) : [50 x *2], [50 x 1] -> [50 x 1 x *2]
Validating --> H1 = Sigmoid (W0*features+B0) : [50 x 1 x *2] -> [50 x 1 x *2]
Validating --> W1*H1 = Times (W1, H1) : [50 x 50], [50 x 1 x *2] -> [50 x 1 x *2]
Validating --> B1 = LearnableParameter() :  -> [50 x 1]
Validating --> W1*H1+B1 = Plus (W1*H1, B1) : [50 x 1 x *2], [50 x 1] -> [50 x 1 x *2]
Validating --> H2 = Sigmoid (W1*H1+B1) : [50 x 1 x *2] -> [50 x 1 x *2]
Validating --> W2*H1 = Times (W2, H2) : [2 x 50], [50 x 1 x *2] -> [2 x 1 x *2]
Validating --> B2 = LearnableParameter() :  -> [2 x 1]
Validating --> HLast = Plus (W2*H1, B2) : [2 x 1 x *2], [2 x 1] -> [2 x 1 x *2]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax (labels, HLast) : [2 x *2], [2 x 1 x *2] -> [1]
Validating --> EvalErrorPrediction = ErrorPrediction (labels, HLast) : [2 x *2], [2 x 1 x *2] -> [1]
Validating --> PosteriorProb = Softmax (HLast) : [2 x 1 x *2] -> [2 x 1 x *2]
Validating --> Prior = Mean (labels) : [2 x *2] -> [2]
Validating --> LogOfPrior = Log (Prior) : [2] -> [2]
Validating --> ScaledLogLikelihood = Minus (HLast, LogOfPrior) : [2 x 1 x *2], [2] -> [2 x 1 x *2]

Validating network. 17 nodes to process in pass 2.


Validating network, final pass.



12 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.



Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[B0 Gradient[50 x 1]] [B1 Gradient[50 x 1]] [B2 Gradient[2 x 1]] [CrossEntropyWithSoftmax Gradient[1]] [CrossEntropyWithSoftmax Value[1]] [EvalErrorPrediction Gradient[1]] [EvalErrorPrediction Value[1]] [H1 Gradient[50 x 1 x *2]] [H2 Gradient[50 x 1 x *2]] [HLast Gradient[2 x 1 x *2]] [InvStdOfFeatures Gradient[2]] [LogOfPrior Gradient[2]] [MVNormalizedFeatures Gradient[2 x *2]] [MeanOfFeatures Gradient[2]] [PosteriorProb Gradient[2 x 1 x *2]] [PosteriorProb Value[2 x 1 x *2]] [Prior Gradient[2]] [ScaledLogLikelihood Gradient[2 x 1 x *2]] [W0 Gradient[50 x 2]] [W0*features Gradient[50 x *2]] [W0*features+B0 Gradient[50 x 1 x *2]] [W1 Gradient[50 x 50]] [W1*H1 Gradient[50 x 1 x *2]] [W1*H1+B1 Gradient[50 x 1 x *2]] [W2 Gradient[2 x 50]] [W2*H1 Gradient[2 x 1 x *2]] [features Gradient[2 x *2]] [labels Gradient[2 x *2]] }
0x7fc50a007e28: {[B2 Value[2 x 1]] }
0x7fc50a0096c8: {[features Value[2 x *2]] }
0x7fc50a00a778: {[InvStdOfFeatures Value[2]] }
0x7fc50a21a508: {[B0 Value[50 x 1]] }
0x7fc50a21a878: {[ScaledLogLikelihood Value[2 x 1 x *2]] }
0x7fc50a21b608: {[B1 Value[50 x 1]] }
0x7fc50f99b9b8: {[W0*features+B0 Value[50 x 1 x *2]] }
0x7fc50f99bb78: {[H1 Value[50 x 1 x *2]] }
0x7fc50f99bd38: {[W1*H1 Value[50 x 1 x *2]] }
0x7fc50f99bef8: {[W1*H1+B1 Value[50 x 1 x *2]] }
0x7fc50f99c0b8: {[H2 Value[50 x 1 x *2]] }
0x7fc50f99c278: {[W2*H1 Value[2 x 1 x *2]] }
0x7fc50f99c438: {[HLast Value[2 x 1 x *2]] }
0x7fc50f9bd4f8: {[labels Value[2 x *2]] }
0x7fc50f9be968: {[MeanOfFeatures Value[2]] }
0x7fc50f9bf158: {[Prior Value[2]] }
0x7fc50f9c0108: {[W0 Value[50 x 2]] }
0x7fc50f9c04d8: {[W1 Value[50 x 50]] }
0x7fc50f9cc998: {[W2 Value[2 x 50]] }
0x7fc50f9d2278: {[MVNormalizedFeatures Value[2 x *2]] }
0x7fc50f9d2368: {[LogOfPrior Value[2]] }
0x7fc50f9d3ba8: {[W0*features Value[50 x *2]] }

Minibatch[0]: ActualMBSize = 603
Written to /tmp/cntk-test-20160714122957.627315/Speech_Simple@release_gpu/SimpleOutput*
Total Samples Evaluated = 603

07/14/2016 12:34:07: Action "write" complete.

07/14/2016 12:34:07: __COMPLETED__