# end-to-end test for recurrent LSTM for speech

precision = "float"
deviceId = $DeviceId$

command = speechTrain

// Note: These options are overridden from the command line in some test cases.
frameMode = false
truncated = true
parallelTrain = false

speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.dnn"
    #deviceId = $DeviceId$
    traceLevel = 1
    
    SGD = [
        epochSize = 20480
        minibatchSize = 20
        learningRatesPerMB = 0.5
        numMBsToShowResult = 10
        momentumPerMB = 0:0.9
        maxEpochs = 4
        keepCheckPointFiles = true       
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        nbruttsineachrecurrentiter = 32
        randomize = "auto"
        verbosity = 0

        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]

        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
          
            labelDim = 132
            labelType = "category"
        ]
    ]

    # define network using BrainScript
    BrainScriptNetworkBuilder = [

        # import some namespaces
        # TODO: allow to say import BS.RNNs LSTMP or import BS.RNNs to import all (literally creates new dict members mirroring those)
        RecurrentLSTMP2 = BS.RNNs.RecurrentLSTMP2
        Parameters = BS.Parameters

        useSelfStabilization = true

        // define basic I/O
        baseFeatDim = 33
        featDim = 11 * baseFeatDim
        labelDim = 132

        // hidden dimensions
        innerCellDim  = 1024
        hiddenDim     = 256
        numLSTMLayers = 3        // number of hidden LSTM model layers

        // features
        features = Input((1 : featDim),  tag='feature') // TEST: Artificially reading data transposed
        realFeatures = Transpose (features)             //       and swapping them back to (featDim:1), for testing Transpose()
        labels   = Input(labelDim, tag='label')
        feashift = RowSlice(featDim - baseFeatDim, baseFeatDim, realFeatures);

        featNorm = MeanVarNorm(feashift)

        // define the stack of hidden LSTM layers
        S(x) = Parameters.Stabilize (x, enabled=useSelfStabilization)
        LSTMoutput[k:1..numLSTMLayers] =
            if k == 1
            then RecurrentLSTMP2 (hiddenDim, cellDim=innerCellDim, S(featNorm),        inputDim=baseFeatDim, enableSelfStabilization=useSelfStabilization).h
            else RecurrentLSTMP2 (hiddenDim, cellDim=innerCellDim, S(LSTMoutput[k-1]), inputDim=hiddenDim,   enableSelfStabilization=useSelfStabilization).h

        // and add a softmax layer on top
        W = Parameters.WeightParam (labelDim, hiddenDim)
        B = Parameters.BiasParam   (labelDim)

        z = W * S(LSTMoutput[numLSTMLayers]) + B; // top-level input to Softmax

        // training
        useExplicitCriterion = false
        crNode = CrossEntropyWithSoftmax(labels, z)                 // this is the objective, as a node
        crExplicit = -(ReduceSum (labels .* LogSoftmax (z)))        // manually-defined per-sample objective
        ce = Pass (if useExplicitCriterion then crExplicit else crNode, tag='criterion')
        err = ErrorPrediction(labels, z, tag='evaluation')          // this also gets tracked

        // decoding
        logPrior = LogPrior(labels)	 
        ScaledLogLikelihood = Minus(z, logPrior, tag='output')      // sadly we can't say x - y since we want to assign a tag
    ]
]
