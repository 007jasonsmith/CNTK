CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 268381192 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/debug/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
07/13/2016 04:31:57: -------------------------------------------------------------------
07/13/2016 04:31:57: Build info: 

07/13/2016 04:31:57: 		Built time: Jul 13 2016 03:39:41
07/13/2016 04:31:57: 		Last modified date: Fri Jul  8 10:29:45 2016
07/13/2016 04:31:57: 		Build type: Debug
07/13/2016 04:31:57: 		Build target: GPU
07/13/2016 04:31:57: 		With 1bit-SGD: no
07/13/2016 04:31:57: 		Math lib: mkl
07/13/2016 04:31:57: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
07/13/2016 04:31:57: 		CUB_PATH: C:\src\cub-1.4.1
07/13/2016 04:31:57: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
07/13/2016 04:31:57: 		Build Branch: HEAD
07/13/2016 04:31:57: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
07/13/2016 04:31:57: 		Built by svcphil on liana-08-w
07/13/2016 04:31:57: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
07/13/2016 04:31:57: -------------------------------------------------------------------
07/13/2016 04:32:00: -------------------------------------------------------------------
07/13/2016 04:32:00: GPU info:

07/13/2016 04:32:00: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/13/2016 04:32:00: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/13/2016 04:32:00: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/13/2016 04:32:00: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
07/13/2016 04:32:00: -------------------------------------------------------------------

07/13/2016 04:32:00: Running on DPHAIM-24 at 2016/07/13 04:32:00
07/13/2016 04:32:00: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu  DeviceId=0  timestamping=true



07/13/2016 04:32:00: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/13/2016 04:32:00: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu
DeviceId=0
timestamping=true

07/13/2016 04:32:00: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/13/2016 04:32:00: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/13/2016 04:32:00: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu
DeviceId=0
timestamping=true

07/13/2016 04:32:00: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/13/2016 04:32:00: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
07/13/2016 04:32:00: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/13/2016 04:32:00: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
07/13/2016 04:32:00: Precision = "float"
07/13/2016 04:32:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
07/13/2016 04:32:00: CNTKCommandTrainInfo: dptPre1 : 2
07/13/2016 04:32:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
07/13/2016 04:32:00: CNTKCommandTrainInfo: dptPre2 : 2
07/13/2016 04:32:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech
07/13/2016 04:32:00: CNTKCommandTrainInfo: speechTrain : 4
07/13/2016 04:32:00: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

07/13/2016 04:32:00: ##############################################################################
07/13/2016 04:32:00: #                                                                            #
07/13/2016 04:32:00: # Action "train"                                                             #
07/13/2016 04:32:00: #                                                                            #
07/13/2016 04:32:00: ##############################################################################

07/13/2016 04:32:00: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/13/2016 04:32:02: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/13/2016 04:32:04: Created model with 19 nodes on GPU 0.

07/13/2016 04:32:04: Training criterion node(s):
07/13/2016 04:32:04: 	ce = CrossEntropyWithSoftmax

07/13/2016 04:32:04: Evaluation criterion node(s):

07/13/2016 04:32:04: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
00000025CFE78370: {[scaledLogLikelihood Value[132 x 1 x *]] }
00000025CFE78850: {[globalMean Value[363 x 1]] }
00000025CFE79960: {[labels Value[132 x *]] }
00000025CFE79B00: {[globalInvStd Value[363 x 1]] }
00000025CFE79BD0: {[globalPrior Value[132 x 1]] }
00000025CFE79D70: {[HL1.W Value[512 x 363]] }
00000025CFE79E40: {[HL1.b Value[512 x 1]] }
00000025CFE79F10: {[OL.W Value[132 x 512]] }
00000025CFE79FE0: {[OL.b Value[132 x 1]] }
00000025CFE7A0B0: {[err Value[1]] }
00000025CFF3E1C0: {[features Value[363 x *]] }
00000025D2385B80: {[ce Gradient[1]] }
00000025D2386060: {[featNorm Value[363 x *]] }
00000025D2386130: {[logPrior Value[132 x 1]] }
00000025D2386200: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
00000025D2386470: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
00000025D23867B0: {[OL.b Gradient[132 x 1]] }
00000025D2386BC0: {[OL.t Gradient[132 x 1 x *]] }
00000025D2386D60: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
00000025D2386F00: {[HL1.t Value[512 x *]] }
00000025D2387310: {[ce Value[1]] }
00000025D23878C0: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
00000025D2387A60: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }

07/13/2016 04:32:04: No PreCompute nodes found, skipping PreCompute step.

07/13/2016 04:32:04: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/13/2016 04:32:08: Starting minibatch loop.
07/13/2016 04:32:08:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.89978218 * 2560; err = 0.84375000 * 2560; time = 0.4837s; samplesPerSecond = 5293.0
07/13/2016 04:32:09:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.96755714 * 2560; err = 0.72031250 * 2560; time = 0.1835s; samplesPerSecond = 13952.5
07/13/2016 04:32:09:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55723495 * 2560; err = 0.65859375 * 2560; time = 0.1836s; samplesPerSecond = 13947.1
07/13/2016 04:32:09:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.29642792 * 2560; err = 0.61992187 * 2560; time = 0.1836s; samplesPerSecond = 13942.8
07/13/2016 04:32:09:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.02396393 * 2560; err = 0.55117187 * 2560; time = 0.1832s; samplesPerSecond = 13976.0
07/13/2016 04:32:09:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87309265 * 2560; err = 0.51484375 * 2560; time = 0.1833s; samplesPerSecond = 13964.8
07/13/2016 04:32:10:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.78157196 * 2560; err = 0.50507813 * 2560; time = 0.1833s; samplesPerSecond = 13966.9
07/13/2016 04:32:10:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.75391235 * 2560; err = 0.50781250 * 2560; time = 0.1830s; samplesPerSecond = 13988.2
07/13/2016 04:32:10:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.66460266 * 2560; err = 0.45742187 * 2560; time = 0.1837s; samplesPerSecond = 13939.3
07/13/2016 04:32:10:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62184296 * 2560; err = 0.47968750 * 2560; time = 0.1833s; samplesPerSecond = 13965.9
07/13/2016 04:32:10:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.65328217 * 2560; err = 0.47265625 * 2560; time = 0.1838s; samplesPerSecond = 13929.4
07/13/2016 04:32:10:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.50686951 * 2560; err = 0.44921875 * 2560; time = 0.1831s; samplesPerSecond = 13983.9
07/13/2016 04:32:11:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.46723938 * 2560; err = 0.42304687 * 2560; time = 0.1832s; samplesPerSecond = 13971.4
07/13/2016 04:32:11:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.49163513 * 2560; err = 0.44140625 * 2560; time = 0.1834s; samplesPerSecond = 13957.8
07/13/2016 04:32:11:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46437683 * 2560; err = 0.43398437 * 2560; time = 0.1835s; samplesPerSecond = 13953.8
07/13/2016 04:32:11:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.43047485 * 2560; err = 0.43867187 * 2560; time = 0.1830s; samplesPerSecond = 13992.7
07/13/2016 04:32:11:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.42106018 * 2560; err = 0.41992188 * 2560; time = 0.1835s; samplesPerSecond = 13953.5
07/13/2016 04:32:12:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.46538086 * 2560; err = 0.42421875 * 2560; time = 0.1832s; samplesPerSecond = 13971.3
07/13/2016 04:32:12:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.47427673 * 2560; err = 0.44062500 * 2560; time = 0.1835s; samplesPerSecond = 13951.5
07/13/2016 04:32:12:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42847290 * 2560; err = 0.44023438 * 2560; time = 0.1833s; samplesPerSecond = 13963.2
07/13/2016 04:32:12:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.34078369 * 2560; err = 0.41171875 * 2560; time = 0.1834s; samplesPerSecond = 13956.7
07/13/2016 04:32:12:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.39474487 * 2560; err = 0.42734375 * 2560; time = 0.1837s; samplesPerSecond = 13939.4
07/13/2016 04:32:12:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.40151062 * 2560; err = 0.41250000 * 2560; time = 0.1832s; samplesPerSecond = 13972.5
07/13/2016 04:32:13:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.39345703 * 2560; err = 0.42734375 * 2560; time = 0.1832s; samplesPerSecond = 13976.6
07/13/2016 04:32:13:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.32485046 * 2560; err = 0.40156250 * 2560; time = 0.1832s; samplesPerSecond = 13970.1
07/13/2016 04:32:13:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27032471 * 2560; err = 0.39765625 * 2560; time = 0.1833s; samplesPerSecond = 13964.8
07/13/2016 04:32:13:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.32375488 * 2560; err = 0.39257813 * 2560; time = 0.1831s; samplesPerSecond = 13982.4
07/13/2016 04:32:13:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.25393982 * 2560; err = 0.38320312 * 2560; time = 0.1833s; samplesPerSecond = 13966.3
07/13/2016 04:32:14:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23377075 * 2560; err = 0.36953125 * 2560; time = 0.1831s; samplesPerSecond = 13982.1
07/13/2016 04:32:14:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20861511 * 2560; err = 0.35976562 * 2560; time = 0.1832s; samplesPerSecond = 13975.5
07/13/2016 04:32:14:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23675232 * 2560; err = 0.36757812 * 2560; time = 0.1834s; samplesPerSecond = 13960.9
07/13/2016 04:32:14:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.22960205 * 2560; err = 0.37460938 * 2560; time = 0.1671s; samplesPerSecond = 15323.7
07/13/2016 04:32:14: Finished Epoch[ 1 of 2]: [Training] ce = 1.65172386 * 81920; err = 0.46774902 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=10.4221s
07/13/2016 04:32:14: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech.1'

07/13/2016 04:32:14: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/13/2016 04:32:14: Starting minibatch loop.
07/13/2016 04:32:14:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.21869726 * 2560; err = 0.36992188 * 2560; time = 0.1919s; samplesPerSecond = 13340.9
07/13/2016 04:32:15:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.18345690 * 2560; err = 0.36679688 * 2560; time = 0.1834s; samplesPerSecond = 13959.6
07/13/2016 04:32:15:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.17220421 * 2560; err = 0.35898438 * 2560; time = 0.1832s; samplesPerSecond = 13971.8
07/13/2016 04:32:15:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.20035286 * 2560; err = 0.35781250 * 2560; time = 0.1820s; samplesPerSecond = 14064.6
07/13/2016 04:32:15:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.19499741 * 2560; err = 0.37460938 * 2560; time = 0.1823s; samplesPerSecond = 14039.2
07/13/2016 04:32:15:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.16373482 * 2560; err = 0.34687500 * 2560; time = 0.1820s; samplesPerSecond = 14064.8
07/13/2016 04:32:16:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.13869247 * 2560; err = 0.34804687 * 2560; time = 0.1818s; samplesPerSecond = 14078.3
07/13/2016 04:32:16:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.19293823 * 2560; err = 0.36992188 * 2560; time = 0.1820s; samplesPerSecond = 14063.9
07/13/2016 04:32:16:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.23978348 * 2560; err = 0.37539062 * 2560; time = 0.1822s; samplesPerSecond = 14050.6
07/13/2016 04:32:16:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18622742 * 2560; err = 0.36406250 * 2560; time = 0.1819s; samplesPerSecond = 14071.2
07/13/2016 04:32:16:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16710892 * 2560; err = 0.35703125 * 2560; time = 0.1819s; samplesPerSecond = 14074.7
07/13/2016 04:32:16:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.24683685 * 2560; err = 0.38554688 * 2560; time = 0.1820s; samplesPerSecond = 14064.7
07/13/2016 04:32:17:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.18601685 * 2560; err = 0.35273437 * 2560; time = 0.1819s; samplesPerSecond = 14071.5
07/13/2016 04:32:17:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.21721497 * 2560; err = 0.37617187 * 2560; time = 0.1821s; samplesPerSecond = 14060.9
07/13/2016 04:32:17:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.19934692 * 2560; err = 0.36953125 * 2560; time = 0.1820s; samplesPerSecond = 14068.2
07/13/2016 04:32:17:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.15099945 * 2560; err = 0.34257813 * 2560; time = 0.1823s; samplesPerSecond = 14043.0
07/13/2016 04:32:17:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.14984589 * 2560; err = 0.35703125 * 2560; time = 0.1824s; samplesPerSecond = 14035.9
07/13/2016 04:32:18:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.19028320 * 2560; err = 0.35898438 * 2560; time = 0.1819s; samplesPerSecond = 14073.0
07/13/2016 04:32:18:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.16434784 * 2560; err = 0.36406250 * 2560; time = 0.1822s; samplesPerSecond = 14049.2
07/13/2016 04:32:18:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08853760 * 2560; err = 0.33359375 * 2560; time = 0.1820s; samplesPerSecond = 14064.4
07/13/2016 04:32:18:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.15194244 * 2560; err = 0.35039063 * 2560; time = 0.1824s; samplesPerSecond = 14037.3
07/13/2016 04:32:18:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16113434 * 2560; err = 0.35625000 * 2560; time = 0.1822s; samplesPerSecond = 14053.5
07/13/2016 04:32:18:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.18479004 * 2560; err = 0.36757812 * 2560; time = 0.1819s; samplesPerSecond = 14077.1
07/13/2016 04:32:19:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.14554138 * 2560; err = 0.34843750 * 2560; time = 0.1822s; samplesPerSecond = 14053.0
07/13/2016 04:32:19:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.15263367 * 2560; err = 0.35390625 * 2560; time = 0.1822s; samplesPerSecond = 14053.7
07/13/2016 04:32:19:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.08563538 * 2560; err = 0.33437500 * 2560; time = 0.1819s; samplesPerSecond = 14071.6
07/13/2016 04:32:19:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10797424 * 2560; err = 0.34882812 * 2560; time = 0.1821s; samplesPerSecond = 14059.1
07/13/2016 04:32:19:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.07031860 * 2560; err = 0.33593750 * 2560; time = 0.1822s; samplesPerSecond = 14048.0
07/13/2016 04:32:20:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.09429016 * 2560; err = 0.33476563 * 2560; time = 0.1820s; samplesPerSecond = 14062.2
07/13/2016 04:32:20:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14633789 * 2560; err = 0.35351563 * 2560; time = 0.1822s; samplesPerSecond = 14051.2
07/13/2016 04:32:20:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.10476990 * 2560; err = 0.34335938 * 2560; time = 0.1820s; samplesPerSecond = 14062.4
07/13/2016 04:32:20:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07355957 * 2560; err = 0.32695313 * 2560; time = 0.1656s; samplesPerSecond = 15456.8
07/13/2016 04:32:20: Finished Epoch[ 2 of 2]: [Training] ce = 1.16032972 * 81920; err = 0.35574951 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=5.84679s
07/13/2016 04:32:20: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech'
07/13/2016 04:32:20: CNTKCommandTrainEnd: dptPre1

07/13/2016 04:32:20: Action "train" complete.


07/13/2016 04:32:20: ##############################################################################
07/13/2016 04:32:20: #                                                                            #
07/13/2016 04:32:20: # Action "edit"                                                              #
07/13/2016 04:32:20: #                                                                            #
07/13/2016 04:32:20: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/13/2016 04:32:20: Action "edit" complete.


07/13/2016 04:32:20: ##############################################################################
07/13/2016 04:32:20: #                                                                            #
07/13/2016 04:32:20: # Action "train"                                                             #
07/13/2016 04:32:20: #                                                                            #
07/13/2016 04:32:20: ##############################################################################

07/13/2016 04:32:20: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/13/2016 04:32:22: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/13/2016 04:32:22: Loaded model with 24 nodes on GPU 0.

07/13/2016 04:32:22: Training criterion node(s):
07/13/2016 04:32:22: 	ce = CrossEntropyWithSoftmax

07/13/2016 04:32:22: Evaluation criterion node(s):

07/13/2016 04:32:22: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
00000025E3F70580: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
00000025E3F70650: {[ce Gradient[1]] }
00000025E3F707F0: {[HL1.b Value[512 x 1]] }
00000025E3F708C0: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
00000025E3F70990: {[OL.t Gradient[132 x 1 x *3]] }
00000025E3F70A60: {[OL.b Gradient[132 x 1]] }
00000025E3F70B30: {[scaledLogLikelihood Value[132 x 1 x *3]] }
00000025E3F70C00: {[ce Value[1]] }
00000025E3F70CD0: {[HL1.t Value[512 x *3]] }
00000025E3F70DA0: {[features Value[363 x *3]] }
00000025E3F71010: {[globalPrior Value[132 x 1]] }
00000025E3F710E0: {[OL.W Value[132 x 512]] }
00000025E3F711B0: {[logPrior Value[132 x 1]] }
00000025E3F71420: {[err Value[1]] }
00000025E3F714F0: {[HL1.W Value[512 x 363]] }
00000025E3F715C0: {[globalMean Value[363 x 1]] }
00000025E3F71760: {[globalInvStd Value[363 x 1]] }
00000025E3F71830: {[HL2.W Value[512 x 512]] }
00000025E3F71900: {[labels Value[132 x *3]] }
00000025E3F719D0: {[featNorm Value[363 x *3]] }
00000025E3F71AA0: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
00000025E3F71B70: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
00000025E3F71C40: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
00000025E3F71D10: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
00000025E3F71DE0: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
00000025E3F72050: {[OL.b Value[132 x 1]] }
00000025E3F721F0: {[HL2.b Value[512 x 1]] }
00000025E3F722C0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }

07/13/2016 04:32:22: No PreCompute nodes found, skipping PreCompute step.

07/13/2016 04:32:22: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/13/2016 04:32:26: Starting minibatch loop.
07/13/2016 04:32:27:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 4.49739151 * 2560; err = 0.80429688 * 2560; time = 0.2039s; samplesPerSecond = 12556.1
07/13/2016 04:32:27:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.83226395 * 2560; err = 0.68125000 * 2560; time = 0.1951s; samplesPerSecond = 13120.2
07/13/2016 04:32:27:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.25921097 * 2560; err = 0.59921875 * 2560; time = 0.1942s; samplesPerSecond = 13179.2
07/13/2016 04:32:27:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.91240921 * 2560; err = 0.51210937 * 2560; time = 0.1938s; samplesPerSecond = 13210.0
07/13/2016 04:32:27:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.69259949 * 2560; err = 0.46679688 * 2560; time = 0.1945s; samplesPerSecond = 13162.8
07/13/2016 04:32:28:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.59069672 * 2560; err = 0.45312500 * 2560; time = 0.1945s; samplesPerSecond = 13160.9
07/13/2016 04:32:28:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.48813324 * 2560; err = 0.43789062 * 2560; time = 0.1949s; samplesPerSecond = 13132.2
07/13/2016 04:32:28:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.48960571 * 2560; err = 0.43515625 * 2560; time = 0.1943s; samplesPerSecond = 13178.8
07/13/2016 04:32:28:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.45628204 * 2560; err = 0.42187500 * 2560; time = 0.1934s; samplesPerSecond = 13238.9
07/13/2016 04:32:28:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41567383 * 2560; err = 0.40820313 * 2560; time = 0.1940s; samplesPerSecond = 13197.9
07/13/2016 04:32:29:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.42048950 * 2560; err = 0.41406250 * 2560; time = 0.1945s; samplesPerSecond = 13161.9
07/13/2016 04:32:29:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.34279480 * 2560; err = 0.39726563 * 2560; time = 0.1947s; samplesPerSecond = 13150.1
07/13/2016 04:32:29:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.31633148 * 2560; err = 0.38789062 * 2560; time = 0.1944s; samplesPerSecond = 13168.5
07/13/2016 04:32:29:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.33296814 * 2560; err = 0.39804688 * 2560; time = 0.1946s; samplesPerSecond = 13158.2
07/13/2016 04:32:29:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.32084351 * 2560; err = 0.39609375 * 2560; time = 0.1938s; samplesPerSecond = 13208.1
07/13/2016 04:32:30:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27189636 * 2560; err = 0.38125000 * 2560; time = 0.1946s; samplesPerSecond = 13157.4
07/13/2016 04:32:30:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29380188 * 2560; err = 0.38554688 * 2560; time = 0.1948s; samplesPerSecond = 13142.7
07/13/2016 04:32:30:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.31463013 * 2560; err = 0.38984375 * 2560; time = 0.1947s; samplesPerSecond = 13145.2
07/13/2016 04:32:30:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33578796 * 2560; err = 0.40664062 * 2560; time = 0.1943s; samplesPerSecond = 13176.3
07/13/2016 04:32:30:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.32202454 * 2560; err = 0.41484375 * 2560; time = 0.1936s; samplesPerSecond = 13221.5
07/13/2016 04:32:31:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23669434 * 2560; err = 0.37460938 * 2560; time = 0.1950s; samplesPerSecond = 13131.4
07/13/2016 04:32:31:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.27109985 * 2560; err = 0.38906250 * 2560; time = 0.1945s; samplesPerSecond = 13159.2
07/13/2016 04:32:31:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.26419678 * 2560; err = 0.37578125 * 2560; time = 0.1944s; samplesPerSecond = 13171.2
07/13/2016 04:32:31:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.23778992 * 2560; err = 0.37265625 * 2560; time = 0.1941s; samplesPerSecond = 13189.3
07/13/2016 04:32:31:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.21040344 * 2560; err = 0.36757812 * 2560; time = 0.1938s; samplesPerSecond = 13212.9
07/13/2016 04:32:32:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.18387146 * 2560; err = 0.36562500 * 2560; time = 0.1940s; samplesPerSecond = 13195.6
07/13/2016 04:32:32:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23827515 * 2560; err = 0.37148437 * 2560; time = 0.1947s; samplesPerSecond = 13148.5
07/13/2016 04:32:32:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.18418274 * 2560; err = 0.36328125 * 2560; time = 0.1949s; samplesPerSecond = 13132.2
07/13/2016 04:32:32:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.16851501 * 2560; err = 0.35234375 * 2560; time = 0.1945s; samplesPerSecond = 13158.7
07/13/2016 04:32:32:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14337463 * 2560; err = 0.34375000 * 2560; time = 0.1939s; samplesPerSecond = 13202.0
07/13/2016 04:32:33:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.17227478 * 2560; err = 0.34882812 * 2560; time = 0.1939s; samplesPerSecond = 13205.5
07/13/2016 04:32:33:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.18431091 * 2560; err = 0.36835937 * 2560; time = 0.1805s; samplesPerSecond = 14185.7
07/13/2016 04:32:33: Finished Epoch[ 1 of 2]: [Training] ce = 1.51252575 * 81920; err = 0.42452393 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=10.2645s
07/13/2016 04:32:33: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.1'

07/13/2016 04:32:33: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/13/2016 04:32:33: Starting minibatch loop.
07/13/2016 04:32:33:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.17448177 * 2560; err = 0.35195312 * 2560; time = 0.1967s; samplesPerSecond = 13015.8
07/13/2016 04:32:33:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.14536781 * 2560; err = 0.35664062 * 2560; time = 0.1952s; samplesPerSecond = 13112.3
07/13/2016 04:32:33:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.15722904 * 2560; err = 0.34531250 * 2560; time = 0.1940s; samplesPerSecond = 13197.0
07/13/2016 04:32:34:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.14344521 * 2560; err = 0.34804687 * 2560; time = 0.1935s; samplesPerSecond = 13228.4
07/13/2016 04:32:34:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.14842377 * 2560; err = 0.36562500 * 2560; time = 0.1938s; samplesPerSecond = 13209.9
07/13/2016 04:32:34:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14489059 * 2560; err = 0.34218750 * 2560; time = 0.1936s; samplesPerSecond = 13222.5
07/13/2016 04:32:34:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.09631195 * 2560; err = 0.33984375 * 2560; time = 0.1932s; samplesPerSecond = 13247.6
07/13/2016 04:32:34:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16026917 * 2560; err = 0.35546875 * 2560; time = 0.1932s; samplesPerSecond = 13253.1
07/13/2016 04:32:35:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.16528091 * 2560; err = 0.36015625 * 2560; time = 0.1938s; samplesPerSecond = 13211.9
07/13/2016 04:32:35:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.12257309 * 2560; err = 0.34492187 * 2560; time = 0.1935s; samplesPerSecond = 13229.8
07/13/2016 04:32:35:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.12313004 * 2560; err = 0.34765625 * 2560; time = 0.1939s; samplesPerSecond = 13205.3
07/13/2016 04:32:35:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.18492050 * 2560; err = 0.36171875 * 2560; time = 0.1941s; samplesPerSecond = 13187.3
07/13/2016 04:32:35:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.13058014 * 2560; err = 0.33476563 * 2560; time = 0.1937s; samplesPerSecond = 13218.4
07/13/2016 04:32:36:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16725922 * 2560; err = 0.35781250 * 2560; time = 0.1932s; samplesPerSecond = 13250.9
07/13/2016 04:32:36:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.12244568 * 2560; err = 0.34648438 * 2560; time = 0.1938s; samplesPerSecond = 13209.9
07/13/2016 04:32:36:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09480591 * 2560; err = 0.33671875 * 2560; time = 0.1937s; samplesPerSecond = 13214.3
07/13/2016 04:32:36:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.11218109 * 2560; err = 0.34140625 * 2560; time = 0.1939s; samplesPerSecond = 13201.3
07/13/2016 04:32:36:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11966095 * 2560; err = 0.33398438 * 2560; time = 0.1934s; samplesPerSecond = 13238.7
07/13/2016 04:32:37:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10485687 * 2560; err = 0.33671875 * 2560; time = 0.1931s; samplesPerSecond = 13259.1
07/13/2016 04:32:37:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.06019897 * 2560; err = 0.32617188 * 2560; time = 0.1932s; samplesPerSecond = 13250.4
07/13/2016 04:32:37:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.10600891 * 2560; err = 0.34101562 * 2560; time = 0.1935s; samplesPerSecond = 13232.6
07/13/2016 04:32:37:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.13724823 * 2560; err = 0.34101562 * 2560; time = 0.1940s; samplesPerSecond = 13198.5
07/13/2016 04:32:37:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12464600 * 2560; err = 0.34609375 * 2560; time = 0.1945s; samplesPerSecond = 13161.5
07/13/2016 04:32:38:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.10831604 * 2560; err = 0.33593750 * 2560; time = 0.1935s; samplesPerSecond = 13229.7
07/13/2016 04:32:38:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09707031 * 2560; err = 0.34023437 * 2560; time = 0.1927s; samplesPerSecond = 13281.9
07/13/2016 04:32:38:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.04812317 * 2560; err = 0.32773438 * 2560; time = 0.1936s; samplesPerSecond = 13223.9
07/13/2016 04:32:38:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.04979248 * 2560; err = 0.33398438 * 2560; time = 0.1945s; samplesPerSecond = 13162.0
07/13/2016 04:32:38:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.03223572 * 2560; err = 0.31835938 * 2560; time = 0.1936s; samplesPerSecond = 13223.8
07/13/2016 04:32:38:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.05677490 * 2560; err = 0.32773438 * 2560; time = 0.1936s; samplesPerSecond = 13222.7
07/13/2016 04:32:39:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10880737 * 2560; err = 0.34296875 * 2560; time = 0.1931s; samplesPerSecond = 13260.3
07/13/2016 04:32:39:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08513489 * 2560; err = 0.33476563 * 2560; time = 0.1935s; samplesPerSecond = 13230.0
07/13/2016 04:32:39:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04244080 * 2560; err = 0.31757812 * 2560; time = 0.1799s; samplesPerSecond = 14230.3
07/13/2016 04:32:39: Finished Epoch[ 2 of 2]: [Training] ce = 1.11484098 * 81920; err = 0.34190674 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=6.20709s
07/13/2016 04:32:39: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech'
07/13/2016 04:32:39: CNTKCommandTrainEnd: dptPre2

07/13/2016 04:32:39: Action "train" complete.


07/13/2016 04:32:39: ##############################################################################
07/13/2016 04:32:39: #                                                                            #
07/13/2016 04:32:39: # Action "edit"                                                              #
07/13/2016 04:32:39: #                                                                            #
07/13/2016 04:32:39: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


07/13/2016 04:32:40: Action "edit" complete.


07/13/2016 04:32:40: ##############################################################################
07/13/2016 04:32:40: #                                                                            #
07/13/2016 04:32:40: # Action "train"                                                             #
07/13/2016 04:32:40: #                                                                            #
07/13/2016 04:32:40: ##############################################################################

07/13/2016 04:32:40: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

07/13/2016 04:32:41: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/13/2016 04:32:42: Loaded model with 29 nodes on GPU 0.

07/13/2016 04:32:42: Training criterion node(s):
07/13/2016 04:32:42: 	ce = CrossEntropyWithSoftmax

07/13/2016 04:32:42: Evaluation criterion node(s):

07/13/2016 04:32:42: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
00000025D2387240: {[OL.t Gradient[132 x 1 x *6]] }
00000025D23877F0: {[OL.b Gradient[132 x 1]] }
00000025D2387A60: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
00000025E3F703E0: {[ce Gradient[1]] }
00000025E3F70580: {[scaledLogLikelihood Value[132 x 1 x *6]] }
00000025E3F70650: {[HL2.W Value[512 x 512]] }
00000025E3F707F0: {[labels Value[132 x *6]] }
00000025E3F708C0: {[OL.b Value[132 x 1]] }
00000025E3F70990: {[OL.W Value[132 x 512]] }
00000025E3F70A60: {[globalInvStd Value[363 x 1]] }
00000025E3F70B30: {[HL2.b Value[512 x 1]] }
00000025E3F70C00: {[HL3.W Value[512 x 512]] }
00000025E3F70CD0: {[ce Value[1]] }
00000025E3F70DA0: {[logPrior Value[132 x 1]] }
00000025E3F70E70: {[features Value[363 x *6]] }
00000025E3F70F40: {[featNorm Value[363 x *6]] }
00000025E3F71010: {[HL1.t Value[512 x *6]] }
00000025E3F710E0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
00000025E3F711B0: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
00000025E3F71350: {[globalMean Value[363 x 1]] }
00000025E3F71420: {[HL1.b Value[512 x 1]] }
00000025E3F714F0: {[HL1.W Value[512 x 363]] }
00000025E3F715C0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
00000025E3F71760: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
00000025E3F71830: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
00000025E3F71900: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
00000025E3F719D0: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
00000025E3F71AA0: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
00000025E3F71B70: {[globalPrior Value[132 x 1]] }
00000025E3F71C40: {[HL3.b Value[512 x 1]] }
00000025E3F71D10: {[err Value[1]] }
00000025E3F71DE0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
00000025E3F722C0: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }

07/13/2016 04:32:42: No PreCompute nodes found, skipping PreCompute step.

07/13/2016 04:32:42: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

07/13/2016 04:32:46: Starting minibatch loop.
07/13/2016 04:32:46:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.12455330 * 2560; err = 0.82734375 * 2560; time = 0.2386s; samplesPerSecond = 10729.9
07/13/2016 04:32:46:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.55599785 * 2560; err = 0.63007813 * 2560; time = 0.2276s; samplesPerSecond = 11249.2
07/13/2016 04:32:46:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.03516159 * 2560; err = 0.53945312 * 2560; time = 0.2279s; samplesPerSecond = 11235.3
07/13/2016 04:32:46:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.73739853 * 2560; err = 0.47500000 * 2560; time = 0.2278s; samplesPerSecond = 11239.9
07/13/2016 04:32:47:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.54207916 * 2560; err = 0.43515625 * 2560; time = 0.2279s; samplesPerSecond = 11231.8
07/13/2016 04:32:47:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.44409790 * 2560; err = 0.41328125 * 2560; time = 0.2287s; samplesPerSecond = 11196.0
07/13/2016 04:32:47:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.36059418 * 2560; err = 0.40898438 * 2560; time = 0.2271s; samplesPerSecond = 11272.5
07/13/2016 04:32:47:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35930023 * 2560; err = 0.40117188 * 2560; time = 0.2278s; samplesPerSecond = 11239.9
07/13/2016 04:32:48:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.34254303 * 2560; err = 0.38632813 * 2560; time = 0.2280s; samplesPerSecond = 11229.9
07/13/2016 04:32:48:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.30505676 * 2560; err = 0.38320312 * 2560; time = 0.2278s; samplesPerSecond = 11237.8
07/13/2016 04:32:48:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.30881348 * 2560; err = 0.38476563 * 2560; time = 0.2275s; samplesPerSecond = 11255.0
07/13/2016 04:32:48:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.23755188 * 2560; err = 0.37304688 * 2560; time = 0.2276s; samplesPerSecond = 11245.8
07/13/2016 04:32:49:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.21070251 * 2560; err = 0.35546875 * 2560; time = 0.2280s; samplesPerSecond = 11228.3
07/13/2016 04:32:49:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.24008789 * 2560; err = 0.37109375 * 2560; time = 0.2285s; samplesPerSecond = 11205.7
07/13/2016 04:32:49:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.23422089 * 2560; err = 0.36835937 * 2560; time = 0.2280s; samplesPerSecond = 11230.0
07/13/2016 04:32:49:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.19425964 * 2560; err = 0.35195312 * 2560; time = 0.2269s; samplesPerSecond = 11283.8
07/13/2016 04:32:49:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.21415710 * 2560; err = 0.36289063 * 2560; time = 0.2280s; samplesPerSecond = 11227.7
07/13/2016 04:32:50:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.24289856 * 2560; err = 0.37031250 * 2560; time = 0.2283s; samplesPerSecond = 11212.3
07/13/2016 04:32:50:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.26465759 * 2560; err = 0.38359375 * 2560; time = 0.2281s; samplesPerSecond = 11221.9
07/13/2016 04:32:50:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.22050476 * 2560; err = 0.38085938 * 2560; time = 0.2273s; samplesPerSecond = 11262.9
07/13/2016 04:32:50:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.17745056 * 2560; err = 0.35507813 * 2560; time = 0.2273s; samplesPerSecond = 11262.4
07/13/2016 04:32:51:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.19851379 * 2560; err = 0.37109375 * 2560; time = 0.2279s; samplesPerSecond = 11235.0
07/13/2016 04:32:51:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.21453857 * 2560; err = 0.35820313 * 2560; time = 0.2278s; samplesPerSecond = 11236.8
07/13/2016 04:32:51:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.18011475 * 2560; err = 0.35546875 * 2560; time = 0.2282s; samplesPerSecond = 11220.5
07/13/2016 04:32:51:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.16693726 * 2560; err = 0.35195312 * 2560; time = 0.2269s; samplesPerSecond = 11284.2
07/13/2016 04:32:51:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.12398987 * 2560; err = 0.35234375 * 2560; time = 0.2275s; samplesPerSecond = 11250.4
07/13/2016 04:32:52:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.18822021 * 2560; err = 0.36328125 * 2560; time = 0.2287s; samplesPerSecond = 11193.3
07/13/2016 04:32:52:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.13831482 * 2560; err = 0.35078125 * 2560; time = 0.2284s; samplesPerSecond = 11207.0
07/13/2016 04:32:52:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12718811 * 2560; err = 0.33984375 * 2560; time = 0.2274s; samplesPerSecond = 11258.6
07/13/2016 04:32:52:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.11155396 * 2560; err = 0.34179688 * 2560; time = 0.2272s; samplesPerSecond = 11266.6
07/13/2016 04:32:53:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.13423157 * 2560; err = 0.34101562 * 2560; time = 0.2282s; samplesPerSecond = 11220.7
07/13/2016 04:32:53:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12716675 * 2560; err = 0.34414062 * 2560; time = 0.2139s; samplesPerSecond = 11967.7
07/13/2016 04:32:53: Finished Epoch[ 1 of 4]: [Training] ce = 1.40821428 * 81920; err = 0.40085449 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=11.2521s
07/13/2016 04:32:53: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.1'

07/13/2016 04:32:53: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

07/13/2016 04:32:53: Starting minibatch loop.
07/13/2016 04:32:53:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.20089607 * 5120; err = 0.36757812 * 5120; time = 0.3526s; samplesPerSecond = 14518.7
07/13/2016 04:32:54:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.15295639 * 5120; err = 0.34550781 * 5120; time = 0.3395s; samplesPerSecond = 15083.0
07/13/2016 04:32:54:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.09945831 * 5120; err = 0.33613281 * 5120; time = 0.3378s; samplesPerSecond = 15159.0
07/13/2016 04:32:54:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.09916496 * 5120; err = 0.33867188 * 5120; time = 0.3375s; samplesPerSecond = 15172.3
07/13/2016 04:32:55:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.17260475 * 5120; err = 0.36230469 * 5120; time = 0.3376s; samplesPerSecond = 15166.9
07/13/2016 04:32:55:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.15717964 * 5120; err = 0.35820313 * 5120; time = 0.3374s; samplesPerSecond = 15175.5
07/13/2016 04:32:55:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.14431229 * 5120; err = 0.34296875 * 5120; time = 0.3374s; samplesPerSecond = 15174.6
07/13/2016 04:32:56:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.10515671 * 5120; err = 0.34394531 * 5120; time = 0.3383s; samplesPerSecond = 15133.2
07/13/2016 04:32:56:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.15175400 * 5120; err = 0.35449219 * 5120; time = 0.3366s; samplesPerSecond = 15212.3
07/13/2016 04:32:56:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.11654053 * 5120; err = 0.34101562 * 5120; time = 0.3379s; samplesPerSecond = 15151.8
07/13/2016 04:32:57:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.11851807 * 5120; err = 0.34472656 * 5120; time = 0.3376s; samplesPerSecond = 15167.0
07/13/2016 04:32:57:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.11374130 * 5120; err = 0.34492187 * 5120; time = 0.3375s; samplesPerSecond = 15172.3
07/13/2016 04:32:58:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.04686737 * 5120; err = 0.32265625 * 5120; time = 0.3374s; samplesPerSecond = 15175.7
07/13/2016 04:32:58:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02721252 * 5120; err = 0.32246094 * 5120; time = 0.3379s; samplesPerSecond = 15152.4
07/13/2016 04:32:58:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.08386230 * 5120; err = 0.33144531 * 5120; time = 0.3374s; samplesPerSecond = 15174.5
07/13/2016 04:32:58:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06164856 * 5120; err = 0.32558594 * 5120; time = 0.3080s; samplesPerSecond = 16622.4
07/13/2016 04:32:58: Finished Epoch[ 2 of 4]: [Training] ce = 1.11574211 * 81920; err = 0.34266357 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=5.42593s
07/13/2016 04:32:59: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.2'

07/13/2016 04:32:59: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

07/13/2016 04:32:59: Starting minibatch loop.
07/13/2016 04:32:59:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.12331724 * 5120; err = 0.34121094 * 5120; time = 0.3280s; samplesPerSecond = 15609.7
07/13/2016 04:32:59:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.07871103 * 5120; err = 0.33652344 * 5120; time = 0.3270s; samplesPerSecond = 15655.1
07/13/2016 04:33:00:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.06784973 * 5120; err = 0.33183594 * 5120; time = 0.3273s; samplesPerSecond = 15643.6
07/13/2016 04:33:00:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.08440666 * 5120; err = 0.33398438 * 5120; time = 0.3270s; samplesPerSecond = 15657.5
07/13/2016 04:33:00:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07466774 * 5120; err = 0.33320312 * 5120; time = 0.3285s; samplesPerSecond = 15583.7
07/13/2016 04:33:01:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.05427513 * 5120; err = 0.33125000 * 5120; time = 0.3286s; samplesPerSecond = 15579.9
07/13/2016 04:33:01:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06873093 * 5120; err = 0.32773438 * 5120; time = 0.3274s; samplesPerSecond = 15637.1
07/13/2016 04:33:01:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.08097610 * 5120; err = 0.33007813 * 5120; time = 0.3280s; samplesPerSecond = 15610.3
07/13/2016 04:33:02:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.05431290 * 5120; err = 0.32792969 * 5120; time = 0.3275s; samplesPerSecond = 15633.7
07/13/2016 04:33:02:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.06173096 * 5120; err = 0.32695313 * 5120; time = 0.3275s; samplesPerSecond = 15633.2
07/13/2016 04:33:02:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.04505692 * 5120; err = 0.32792969 * 5120; time = 0.3276s; samplesPerSecond = 15628.1
07/13/2016 04:33:03:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.08151245 * 5120; err = 0.33574219 * 5120; time = 0.3276s; samplesPerSecond = 15630.7
07/13/2016 04:33:03:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.10628204 * 5120; err = 0.33437500 * 5120; time = 0.3279s; samplesPerSecond = 15615.3
07/13/2016 04:33:03:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.05827026 * 5120; err = 0.32636719 * 5120; time = 0.3276s; samplesPerSecond = 15627.1
07/13/2016 04:33:04:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05841064 * 5120; err = 0.33574219 * 5120; time = 0.3279s; samplesPerSecond = 15612.9
07/13/2016 04:33:04:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.04437714 * 5120; err = 0.32773438 * 5120; time = 0.2992s; samplesPerSecond = 17110.4
07/13/2016 04:33:04: Finished Epoch[ 3 of 4]: [Training] ce = 1.07143049 * 81920; err = 0.33178711 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=5.25064s
07/13/2016 04:33:04: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.3'

07/13/2016 04:33:04: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

07/13/2016 04:33:04: Starting minibatch loop.
07/13/2016 04:33:05:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.04450397 * 5120; err = 0.33125000 * 5120; time = 0.3295s; samplesPerSecond = 15537.0
07/13/2016 04:33:07:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.02895847 * 4926; err = 0.31567194 * 4926; time = 2.8909s; samplesPerSecond = 1704.0
07/13/2016 04:33:08:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.00198059 * 5120; err = 0.31601563 * 5120; time = 0.3288s; samplesPerSecond = 15572.2
07/13/2016 04:33:08:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.00561523 * 5120; err = 0.31777344 * 5120; time = 0.3289s; samplesPerSecond = 15567.0
07/13/2016 04:33:08:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.00148926 * 5120; err = 0.31601563 * 5120; time = 0.3276s; samplesPerSecond = 15628.7
07/13/2016 04:33:09:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.00593338 * 5120; err = 0.31406250 * 5120; time = 0.3288s; samplesPerSecond = 15573.6
07/13/2016 04:33:09:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.98752327 * 5120; err = 0.30722656 * 5120; time = 0.3283s; samplesPerSecond = 15596.5
07/13/2016 04:33:09:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.01428757 * 5120; err = 0.31992188 * 5120; time = 0.3279s; samplesPerSecond = 15615.7
07/13/2016 04:33:10:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99691544 * 5120; err = 0.31621094 * 5120; time = 0.3282s; samplesPerSecond = 15598.0
07/13/2016 04:33:10:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96604996 * 5120; err = 0.30937500 * 5120; time = 0.3285s; samplesPerSecond = 15587.3
07/13/2016 04:33:10:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 0.99062958 * 5120; err = 0.30527344 * 5120; time = 0.3274s; samplesPerSecond = 15636.1
07/13/2016 04:33:11:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.99886856 * 5120; err = 0.30976562 * 5120; time = 0.3285s; samplesPerSecond = 15587.8
07/13/2016 04:33:11:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.00958405 * 5120; err = 0.31523438 * 5120; time = 0.3284s; samplesPerSecond = 15592.8
07/13/2016 04:33:11:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97942047 * 5120; err = 0.31171875 * 5120; time = 0.3279s; samplesPerSecond = 15615.4
07/13/2016 04:33:12:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.94226837 * 5120; err = 0.30136719 * 5120; time = 0.3280s; samplesPerSecond = 15611.0
07/13/2016 04:33:12:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.96711578 * 5120; err = 0.30175781 * 5120; time = 0.3073s; samplesPerSecond = 16663.4
07/13/2016 04:33:12: Finished Epoch[ 4 of 4]: [Training] ce = 0.99611807 * 81920; err = 0.31303711 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=7.84612s
07/13/2016 04:33:12: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Speech\DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech'
07/13/2016 04:33:12: CNTKCommandTrainEnd: speechTrain

07/13/2016 04:33:12: Action "train" complete.

07/13/2016 04:33:12: __COMPLETED__