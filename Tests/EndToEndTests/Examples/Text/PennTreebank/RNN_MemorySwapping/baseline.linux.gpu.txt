=== Running /home/tim/git/cntk_dev/build/debug/bin/cntk configFile=/home/tim/git/cntk_dev/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN_MemorySwapping/../../../../../../Examples/Text/PennTreebank/Config/rnn.cntk currentDirectory=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data RunDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu DataDir=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data ConfigDir=/home/tim/git/cntk_dev/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN_MemorySwapping/../../../../../../Examples/Text/PennTreebank/Config OutputDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu DeviceId=0 timestamping=true useMemorySwapping=true initOnCPUOnly=true command=writeWordAndClassInfo:train:test train=[SGD=[maxEpochs=3]] train=[epochSize=2048]] test=[SGD=[maxEpochs=3]] train=[epochSize=2048]]
-------------------------------------------------------------------
Build info: 

		Built time: Jul 21 2016 11:46:07
		Last modified date: Wed Jul 20 17:25:59 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: yes
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: t-tidett/memoryswapping
		Build SHA1: 41c73aa94ec96c8f0639a328d4888e1d82751ac9 (modified)
		Built by tim on tim
		Build Path: /home/tim/git/cntk_dev
-------------------------------------------------------------------
Changed current directory to /home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data
07/21/2016 12:12:16: -------------------------------------------------------------------
07/21/2016 12:12:16: Build info: 

07/21/2016 12:12:16: 		Built time: Jul 21 2016 11:46:07
07/21/2016 12:12:16: 		Last modified date: Wed Jul 20 17:25:59 2016
07/21/2016 12:12:16: 		Build type: debug
07/21/2016 12:12:16: 		Build target: GPU
07/21/2016 12:12:16: 		With 1bit-SGD: yes
07/21/2016 12:12:16: 		Math lib: acml
07/21/2016 12:12:16: 		CUDA_PATH: /usr/local/cuda-7.5
07/21/2016 12:12:16: 		CUB_PATH: /usr/local/cub-1.4.1
07/21/2016 12:12:16: 		CUDNN_PATH: /usr/local/cudnn-4.0
07/21/2016 12:12:16: 		Build Branch: t-tidett/memoryswapping
07/21/2016 12:12:16: 		Build SHA1: 41c73aa94ec96c8f0639a328d4888e1d82751ac9 (modified)
07/21/2016 12:12:16: 		Built by tim on tim
07/21/2016 12:12:16: 		Build Path: /home/tim/git/cntk_dev
07/21/2016 12:12:16: -------------------------------------------------------------------

07/21/2016 12:12:16: Running on localhost at 2016/07/21 12:12:16
07/21/2016 12:12:16: Command line: 
/home/tim/git/cntk_dev/build/debug/bin/cntk  configFile=/home/tim/git/cntk_dev/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN_MemorySwapping/../../../../../../Examples/Text/PennTreebank/Config/rnn.cntk  currentDirectory=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data  RunDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu  DataDir=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data  ConfigDir=/home/tim/git/cntk_dev/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN_MemorySwapping/../../../../../../Examples/Text/PennTreebank/Config  OutputDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu  DeviceId=0  timestamping=true  useMemorySwapping=true  initOnCPUOnly=true  command=writeWordAndClassInfo:train:test  train=[SGD=[maxEpochs=3]]  train=[epochSize=2048]]  test=[SGD=[maxEpochs=3]]  train=[epochSize=2048]]



07/21/2016 12:12:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
07/21/2016 12:12:16: RootDir = ".."
ConfigDir = "$RootDir$/Config"
DataDir   = "$RootDir$/Data"
OutputDir = "$RootDir$/Output"
ModelDir  = "$OutputDir$/Models"
deviceId = "auto"
command = writeWordAndClassInfo:train:test:write
precision  = "float"
traceLevel = 1
modelPath  = "$ModelDir$/rnn.dnn"
numCPUThreads = 1
confVocabSize = 10000
confClassSize = 50
trainFile = "ptb.train.txt"
validFile = "ptb.valid.txt"
testFile  = "ptb.test.txt"
writeWordAndClassInfo = [
    action = "writeWordAndClass"
    inputFile = "$DataDir$/$trainFile$"
    beginSequence = "</s>"
    endSequence   = "</s>"
    outputVocabFile = "$ModelDir$/vocab.txt"
    outputWord2Cls  = "$ModelDir$/word2cls.txt"
    outputCls2Index = "$ModelDir$/cls2idx.txt"
    vocabSize = "$confVocabSize$"
    nbrClass = "$confClassSize$"
    cutoff = 0
    printValues = true
]
train = [
    action = "train"
    traceLevel = 1
epochSize = 0               
    SimpleNetworkBuilder = [
rnnType = "CLASSLSTM"   
recurrentLayer = 1      
        trainingCriterion = "classCrossEntropyWithSoftmax"
        evalCriterion     = "classCrossEntropyWithSoftmax"
        initValueScale = 6.0
        uniformInit = true
        layerSizes = "$confVocabSize$:150:200:10000"
defaultHiddenActivity = 0.1 
        addPrior = false
        addDropoutNodes = false
        applyMeanVarNorm = false
lookupTableOrder = 1        
        vocabSize = "$confVocabSize$"
        nbrClass  = "$confClassSize$"
    ]
    SGD = [
        minibatchSize = 128:256:512
        learningRatesPerSample = 0.1
        momentumPerMB = 0
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0
        maxEpochs = 16
        numMBsToShowResult = 100
        gradUpdateType = "none"
        loadBestModel = true
        dropoutRate = 0.0
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$trainFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11                
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = categoryLabels
            ]
        ]
    ]
    cvReader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.valid.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$validFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
test = [
    action = "eval"
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$testFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
write = [
    action = "write"
    outputPath = "$OutputDir$/Write"
outputNodeNames = TrainNodeClassBasedCrossEntropy 
    format = [
sequencePrologue = "log P(W)="    
        type = "real"
    ]
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 1  
cacheBlockSize = 1              
        wordclass = "$ModelDir$/vocab.txt"
        wfile = "$OutputDir$/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "$confVocabSize$"
        file = "$DataDir$/$testFile$"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "$confVocabSize$"
            labelMappingFile = "$OutputDir$/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
currentDirectory=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data
RunDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu
DataDir=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data
ConfigDir=/home/tim/git/cntk_dev/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN_MemorySwapping/../../../../../../Examples/Text/PennTreebank/Config
OutputDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu
DeviceId=0
timestamping=true
useMemorySwapping=true
initOnCPUOnly=true
command=writeWordAndClassInfo:train:test
train=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]
test=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]

07/21/2016 12:12:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

07/21/2016 12:12:16: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
07/21/2016 12:12:16: RootDir = ".."
ConfigDir = "../Config"
DataDir   = "../Data"
OutputDir = "../Output"
ModelDir  = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models"
deviceId = "auto"
command = writeWordAndClassInfo:train:test:write
precision  = "float"
traceLevel = 1
modelPath  = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn"
numCPUThreads = 1
confVocabSize = 10000
confClassSize = 50
trainFile = "ptb.train.txt"
validFile = "ptb.valid.txt"
testFile  = "ptb.test.txt"
writeWordAndClassInfo = [
    action = "writeWordAndClass"
    inputFile = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.train.txt"
    beginSequence = "</s>"
    endSequence   = "</s>"
    outputVocabFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
    outputWord2Cls  = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/word2cls.txt"
    outputCls2Index = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/cls2idx.txt"
    vocabSize = "10000"
    nbrClass = "50"
    cutoff = 0
    printValues = true
]
train = [
    action = "train"
    traceLevel = 1
epochSize = 0               
    SimpleNetworkBuilder = [
rnnType = "CLASSLSTM"   
recurrentLayer = 1      
        trainingCriterion = "classCrossEntropyWithSoftmax"
        evalCriterion     = "classCrossEntropyWithSoftmax"
        initValueScale = 6.0
        uniformInit = true
        layerSizes = "10000:150:200:10000"
defaultHiddenActivity = 0.1 
        addPrior = false
        addDropoutNodes = false
        applyMeanVarNorm = false
lookupTableOrder = 1        
        vocabSize = "10000"
        nbrClass  = "50"
    ]
    SGD = [
        minibatchSize = 128:256:512
        learningRatesPerSample = 0.1
        momentumPerMB = 0
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0
        maxEpochs = 16
        numMBsToShowResult = 100
        gradUpdateType = "none"
        loadBestModel = true
        dropoutRate = 0.0
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.train.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11                
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = categoryLabels
            ]
        ]
    ]
    cvReader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.valid.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.valid.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
test = [
    action = "eval"
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
write = [
    action = "write"
    outputPath = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Write"
outputNodeNames = TrainNodeClassBasedCrossEntropy 
    format = [
sequencePrologue = "log P(W)="    
        type = "real"
    ]
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 1  
cacheBlockSize = 1              
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]
currentDirectory=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data
RunDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu
DataDir=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data
ConfigDir=/home/tim/git/cntk_dev/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN_MemorySwapping/../../../../../../Examples/Text/PennTreebank/Config
OutputDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu
DeviceId=0
timestamping=true
useMemorySwapping=true
initOnCPUOnly=true
command=writeWordAndClassInfo:train:test
train=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]
test=[SGD=[maxEpochs=3]]
train=[epochSize=2048]]

07/21/2016 12:12:16: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

07/21/2016 12:12:16: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: rnn.cntk:]=true
configparameters: rnn.cntk:command=writeWordAndClassInfo:train:test
configparameters: rnn.cntk:confClassSize=50
configparameters: rnn.cntk:ConfigDir=/home/tim/git/cntk_dev/Tests/EndToEndTests/Examples/Text/PennTreebank/RNN_MemorySwapping/../../../../../../Examples/Text/PennTreebank/Config
configparameters: rnn.cntk:confVocabSize=10000
configparameters: rnn.cntk:currentDirectory=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data
configparameters: rnn.cntk:DataDir=/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data
configparameters: rnn.cntk:deviceId=0
configparameters: rnn.cntk:initOnCPUOnly=true
configparameters: rnn.cntk:ModelDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models
configparameters: rnn.cntk:modelPath=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn
configparameters: rnn.cntk:numCPUThreads=1
configparameters: rnn.cntk:OutputDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu
configparameters: rnn.cntk:precision=float
configparameters: rnn.cntk:RootDir=..
configparameters: rnn.cntk:RunDir=/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu
configparameters: rnn.cntk:test=[
    action = "eval"
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
] [SGD=[maxEpochs=3]]

configparameters: rnn.cntk:testFile=ptb.test.txt
configparameters: rnn.cntk:timestamping=true
configparameters: rnn.cntk:traceLevel=1
configparameters: rnn.cntk:train=[
    action = "train"
    traceLevel = 1
epochSize = 0               
    SimpleNetworkBuilder = [
rnnType = "CLASSLSTM"   
recurrentLayer = 1      
        trainingCriterion = "classCrossEntropyWithSoftmax"
        evalCriterion     = "classCrossEntropyWithSoftmax"
        initValueScale = 6.0
        uniformInit = true
        layerSizes = "10000:150:200:10000"
defaultHiddenActivity = 0.1 
        addPrior = false
        addDropoutNodes = false
        applyMeanVarNorm = false
lookupTableOrder = 1        
        vocabSize = "10000"
        nbrClass  = "50"
    ]
    SGD = [
        minibatchSize = 128:256:512
        learningRatesPerSample = 0.1
        momentumPerMB = 0
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0
        maxEpochs = 16
        numMBsToShowResult = 100
        gradUpdateType = "none"
        loadBestModel = true
        dropoutRate = 0.0
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.train.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11                
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = categoryLabels
            ]
        ]
    ]
    cvReader = [
        readerType = "LMSequenceReader"
        randomize = "none"
nbruttsineachrecurrentiter = 0  
cacheBlockSize = 2000000        
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.valid.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.valid.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
] [SGD=[maxEpochs=3]] [epochSize=2048] [epochSize=2048]

configparameters: rnn.cntk:trainFile=ptb.train.txt
configparameters: rnn.cntk:useMemorySwapping=true
configparameters: rnn.cntk:validFile=ptb.valid.txt
configparameters: rnn.cntk:write=[
    action = "write"
    outputPath = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Write"
outputNodeNames = TrainNodeClassBasedCrossEntropy 
    format = [
sequencePrologue = "log P(W)="    
        type = "real"
    ]
minibatchSize = 8192                
    traceLevel = 1
    epochSize = 0
    reader = [
        readerType = "LMSequenceReader"
randomize = "none"              
nbruttsineachrecurrentiter = 1  
cacheBlockSize = 1              
        wordclass = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
        wfile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sequenceSentence.bin"
        wsize = 256
        wrecords = 1000
        windowSize = "10000"
        file = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.test.txt"
        features = [
            dim = 0
            sectionType = "data"
        ]
        labelIn = [
            dim = 1
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt"
            labelType = "Category"
            beginSequence = "</s>"
            endSequence = "</s>"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 11
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 11
                sectionType = "categoryLabels"
            ]
        ]
        labels = [
            dim = 1
            labelType = "NextWord"
            beginSequence = "O"
            endSequence = "O"
            labelDim = "10000"
            labelMappingFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt"
            elementSize = 4
            sectionType = "labels"
            mapping = [
                wrecords = 3
                elementSize = 10
                sectionType = "labelMapping"
            ]
            category = [
                dim = 3
                sectionType = "categoryLabels"
            ]
        ]
    ]
]

configparameters: rnn.cntk:writeWordAndClassInfo=[
    action = "writeWordAndClass"
    inputFile = "/home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.train.txt"
    beginSequence = "</s>"
    endSequence   = "</s>"
    outputVocabFile = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt"
    outputWord2Cls  = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/word2cls.txt"
    outputCls2Index = "/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/cls2idx.txt"
    vocabSize = "10000"
    nbrClass = "50"
    cutoff = 0
    printValues = true
]

07/21/2016 12:12:16: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
07/21/2016 12:12:16: Commands: writeWordAndClassInfo train test
07/21/2016 12:12:16: Precision = "float"
07/21/2016 12:12:16: Using 1 CPU threads.
07/21/2016 12:12:16: CNTKModelPath: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn
07/21/2016 12:12:16: CNTKCommandTrainInfo: train : 3
07/21/2016 12:12:16: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3

07/21/2016 12:12:16: ##############################################################################
07/21/2016 12:12:16: #                                                                            #
07/21/2016 12:12:16: # Action "writeWordAndClass"                                                 #
07/21/2016 12:12:16: #                                                                            #
07/21/2016 12:12:16: ##############################################################################

Vocabulary file    --> /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/vocab.txt
Word-to-class map  --> /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/word2cls.txt
Class-to-index map --> /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/cls2idx.txt

Reading input file inputFile: /home/tim/git/cntk_dev/Examples/Text/PennTreebank/Data/ptb.train.txt
Vocabulary size 10000.
Created vocabulary file with 10000 entries.
Created word-to-class map with 10000 entries.
Created class-to-index map with 50 entries.

07/21/2016 12:12:17: Action "writeWordAndClass" complete.


07/21/2016 12:12:17: ##############################################################################
07/21/2016 12:12:17: #                                                                            #
07/21/2016 12:12:17: # Action "train"                                                             #
07/21/2016 12:12:17: #                                                                            #
07/21/2016 12:12:17: ##############################################################################

07/21/2016 12:12:17: CNTKCommandTrainBegin: train
SimpleNetworkBuilder Using GPU 0
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt

07/21/2016 12:12:18: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	PosteriorProb = Softmax()
	TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax()
	outputs = TransposeTimes()

Loop[0] --> Loop_AutoName37 -> 31 nodes

	AutoName2	AutoName30	AutoName33
	AutoName1	AutoName21	AutoName24
	AutoName5	AutoName20	AutoName25
	AutoName26	AutoName6	AutoName27
	AutoName0	AutoName8	AutoName11
	AutoName4	AutoName7	AutoName12
	AutoName13	AutoName3	AutoName14
	AutoName15	AutoName17	AutoName18
	AutoName19	AutoName28	AutoName29
	AutoName34	AutoName35	AutoName36
	AutoName37

Validating network. 63 nodes to process in pass 1.

Validating --> W2 = LearnableParameter() :  -> [200 x 10000]
Validating --> WXO0 = LearnableParameter() :  -> [200 x 150]
Validating --> E0 = LearnableParameter() :  -> [150 x 10000]
Validating --> features = SparseInputValue() :  -> [10000 x *]
Validating --> LookupTable = LookupTable (E0, features) : [150 x 10000], [10000 x *] -> [150 x *]
Validating --> AutoName31 = Times (WXO0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bo0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName32 = Plus (AutoName31, bo0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHO0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCO0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXF0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName22 = Times (WXF0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bf0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName23 = Plus (AutoName22, bf0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHF0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCF0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXI0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName9 = Times (WXI0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> bi0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName10 = Plus (AutoName9, bi0) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> WHI0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCI0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXC0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName16 = Times (WXC0, LookupTable) : [200 x 150], [150 x *] -> [200 x *]
Validating --> WHC0 = LearnableParameter() :  -> [200 x 200]
Validating --> bc0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName30 = Times (WHO0, AutoName2) : [200 x 200], [200] -> [200]
Validating --> AutoName33 = Plus (AutoName32, AutoName30) : [200 x 1 x *], [200] -> [200 x 1 x *]
Validating --> AutoName21 = Times (WHF0, AutoName1) : [200 x 200], [200] -> [200]
Validating --> AutoName24 = Plus (AutoName23, AutoName21) : [200 x 1 x *], [200] -> [200 x 1 x *]
Validating --> AutoName20 = DiagTimes (WCF0, AutoName5) : [200 x 1], [200] -> [200]
Validating --> AutoName25 = Plus (AutoName24, AutoName20) : [200 x 1 x *], [200] -> [200 x 1 x *]
Validating --> AutoName26 = Sigmoid (AutoName25) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName27 = ElementTimes (AutoName26, AutoName6) : [200 x 1 x *], [200] -> [200 x 1 x *]
Validating --> AutoName8 = Times (WHI0, AutoName0) : [200 x 200], [200] -> [200]
Validating --> AutoName11 = Plus (AutoName10, AutoName8) : [200 x 1 x *], [200] -> [200 x 1 x *]
Validating --> AutoName7 = DiagTimes (WCI0, AutoName4) : [200 x 1], [200] -> [200]
Validating --> AutoName12 = Plus (AutoName11, AutoName7) : [200 x 1 x *], [200] -> [200 x 1 x *]
Validating --> AutoName13 = Sigmoid (AutoName12) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName14 = Times (WHC0, AutoName3) : [200 x 200], [200] -> [200]
Validating --> AutoName15 = Plus (AutoName14, bc0) : [200], [200 x 1] -> [200 x 1]
Validating --> AutoName17 = Plus (AutoName16, AutoName15) : [200 x *], [200 x 1] -> [200 x 1 x *]
Validating --> AutoName18 = Tanh (AutoName17) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName19 = ElementTimes (AutoName13, AutoName18) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName28 = Plus (AutoName27, AutoName19) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName29 = DiagTimes (WCO0, AutoName28) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName34 = Plus (AutoName33, AutoName29) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName35 = Sigmoid (AutoName34) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName36 = Tanh (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName37 = ElementTimes (AutoName35, AutoName36) : [200 x 1 x *], [200 x 1 x *] -> [200 x 1 x *]
Validating --> outputs = TransposeTimes (W2, AutoName37) : [200 x 10000], [200 x 1 x *] -> [10000 x 1 x *]
Validating --> PosteriorProb = Softmax (outputs) : [10000 x 1 x *] -> [10000 x 1 x *]
Validating --> labels = InputValue() :  -> [4 x *]
Validating --> WeightForClassPostProb = LearnableParameter() :  -> [50 x 200]
Validating --> ClassPostProb = Times (WeightForClassPostProb, AutoName37) : [50 x 200], [200 x 1 x *] -> [50 x 1 x *]
Validating --> TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax (labels, AutoName37, W2, ClassPostProb) : [4 x *], [200 x 1 x *], [200 x 10000], [50 x 1 x *] -> [1]

Validating network. 43 nodes to process in pass 2.

Validating --> AutoName2 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName30 = Times (WHO0, AutoName2) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName1 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName21 = Times (WHF0, AutoName1) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName5 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName20 = DiagTimes (WCF0, AutoName5) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName6 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName0 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName8 = Times (WHI0, AutoName0) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName4 = PastValue (AutoName28) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName7 = DiagTimes (WCI0, AutoName4) : [200 x 1], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName3 = PastValue (AutoName37) : [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName14 = Times (WHC0, AutoName3) : [200 x 200], [200 x 1 x *] -> [200 x 1 x *]
Validating --> AutoName15 = Plus (AutoName14, bc0) : [200 x 1 x *], [200 x 1] -> [200 x 1 x *]

Validating network. 14 nodes to process in pass 3.


Validating network, final pass.



19 out of 63 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

07/21/2016 12:12:18: Created model with 63 nodes on GPU 0.

07/21/2016 12:12:18: Training criterion node(s):
07/21/2016 12:12:18: 	TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[PosteriorProb Gradient[10000 x 1 x *]] [PosteriorProb Value[10000 x 1 x *]] [features Gradient[10000 x *]] [labels Gradient[4 x *]] [outputs Gradient[10000 x 1 x *]] }
0x2a86fb8: {[features Value[10000 x *]] }
0x3bcf158: {[E0 Value[150 x 10000]] }
0x42444a8: {[WXO0 Value[200 x 150]] }
0x4245928: {[WXI0 Value[200 x 150]] }
0x42460a8: {[WXF0 Value[200 x 150]] }
0x48e7d68: {[WXC0 Value[200 x 150]] }
0x48e8628: {[bo0 Value[200 x 1]] }
0x48e9468: {[bc0 Value[200 x 1]] }
0x48e9dc8: {[bi0 Value[200 x 1]] }
0x48ea758: {[bf0 Value[200 x 1]] }
0x48eb0f8: {[WHI0 Value[200 x 200]] }
0x48ebc28: {[WCI0 Value[200 x 1]] }
0x48ec568: {[WHF0 Value[200 x 200]] }
0x48ed408: {[WCF0 Value[200 x 1]] }
0x48edda8: {[WHO0 Value[200 x 200]] }
0x48ee748: {[WCO0 Value[200 x 1]] }
0x48ef0e8: {[WHC0 Value[200 x 200]] }
0x48efd58: {[AutoName0 Value[200 x 1 x *]] }
0x48f03c8: {[AutoName1 Value[200 x 1 x *]] }
0x48f0b28: {[AutoName2 Value[200 x 1 x *]] [WXC0 Gradient[200 x 150]] }
0x48f1148: {[AutoName3 Value[200 x 1 x *]] }
0x48f18a8: {[AutoName4 Value[200 x 1 x *]] }
0x48f1f08: {[AutoName5 Value[200 x 1 x *]] }
0x48f2668: {[AutoName6 Value[200 x 1 x *]] }
0x48f8c18: {[W2 Value[200 x 10000]] }
0x48f9888: {[labels Value[4 x *]] }
0x48fa558: {[WeightForClassPostProb Value[50 x 200]] }
0x48fcdd8: {[outputs Value[10000 x 1 x *]] }
0x48fda48: {[AutoName22 Value[200 x *]] [AutoName31 Gradient[200 x *]] }
0x4900728: {[AutoName32 Value[200 x 1 x *]] [WXO0 Gradient[200 x 150]] }
0x4901578: {[TrainNodeClassBasedCrossEntropy Value[1]] }
0x49033d8: {[AutoName10 Value[200 x 1 x *]] [WXI0 Gradient[200 x 150]] }
0x4903598: {[AutoName16 Value[200 x *]] [AutoName9 Gradient[200 x *]] [bf0 Gradient[200 x 1]] }
0x4903758: {[AutoName30 Value[200 x 1 x *]] }
0x4908818: {[LookupTable Value[150 x *]] }
0x4908c08: {[AutoName31 Value[200 x *]] [E0 Gradient[150 x 10000]] }
0x4909538: {[AutoName23 Value[200 x 1 x *]] [WXF0 Gradient[200 x 150]] }
0x49096f8: {[AutoName22 Gradient[200 x *]] [AutoName9 Value[200 x *]] [bo0 Gradient[200 x 1]] }
0x4909d18: {[AutoName33 Value[200 x 1 x *]] }
0x4909e78: {[AutoName21 Value[200 x 1 x *]] }
0x4909fd8: {[AutoName24 Value[200 x 1 x *]] }
0x490a138: {[AutoName20 Value[200 x 1 x *]] }
0x490a298: {[AutoName25 Value[200 x 1 x *]] }
0x490a3f8: {[AutoName26 Value[200 x 1 x *]] }
0x490a558: {[AutoName27 Value[200 x 1 x *]] }
0x490a6b8: {[AutoName8 Value[200 x 1 x *]] }
0x490a818: {[AutoName11 Value[200 x 1 x *]] }
0x490a978: {[AutoName7 Value[200 x 1 x *]] }
0x490aad8: {[AutoName12 Value[200 x 1 x *]] }
0x490ac38: {[AutoName13 Value[200 x 1 x *]] }
0x490adf8: {[AutoName14 Value[200 x 1 x *]] }
0x490afb8: {[AutoName15 Value[200 x 1 x *]] }
0x490b178: {[AutoName17 Value[200 x 1 x *]] }
0x490b338: {[AutoName18 Value[200 x 1 x *]] }
0x490b4f8: {[AutoName19 Value[200 x 1 x *]] }
0x490b6b8: {[AutoName28 Value[200 x 1 x *]] }
0x490b878: {[AutoName29 Value[200 x 1 x *]] }
0x490ba38: {[AutoName34 Value[200 x 1 x *]] }
0x490bbf8: {[AutoName35 Value[200 x 1 x *]] }
0x490bdb8: {[AutoName36 Value[200 x 1 x *]] }
0x490bf78: {[AutoName37 Value[200 x 1 x *]] }
0x490c138: {[AutoName35 Gradient[200 x 1 x *]] [ClassPostProb Value[50 x 1 x *]] }
0x490c2f8: {[TrainNodeClassBasedCrossEntropy Gradient[1]] }
0x490c4b8: {[AutoName37 Gradient[200 x 1 x *]] }
0x490c678: {[W2 Gradient[200 x 10000]] }
0x490c838: {[AutoName36 Gradient[200 x 1 x *]] [ClassPostProb Gradient[50 x 1 x *]] }
0x490c9f8: {[WeightForClassPostProb Gradient[50 x 200]] }
0x490cbb8: {[AutoName28 Gradient[200 x 1 x *]] }
0x490cd78: {[AutoName34 Gradient[200 x 1 x *]] }
0x490cf38: {[AutoName33 Gradient[200 x 1 x *]] }
0x490d0f8: {[AutoName29 Gradient[200 x 1 x *]] }
0x490d638: {[WCO0 Gradient[200 x 1]] }
0x490d7f8: {[AutoName27 Gradient[200 x 1 x *]] }
0x490d9b8: {[AutoName19 Gradient[200 x 1 x *]] }
0x490db78: {[AutoName13 Gradient[200 x 1 x *]] }
0x490dd38: {[AutoName18 Gradient[200 x 1 x *]] }
0x490def8: {[AutoName17 Gradient[200 x 1 x *]] }
0x490e0b8: {[AutoName16 Gradient[200 x *]] [bi0 Gradient[200 x 1]] }
0x490e278: {[AutoName15 Gradient[200 x 1 x *]] }
0x490e438: {[AutoName14 Gradient[200 x 1 x *]] }
0x490e5f8: {[bc0 Gradient[200 x 1]] }
0x490e7b8: {[WHC0 Gradient[200 x 200]] }
0x490e978: {[AutoName3 Gradient[200 x 1 x *]] }
0x490eb38: {[AutoName12 Gradient[200 x 1 x *]] }
0x490ecf8: {[AutoName11 Gradient[200 x 1 x *]] }
0x490eeb8: {[AutoName7 Gradient[200 x 1 x *]] }
0x490f3f8: {[WCI0 Gradient[200 x 1]] }
0x490f5b8: {[AutoName4 Gradient[200 x 1 x *]] }
0x490f778: {[AutoName10 Gradient[200 x 1 x *]] }
0x490f938: {[AutoName8 Gradient[200 x 1 x *]] }
0x490faf8: {[WHI0 Gradient[200 x 200]] }
0x490fcb8: {[AutoName0 Gradient[200 x 1 x *]] }
0x490fe78: {[AutoName26 Gradient[200 x 1 x *]] }
0x4910038: {[AutoName6 Gradient[200 x 1 x *]] }
0x49101f8: {[AutoName25 Gradient[200 x 1 x *]] }
0x49103b8: {[AutoName24 Gradient[200 x 1 x *]] }
0x4910578: {[AutoName20 Gradient[200 x 1 x *]] }
0x4910ab8: {[WCF0 Gradient[200 x 1]] }
0x4910c78: {[AutoName5 Gradient[200 x 1 x *]] }
0x4910e38: {[AutoName23 Gradient[200 x 1 x *]] }
0x4910ff8: {[AutoName21 Gradient[200 x 1 x *]] }
0x49111b8: {[WHF0 Gradient[200 x 200]] }
0x4911378: {[AutoName1 Gradient[200 x 1 x *]] }
0x4911538: {[AutoName32 Gradient[200 x 1 x *]] }
0x49116f8: {[AutoName30 Gradient[200 x 1 x *]] }
0x49118b8: {[WHO0 Gradient[200 x 200]] }
0x4911a78: {[AutoName2 Gradient[200 x 1 x *]] [LookupTable Gradient[150 x *]] }

07/21/2016 12:12:18: No PreCompute nodes found, skipping PreCompute step.

07/21/2016 12:12:18: Starting Epoch 1: learning rate per sample = 0.100000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

07/21/2016 12:12:18: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
Begin benchmarking for memory swapping...
Step number: 0 step name: LookupTable_forward
Step number: 1 step name: AutoName31_forward
Step number: 2 step name: AutoName32_forward
Step number: 3 step name: AutoName22_forward
Step number: 4 step name: AutoName23_forward
Step number: 5 step name: AutoName9_forward
Step number: 6 step name: AutoName10_forward
Step number: 7 step name: AutoName16_forward
Step number: 8 step name: AutoName2_forward
Step number: 9 step name: AutoName30_forward
Step number: 10 step name: AutoName33_forward
Step number: 11 step name: AutoName1_forward
Step number: 12 step name: AutoName21_forward
Step number: 13 step name: AutoName24_forward
Step number: 14 step name: AutoName5_forward
Step number: 15 step name: AutoName20_forward
Step number: 16 step name: AutoName25_forward
Step number: 17 step name: AutoName26_forward
Step number: 18 step name: AutoName6_forward
Step number: 19 step name: AutoName27_forward
Step number: 20 step name: AutoName0_forward
Step number: 21 step name: AutoName8_forward
Step number: 22 step name: AutoName11_forward
Step number: 23 step name: AutoName4_forward
Step number: 24 step name: AutoName7_forward
Step number: 25 step name: AutoName12_forward
Step number: 26 step name: AutoName13_forward
Step number: 27 step name: AutoName3_forward
Step number: 28 step name: AutoName14_forward
Step number: 29 step name: AutoName15_forward
Step number: 30 step name: AutoName17_forward
Step number: 31 step name: AutoName18_forward
Step number: 32 step name: AutoName19_forward
Step number: 33 step name: AutoName28_forward
Step number: 34 step name: AutoName29_forward
Step number: 35 step name: AutoName34_forward
Step number: 36 step name: AutoName35_forward
Step number: 37 step name: AutoName36_forward
Step number: 38 step name: AutoName37_forward
Step number: 39 step name: ClassPostProb_forward
Step number: 40 step name: TrainNodeClassBasedCrossEntropy_forward
Step number: 41 step name: TrainNodeClassBasedCrossEntropy_backprop
Step number: 42 step name: ClassPostProb_backprop
Step number: 43 step name: AutoName37_backprop
Step number: 44 step name: AutoName36_backprop
Step number: 45 step name: AutoName35_backprop
Step number: 46 step name: AutoName34_backprop
Step number: 47 step name: AutoName29_backprop
Step number: 48 step name: AutoName28_backprop
Step number: 49 step name: AutoName19_backprop
Step number: 50 step name: AutoName18_backprop
Step number: 51 step name: AutoName17_backprop
Step number: 52 step name: AutoName15_backprop
Step number: 53 step name: AutoName14_backprop
Step number: 54 step name: AutoName3_backprop
Step number: 55 step name: AutoName13_backprop
Step number: 56 step name: AutoName12_backprop
Step number: 57 step name: AutoName7_backprop
Step number: 58 step name: AutoName4_backprop
Step number: 59 step name: AutoName11_backprop
Step number: 60 step name: AutoName8_backprop
Step number: 61 step name: AutoName0_backprop
Step number: 62 step name: AutoName27_backprop
Step number: 63 step name: AutoName6_backprop
Step number: 64 step name: AutoName26_backprop
Step number: 65 step name: AutoName25_backprop
Step number: 66 step name: AutoName20_backprop
Step number: 67 step name: AutoName5_backprop
Step number: 68 step name: AutoName24_backprop
Step number: 69 step name: AutoName21_backprop
Step number: 70 step name: AutoName1_backprop
Step number: 71 step name: AutoName33_backprop
Step number: 72 step name: AutoName30_backprop
Step number: 73 step name: AutoName2_backprop
Step number: 74 step name: AutoName16_backprop
Step number: 75 step name: AutoName10_backprop
Step number: 76 step name: AutoName9_backprop
Step number: 77 step name: AutoName23_backprop
Step number: 78 step name: AutoName22_backprop
Step number: 79 step name: AutoName32_backprop
Step number: 80 step name: AutoName31_backprop
Step number: 81 step name: LookupTable_backprop
Memory swapping benchmarking complete!
Swapping buffer: 0x42444a8 with dim 200x150 out at step 1 and in at step 78
Swapping buffer: 0x4245928 with dim 200x150 out at step 5 and in at step 74
Swapping buffer: 0x42460a8 with dim 200x150 out at step 3 and in at step 76
Swapping buffer: 0x48e7d68 with dim 200x150 out at step 7 and in at step 72
Swapping buffer: 0x48e8628 with dim 200x1 out at step 2 and in at step 77
Swapping buffer: 0x48e9468 with dim 200x1 out at step 29 and in at step 51
Swapping buffer: 0x48e9dc8 with dim 200x1 out at step 6 and in at step 71
Swapping buffer: 0x48ea758 with dim 200x1 out at step 4 and in at step 75
Swapping buffer: 0x48eb0f8 with dim 200x200 out at step 21 and in at step 57
Swapping buffer: 0x48ebc28 with dim 200x1 out at step 24 and in at step 56
Swapping buffer: 0x48ec568 with dim 200x200 out at step 12 and in at step 66
Swapping buffer: 0x48ed408 with dim 200x1 out at step 15 and in at step 65
Swapping buffer: 0x48edda8 with dim 200x200 out at step 9 and in at step 69
Swapping buffer: 0x48ee748 with dim 200x1 out at step 34 and in at step 46
Swapping buffer: 0x48ef0e8 with dim 200x200 out at step 28 and in at step 47
Swapping buffer: 0x48fa558 with dim 50x200 out at step 39 and in at step 41
Total swapped memory: 1.111603MB
Total swappable memory: 14.464859MB
WARNING: The same matrix with dim [4, 108] has been transferred between different devices for 20 times.
07/21/2016 12:13:05: Finished Epoch[ 1 of 3]: [Training] TrainNodeClassBasedCrossEntropy = 6.46830734 * 2087; totalSamplesSeen = 2087; learningRatePerSample = 0.1; epochTime=46.369s
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
07/21/2016 12:13:57: Final Results: Minibatch[1-704]: TrainNodeClassBasedCrossEntropy = 6.55080699 * 73760; perplexity = 699.80868136
07/21/2016 12:13:57: Finished Epoch[ 1 of 3]: [Validate] TrainNodeClassBasedCrossEntropy = 6.55080699 * 73760
07/21/2016 12:13:57: SGD: Saving checkpoint model '/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn.1'

07/21/2016 12:13:57: Starting Epoch 2: learning rate per sample = 0.100000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

07/21/2016 12:13:57: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
07/21/2016 12:14:00: Finished Epoch[ 2 of 3]: [Training] TrainNodeClassBasedCrossEntropy = 6.58921659 * 2099; totalSamplesSeen = 4186; learningRatePerSample = 0.1; epochTime=2.98334s
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
07/21/2016 12:14:35: Final Results: Minibatch[1-353]: TrainNodeClassBasedCrossEntropy = 6.47190087 * 73760; perplexity = 646.71187213
07/21/2016 12:14:35: Finished Epoch[ 2 of 3]: [Validate] TrainNodeClassBasedCrossEntropy = 6.47190087 * 73760
07/21/2016 12:14:36: SGD: Saving checkpoint model '/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn.2'

07/21/2016 12:14:36: Starting Epoch 3: learning rate per sample = 0.100000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

07/21/2016 12:14:36: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
07/21/2016 12:14:40: Finished Epoch[ 3 of 3]: [Training] TrainNodeClassBasedCrossEntropy = 7.17825183 * 2276; totalSamplesSeen = 6462; learningRatePerSample = 0.1; epochTime=3.79828s
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
07/21/2016 12:15:06: Final Results: Minibatch[1-193]: TrainNodeClassBasedCrossEntropy = 11.34215436 * 73760; perplexity = 84301.45140267
07/21/2016 12:15:06: Finished Epoch[ 3 of 3]: [Validate] TrainNodeClassBasedCrossEntropy = 11.34215436 * 73760
07/21/2016 12:15:06: Loading (rolling back to) previous model with best training-criterion value: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn.2.
07/21/2016 12:15:07: learnRatePerSample reduced to 0.050000001
07/21/2016 12:15:07: SGD: revoke back to and update checkpoint file for epoch 2

07/21/2016 12:15:07: Starting Epoch 3: learning rate per sample = 0.050000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

07/21/2016 12:15:07: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
07/21/2016 12:15:09: Finished Epoch[ 3 of 3]: [Training] TrainNodeClassBasedCrossEntropy = 6.47460508 * 2276; totalSamplesSeen = 6462; learningRatePerSample = 0.050000001; epochTime=2.19471s
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
07/21/2016 12:15:36: Final Results: Minibatch[1-193]: TrainNodeClassBasedCrossEntropy = 6.71366701 * 73760; perplexity = 823.58520584
07/21/2016 12:15:36: Finished Epoch[ 3 of 3]: [Validate] TrainNodeClassBasedCrossEntropy = 6.71366701 * 73760
07/21/2016 12:15:36: Loading (rolling back to) previous model with best training-criterion value: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn.2.
07/21/2016 12:15:36: learnRatePerSample reduced to 0.025
07/21/2016 12:15:36: SGD: revoke back to and update checkpoint file for epoch 2

07/21/2016 12:15:37: Starting Epoch 3: learning rate per sample = 0.025000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

07/21/2016 12:15:37: Starting minibatch loop.
LMSequenceReader: Reading epoch data... 42068 sequences read.
07/21/2016 12:15:39: Finished Epoch[ 3 of 3]: [Training] TrainNodeClassBasedCrossEntropy = 6.24824768 * 2276; totalSamplesSeen = 6462; learningRatePerSample = 0.025; epochTime=2.61889s
LMSequenceReader: Reading epoch data... 3370 sequences read.
LMSequenceReader: Reading epoch data... 0 sequences read.
07/21/2016 12:16:05: Final Results: Minibatch[1-193]: TrainNodeClassBasedCrossEntropy = 6.43806506 * 73760; perplexity = 625.19591502
07/21/2016 12:16:05: Finished Epoch[ 3 of 3]: [Validate] TrainNodeClassBasedCrossEntropy = 6.43806506 * 73760
07/21/2016 12:16:06: SGD: Saving checkpoint model '/tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/Models/rnn.dnn'
07/21/2016 12:16:06: CNTKCommandTrainEnd: train

07/21/2016 12:16:06: Action "train" complete.


07/21/2016 12:16:06: ##############################################################################
07/21/2016 12:16:06: #                                                                            #
07/21/2016 12:16:06: # Action "eval"                                                              #
07/21/2016 12:16:06: #                                                                            #
07/21/2016 12:16:06: ##############################################################################

LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.txt
LMSequenceReader: Label mapping will be created internally on the fly because the labelMappingFile was not found: /tmp/cntk-test-20160721121216.198065/Examples/Text/PennTreebank_RNN_MemorySwapping@debug_gpu/sentenceLabels.out.txt

Post-processing network...

3 roots:
	PosteriorProb = Softmax()
	TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax()
	outputs = TransposeTimes()

Loop[0] --> Loop_AutoName37 -> 31 nodes

	AutoName2	AutoName30	AutoName33
	AutoName1	AutoName21	AutoName24
	AutoName5	AutoName20	AutoName25
	AutoName26	AutoName6	AutoName27
	AutoName0	AutoName8	AutoName11
	AutoName4	AutoName7	AutoName12
	AutoName13	AutoName3	AutoName14
	AutoName15	AutoName17	AutoName18
	AutoName19	AutoName28	AutoName29
	AutoName34	AutoName35	AutoName36
	AutoName37

Validating network. 63 nodes to process in pass 1.

Validating --> W2 = LearnableParameter() :  -> [200 x 10000]
Validating --> WXO0 = LearnableParameter() :  -> [200 x 150]
Validating --> E0 = LearnableParameter() :  -> [150 x 10000]
Validating --> features = SparseInputValue() :  -> [10000 x *1]
Validating --> LookupTable = LookupTable (E0, features) : [150 x 10000], [10000 x *1] -> [150 x *1]
Validating --> AutoName31 = Times (WXO0, LookupTable) : [200 x 150], [150 x *1] -> [200 x *1]
Validating --> bo0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName32 = Plus (AutoName31, bo0) : [200 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> WHO0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCO0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXF0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName22 = Times (WXF0, LookupTable) : [200 x 150], [150 x *1] -> [200 x *1]
Validating --> bf0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName23 = Plus (AutoName22, bf0) : [200 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> WHF0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCF0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXI0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName9 = Times (WXI0, LookupTable) : [200 x 150], [150 x *1] -> [200 x *1]
Validating --> bi0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName10 = Plus (AutoName9, bi0) : [200 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> WHI0 = LearnableParameter() :  -> [200 x 200]
Validating --> WCI0 = LearnableParameter() :  -> [200 x 1]
Validating --> WXC0 = LearnableParameter() :  -> [200 x 150]
Validating --> AutoName16 = Times (WXC0, LookupTable) : [200 x 150], [150 x *1] -> [200 x *1]
Validating --> WHC0 = LearnableParameter() :  -> [200 x 200]
Validating --> bc0 = LearnableParameter() :  -> [200 x 1]
Validating --> AutoName30 = Times (WHO0, AutoName2) : [200 x 200], [200 x 1] -> [200 x 1]
Validating --> AutoName33 = Plus (AutoName32, AutoName30) : [200 x 1 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> AutoName21 = Times (WHF0, AutoName1) : [200 x 200], [200 x 1] -> [200 x 1]
Validating --> AutoName24 = Plus (AutoName23, AutoName21) : [200 x 1 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> AutoName20 = DiagTimes (WCF0, AutoName5) : [200 x 1], [200 x 1] -> [200 x 1]
Validating --> AutoName25 = Plus (AutoName24, AutoName20) : [200 x 1 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> AutoName26 = Sigmoid (AutoName25) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName27 = ElementTimes (AutoName26, AutoName6) : [200 x 1 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> AutoName8 = Times (WHI0, AutoName0) : [200 x 200], [200 x 1] -> [200 x 1]
Validating --> AutoName11 = Plus (AutoName10, AutoName8) : [200 x 1 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> AutoName7 = DiagTimes (WCI0, AutoName4) : [200 x 1], [200 x 1] -> [200 x 1]
Validating --> AutoName12 = Plus (AutoName11, AutoName7) : [200 x 1 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> AutoName13 = Sigmoid (AutoName12) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName14 = Times (WHC0, AutoName3) : [200 x 200], [200 x 1] -> [200 x 1]
Validating --> AutoName15 = Plus (AutoName14, bc0) : [200 x 1], [200 x 1] -> [200 x 1]
Validating --> AutoName17 = Plus (AutoName16, AutoName15) : [200 x *1], [200 x 1] -> [200 x 1 x *1]
Validating --> AutoName18 = Tanh (AutoName17) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName19 = ElementTimes (AutoName13, AutoName18) : [200 x 1 x *1], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName28 = Plus (AutoName27, AutoName19) : [200 x 1 x *1], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName29 = DiagTimes (WCO0, AutoName28) : [200 x 1], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName34 = Plus (AutoName33, AutoName29) : [200 x 1 x *1], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName35 = Sigmoid (AutoName34) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName36 = Tanh (AutoName28) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName37 = ElementTimes (AutoName35, AutoName36) : [200 x 1 x *1], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> outputs = TransposeTimes (W2, AutoName37) : [200 x 10000], [200 x 1 x *1] -> [10000 x 1 x *1]
Validating --> PosteriorProb = Softmax (outputs) : [10000 x 1 x *1] -> [10000 x 1 x *1]
Validating --> labels = InputValue() :  -> [4 x *1]
Validating --> WeightForClassPostProb = LearnableParameter() :  -> [50 x 200]
Validating --> ClassPostProb = Times (WeightForClassPostProb, AutoName37) : [50 x 200], [200 x 1 x *1] -> [50 x 1 x *1]
Validating --> TrainNodeClassBasedCrossEntropy = ClassBasedCrossEntropyWithSoftmax (labels, AutoName37, W2, ClassPostProb) : [4 x *1], [200 x 1 x *1], [200 x 10000], [50 x 1 x *1] -> [1]

Validating network. 43 nodes to process in pass 2.

Validating --> AutoName2 = PastValue (AutoName37) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName30 = Times (WHO0, AutoName2) : [200 x 200], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName1 = PastValue (AutoName37) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName21 = Times (WHF0, AutoName1) : [200 x 200], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName5 = PastValue (AutoName28) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName20 = DiagTimes (WCF0, AutoName5) : [200 x 1], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName6 = PastValue (AutoName28) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName0 = PastValue (AutoName37) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName8 = Times (WHI0, AutoName0) : [200 x 200], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName4 = PastValue (AutoName28) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName7 = DiagTimes (WCI0, AutoName4) : [200 x 1], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName3 = PastValue (AutoName37) : [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName14 = Times (WHC0, AutoName3) : [200 x 200], [200 x 1 x *1] -> [200 x 1 x *1]
Validating --> AutoName15 = Plus (AutoName14, bc0) : [200 x 1 x *1], [200 x 1] -> [200 x 1 x *1]

Validating network. 14 nodes to process in pass 3.


Validating network, final pass.



19 out of 63 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[AutoName0 Gradient[200 x 1 x *1]] [AutoName1 Gradient[200 x 1 x *1]] [AutoName10 Gradient[200 x 1 x *1]] [AutoName11 Gradient[200 x 1 x *1]] [AutoName12 Gradient[200 x 1 x *1]] [AutoName13 Gradient[200 x 1 x *1]] [AutoName14 Gradient[200 x 1 x *1]] [AutoName15 Gradient[200 x 1 x *1]] [AutoName16 Gradient[200 x *1]] [AutoName17 Gradient[200 x 1 x *1]] [AutoName18 Gradient[200 x 1 x *1]] [AutoName19 Gradient[200 x 1 x *1]] [AutoName2 Gradient[200 x 1 x *1]] [AutoName20 Gradient[200 x 1 x *1]] [AutoName21 Gradient[200 x 1 x *1]] [AutoName22 Gradient[200 x *1]] [AutoName23 Gradient[200 x 1 x *1]] [AutoName24 Gradient[200 x 1 x *1]] [AutoName25 Gradient[200 x 1 x *1]] [AutoName26 Gradient[200 x 1 x *1]] [AutoName27 Gradient[200 x 1 x *1]] [AutoName28 Gradient[200 x 1 x *1]] [AutoName29 Gradient[200 x 1 x *1]] [AutoName3 Gradient[200 x 1 x *1]] [AutoName30 Gradient[200 x 1 x *1]] [AutoName31 Gradient[200 x *1]] [AutoName32 Gradient[200 x 1 x *1]] [AutoName33 Gradient[200 x 1 x *1]] [AutoName34 Gradient[200 x 1 x *1]] [AutoName35 Gradient[200 x 1 x *1]] [AutoName36 Gradient[200 x 1 x *1]] [AutoName37 Gradient[200 x 1 x *1]] [AutoName4 Gradient[200 x 1 x *1]] [AutoName5 Gradient[200 x 1 x *1]] [AutoName6 Gradient[200 x 1 x *1]] [AutoName7 Gradient[200 x 1 x *1]] [AutoName8 Gradient[200 x 1 x *1]] [AutoName9 Gradient[200 x *1]] [ClassPostProb Gradient[50 x 1 x *1]] [E0 Gradient[150 x 10000]] [LookupTable Gradient[150 x *1]] [PosteriorProb Gradient[10000 x 1 x *1]] [PosteriorProb Value[10000 x 1 x *1]] [TrainNodeClassBasedCrossEntropy Gradient[1]] [W2 Gradient[200 x 10000]] [WCF0 Gradient[200 x 1]] [WCI0 Gradient[200 x 1]] [WCO0 Gradient[200 x 1]] [WHC0 Gradient[200 x 200]] [WHF0 Gradient[200 x 200]] [WHI0 Gradient[200 x 200]] [WHO0 Gradient[200 x 200]] [WXC0 Gradient[200 x 150]] [WXF0 Gradient[200 x 150]] [WXI0 Gradient[200 x 150]] [WXO0 Gradient[200 x 150]] [WeightForClassPostProb Gradient[50 x 200]] [bc0 Gradient[200 x 1]] [bf0 Gradient[200 x 1]] [bi0 Gradient[200 x 1]] [bo0 Gradient[200 x 1]] [features Gradient[10000 x *1]] [labels Gradient[4 x *1]] [outputs Gradient[10000 x 1 x *1]] [outputs Value[10000 x 1 x *1]] }
0x52f1808: {[AutoName16 Value[200 x *1]] }
0x546c058: {[labels Value[4 x *1]] }
0x5496978: {[WCI0 Value[200 x 1]] }
0x54fd608: {[E0 Value[150 x 10000]] }
0x5665fd8: {[features Value[10000 x *1]] }
0x574b808: {[bf0 Value[200 x 1]] }
0x58db0f8: {[bi0 Value[200 x 1]] }
0x58e04e8: {[bo0 Value[200 x 1]] }
0x5a52d08: {[AutoName6 Value[200 x 1 x *1]] }
0x5a6f938: {[AutoName22 Value[200 x *1]] }
0x5a6fa88: {[TrainNodeClassBasedCrossEntropy Value[1]] }
0x5b65468: {[AutoName36 Value[200 x 1 x *1]] }
0x5b65628: {[AutoName37 Value[200 x 1 x *1]] }
0x5c802e8: {[bc0 Value[200 x 1]] }
0x5deb138: {[AutoName23 Value[200 x 1 x *1]] }
0x5deb238: {[AutoName10 Value[200 x 1 x *1]] }
0x614d888: {[ClassPostProb Value[50 x 1 x *1]] }
0x61a4d78: {[AutoName13 Value[200 x 1 x *1]] }
0x61a4f38: {[AutoName14 Value[200 x 1 x *1]] }
0x620cf68: {[AutoName3 Value[200 x 1 x *1]] }
0x6248df8: {[AutoName30 Value[200 x 1 x *1]] }
0x6248fb8: {[AutoName33 Value[200 x 1 x *1]] }
0x64028e8: {[AutoName7 Value[200 x 1 x *1]] }
0x6402aa8: {[AutoName12 Value[200 x 1 x *1]] }
0x64b99b8: {[AutoName0 Value[200 x 1 x *1]] }
0x64b9a58: {[AutoName2 Value[200 x 1 x *1]] }
0x6505e98: {[LookupTable Value[150 x *1]] }
0x66ef508: {[WCF0 Value[200 x 1]] }
0x6735318: {[AutoName9 Value[200 x *1]] }
0x68dda58: {[AutoName34 Value[200 x 1 x *1]] }
0x68ddc18: {[AutoName35 Value[200 x 1 x *1]] }
0x68f9a08: {[AutoName5 Value[200 x 1 x *1]] }
0x6941198: {[AutoName26 Value[200 x 1 x *1]] }
0x6941358: {[AutoName27 Value[200 x 1 x *1]] }
0x6a15278: {[AutoName28 Value[200 x 1 x *1]] }
0x6a15438: {[AutoName29 Value[200 x 1 x *1]] }
0x6bc3ad8: {[AutoName4 Value[200 x 1 x *1]] }
0x6c8a758: {[AutoName31 Value[200 x *1]] }
0x6d5d258: {[AutoName18 Value[200 x 1 x *1]] }
0x6d5d418: {[AutoName19 Value[200 x 1 x *1]] }
0x6e13fa8: {[AutoName15 Value[200 x 1 x *1]] }
0x6e14168: {[AutoName17 Value[200 x 1 x *1]] }
0x784a758: {[AutoName32 Value[200 x 1 x *1]] }
0x784a9e8: {[AutoName1 Value[200 x 1 x *1]] }
0x7caafd8: {[AutoName8 Value[200 x 1 x *1]] }
0x7cab198: {[AutoName11 Value[200 x 1 x *1]] }
0x8151b18: {[WXO0 Value[200 x 150]] }
0x821f318: {[WXI0 Value[200 x 150]] }
0x825d898: {[W2 Value[200 x 10000]] }
0x8284ca8: {[AutoName20 Value[200 x 1 x *1]] }
0x8284e68: {[AutoName25 Value[200 x 1 x *1]] }
0x82e6d38: {[WXF0 Value[200 x 150]] }
0x8451268: {[WXC0 Value[200 x 150]] }
0x84b0eb8: {[WHO0 Value[200 x 200]] }
0x8577328: {[WHI0 Value[200 x 200]] }
0x872b788: {[WHF0 Value[200 x 200]] }
0x8855c88: {[WHC0 Value[200 x 200]] }
0x88f1658: {[WeightForClassPostProb Value[50 x 200]] }
0x89b17b8: {[AutoName21 Value[200 x 1 x *1]] }
0x89b1978: {[AutoName24 Value[200 x 1 x *1]] }
0x89bd838: {[WCO0 Value[200 x 1]] }

LMSequenceReader: Reading epoch data... 3760 sequences read.
Step number: 0 step name: LookupTable_forward
Step number: 1 step name: AutoName31_forward
Step number: 2 step name: AutoName32_forward
Step number: 3 step name: AutoName22_forward
Step number: 4 step name: AutoName23_forward
Step number: 5 step name: AutoName9_forward
Step number: 6 step name: AutoName10_forward
Step number: 7 step name: AutoName16_forward
Step number: 8 step name: AutoName2_forward
Step number: 9 step name: AutoName30_forward
Step number: 10 step name: AutoName33_forward
Step number: 11 step name: AutoName1_forward
Step number: 12 step name: AutoName21_forward
Step number: 13 step name: AutoName24_forward
Step number: 14 step name: AutoName5_forward
Step number: 15 step name: AutoName20_forward
Step number: 16 step name: AutoName25_forward
Step number: 17 step name: AutoName26_forward
Step number: 18 step name: AutoName6_forward
Step number: 19 step name: AutoName27_forward
Step number: 20 step name: AutoName0_forward
Step number: 21 step name: AutoName8_forward
Step number: 22 step name: AutoName11_forward
Step number: 23 step name: AutoName4_forward
Step number: 24 step name: AutoName7_forward
Step number: 25 step name: AutoName12_forward
Step number: 26 step name: AutoName13_forward
Step number: 27 step name: AutoName3_forward
Step number: 28 step name: AutoName14_forward
Step number: 29 step name: AutoName15_forward
Step number: 30 step name: AutoName17_forward
Step number: 31 step name: AutoName18_forward
Step number: 32 step name: AutoName19_forward
Step number: 33 step name: AutoName28_forward
Step number: 34 step name: AutoName29_forward
Step number: 35 step name: AutoName34_forward
Step number: 36 step name: AutoName35_forward
Step number: 37 step name: AutoName36_forward
Step number: 38 step name: AutoName37_forward
Step number: 39 step name: ClassPostProb_forward
Step number: 40 step name: TrainNodeClassBasedCrossEntropy_forward
Total swapped memory: 0.000000MB
Total swappable memory: 14.515686MB
WARNING: The same matrix with dim [4, 1872] has been transferred between different devices for 20 times.
LMSequenceReader: Reading epoch data... 0 sequences read.
07/21/2016 12:19:32: Minibatch[1-60]: TrainNodeClassBasedCrossEntropy = 6.42073109 * 82402
07/21/2016 12:19:32: Final Results: Minibatch[1-60]: TrainNodeClassBasedCrossEntropy = 6.42073109 * 82402; perplexity = 614.45216869

07/21/2016 12:19:32: Action "eval" complete.

07/21/2016 12:19:32: __COMPLETED__
