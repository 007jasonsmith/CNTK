CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3550 @ 3.07GHz
    Hardware threads: 4
    Total Memory: 12580388 kB
-------------------------------------------------------------------
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/13/2016 04:44:51: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 07/13/2016 04:44:51: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:44:51: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:51: 		Built time: Jul 13 2016 03:55:32
MPI Rank 0: 07/13/2016 04:44:51: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 0: 07/13/2016 04:44:51: 		Build type: Release
MPI Rank 0: 07/13/2016 04:44:51: 		Build target: GPU
MPI Rank 0: 07/13/2016 04:44:51: 		With 1bit-SGD: no
MPI Rank 0: 07/13/2016 04:44:51: 		Math lib: mkl
MPI Rank 0: 07/13/2016 04:44:51: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/13/2016 04:44:51: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/13/2016 04:44:51: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/13/2016 04:44:51: 		Build Branch: HEAD
MPI Rank 0: 07/13/2016 04:44:51: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 0: 07/13/2016 04:44:51: 		Built by svcphil on LIANA-09-w
MPI Rank 0: 07/13/2016 04:44:51: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/13/2016 04:44:51: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:44:52: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: Running on cntk-muc02 at 2016/07/13 04:44:52
MPI Rank 0: 07/13/2016 04:44:52: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:44:52: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:44:52: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/13/2016 04:44:52: Commands: train
MPI Rank 0: 07/13/2016 04:44:52: Precision = "float"
MPI Rank 0: 07/13/2016 04:44:52: Using 1 CPU threads.
MPI Rank 0: 07/13/2016 04:44:52: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 07/13/2016 04:44:52: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/13/2016 04:44:52: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: ##############################################################################
MPI Rank 0: 07/13/2016 04:44:52: #                                                                            #
MPI Rank 0: 07/13/2016 04:44:52: # Action "train"                                                             #
MPI Rank 0: 07/13/2016 04:44:52: #                                                                            #
MPI Rank 0: 07/13/2016 04:44:52: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: Creating virgin network.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: Created model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:52: Training criterion node(s):
MPI Rank 0: 07/13/2016 04:44:52: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 000000C6B5D7E9E0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 000000C6B5D7EB20: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 000000C6B5D7ED00: {[WD1 Value[64 x 288]] }
MPI Rank 0: 000000C6B5D7F2A0: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 000000C6D329C600: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 000000C6D329C740: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 000000C6D329CA60: {[Keyword Value[49292 x *]] }
MPI Rank 0: 000000C6D329CCE0: {[G Value[1 x 1]] }
MPI Rank 0: 000000C6D329CD80: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 000000C6D329CEC0: {[SIM Gradient[51 x *]] }
MPI Rank 0: 000000C6D329D000: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 000000C6D329D0A0: {[Query Value[49292 x *]] }
MPI Rank 0: 000000C6D329D140: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 000000C6D329D1E0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 000000C6D329D280: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 000000C6D329D320: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 000000C6D329D460: {[ce Gradient[1]] }
MPI Rank 0: 000000C6D329D500: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 000000C6D329D5A0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 000000C6D329D640: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 000000C6D329DD20: {[S Value[1 x 1]] }
MPI Rank 0: 000000C6D329DDC0: {[N Value[1 x 1]] }
MPI Rank 0: 000000C6D329DF00: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 000000C6D329E0E0: {[SIM Value[51 x *]] }
MPI Rank 0: 000000C6D329E180: {[ce Value[1]] }
MPI Rank 0: 000000C6D329E2C0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 000000C6D329E4A0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/13/2016 04:44:52: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:55: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:55: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:45:05:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.41944122 * 10240; time = 9.5744s; samplesPerSecond = 1069.5
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:45:14:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38406754 * 10240; time = 9.6260s; samplesPerSecond = 1063.8
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:45:19: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.2631s
MPI Rank 0: 07/13/2016 04:45:21: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 0: 07/13/2016 04:45:21: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 0: 07/13/2016 04:45:23: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:24: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:24: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/13/2016 04:45:34:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29922523 * 10240; time = 9.7582s; samplesPerSecond = 1049.4
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:45:44:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.08742409 * 10240; time = 9.9880s; samplesPerSecond = 1025.2
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:45:49: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.8072s
MPI Rank 0: 07/13/2016 04:45:51: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 0: 07/13/2016 04:45:51: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 0: 07/13/2016 04:45:53: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/13/2016 04:46:04:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90042439 * 10240; time = 10.0583s; samplesPerSecond = 1018.1
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/13/2016 04:46:14:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.85719700 * 10240; time = 10.0147s; samplesPerSecond = 1022.5
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.07 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.07 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/13/2016 04:46:19: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.2353s
MPI Rank 0: 07/13/2016 04:46:21: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 0: 07/13/2016 04:46:21: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 0: 07/13/2016 04:46:23: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 07/13/2016 04:46:24: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:24: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:24: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/13/2016 04:44:52: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:44:52: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: 		Built time: Jul 13 2016 03:55:32
MPI Rank 1: 07/13/2016 04:44:52: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 1: 07/13/2016 04:44:52: 		Build type: Release
MPI Rank 1: 07/13/2016 04:44:52: 		Build target: GPU
MPI Rank 1: 07/13/2016 04:44:52: 		With 1bit-SGD: no
MPI Rank 1: 07/13/2016 04:44:52: 		Math lib: mkl
MPI Rank 1: 07/13/2016 04:44:52: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/13/2016 04:44:52: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/13/2016 04:44:52: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/13/2016 04:44:52: 		Build Branch: HEAD
MPI Rank 1: 07/13/2016 04:44:52: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 1: 07/13/2016 04:44:52: 		Built by svcphil on LIANA-09-w
MPI Rank 1: 07/13/2016 04:44:52: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:44:52: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: Running on cntk-muc02 at 2016/07/13 04:44:52
MPI Rank 1: 07/13/2016 04:44:52: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:44:52: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:44:52: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/13/2016 04:44:52: Commands: train
MPI Rank 1: 07/13/2016 04:44:52: Precision = "float"
MPI Rank 1: 07/13/2016 04:44:52: Using 1 CPU threads.
MPI Rank 1: 07/13/2016 04:44:52: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 07/13/2016 04:44:52: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/13/2016 04:44:52: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: ##############################################################################
MPI Rank 1: 07/13/2016 04:44:52: #                                                                            #
MPI Rank 1: 07/13/2016 04:44:52: # Action "train"                                                             #
MPI Rank 1: 07/13/2016 04:44:52: #                                                                            #
MPI Rank 1: 07/13/2016 04:44:52: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:52: Creating virgin network.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:53: Created model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:53: Training criterion node(s):
MPI Rank 1: 07/13/2016 04:44:53: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000B6D3A1D700: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000B6D3A1DB60: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000B6D3A1DDE0: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 000000B6D3A1E560: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000B6F3351FD0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000B6F3352070: {[ce Gradient[1]] }
MPI Rank 1: 000000B6F3352110: {[S Value[1 x 1]] }
MPI Rank 1: 000000B6F33521B0: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 000000B6F33524D0: {[SIM Gradient[51 x *]] }
MPI Rank 1: 000000B6F3352610: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 000000B6F33526B0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000B6F33529D0: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 000000B6F3352B10: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 000000B6F3352C50: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 000000B6F3352E30: {[SIM Value[51 x *]] }
MPI Rank 1: 000000B6F3352ED0: {[ce Value[1]] }
MPI Rank 1: 000000B6F3352F70: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 000000B6F3353010: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 000000B6F33530B0: {[G Value[1 x 1]] }
MPI Rank 1: 000000B6F3353290: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 000000B6F3353470: {[Keyword Value[49292 x *]] }
MPI Rank 1: 000000B6F33536F0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 000000B6F3353790: {[N Value[1 x 1]] }
MPI Rank 1: 000000B6F3353830: {[Query Value[49292 x *]] }
MPI Rank 1: 000000B6F33538D0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 000000B6F3353970: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 000000B6F3353AB0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/13/2016 04:44:53: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:55: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:55: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.02 seconds
MPI Rank 1: 07/13/2016 04:45:05:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.42829742 * 10240; time = 9.5600s; samplesPerSecond = 1071.1
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.05 seconds
MPI Rank 1: 07/13/2016 04:45:14:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38771057 * 10240; time = 9.6276s; samplesPerSecond = 1063.6
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.05 seconds
MPI Rank 1: 07/13/2016 04:45:19: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.2628s
MPI Rank 1: 07/13/2016 04:45:21: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 1: 07/13/2016 04:45:21: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:24: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:24: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.04 seconds
MPI Rank 1: 07/13/2016 04:45:34:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34065857 * 10240; time = 9.7324s; samplesPerSecond = 1052.2
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.06 seconds
MPI Rank 1: 07/13/2016 04:45:44:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11009483 * 10240; time = 9.9865s; samplesPerSecond = 1025.4
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 0.07 seconds
MPI Rank 1: 07/13/2016 04:45:49: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.8069s
MPI Rank 1: 07/13/2016 04:45:51: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 1: 07/13/2016 04:45:51: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.07 seconds
MPI Rank 1: 07/13/2016 04:46:04:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93978710 * 10240; time = 10.0593s; samplesPerSecond = 1018.0
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 0.08 seconds
MPI Rank 1: 07/13/2016 04:46:14:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86772938 * 10240; time = 10.0147s; samplesPerSecond = 1022.5
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 0.07 seconds
MPI Rank 1: 07/13/2016 04:46:19: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.2355s
MPI Rank 1: 07/13/2016 04:46:21: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 1: 07/13/2016 04:46:21: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 1: 07/13/2016 04:46:24: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:24: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:24: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/13/2016 04:44:52: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:44:52: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:52: 		Built time: Jul 13 2016 03:55:32
MPI Rank 2: 07/13/2016 04:44:52: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 2: 07/13/2016 04:44:52: 		Build type: Release
MPI Rank 2: 07/13/2016 04:44:52: 		Build target: GPU
MPI Rank 2: 07/13/2016 04:44:52: 		With 1bit-SGD: no
MPI Rank 2: 07/13/2016 04:44:52: 		Math lib: mkl
MPI Rank 2: 07/13/2016 04:44:52: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/13/2016 04:44:52: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/13/2016 04:44:52: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/13/2016 04:44:52: 		Build Branch: HEAD
MPI Rank 2: 07/13/2016 04:44:52: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 2: 07/13/2016 04:44:52: 		Built by svcphil on LIANA-09-w
MPI Rank 2: 07/13/2016 04:44:52: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/13/2016 04:44:52: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:44:53: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:44:53: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 07/13/2016 04:44:53: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: Running on cntk-muc02 at 2016/07/13 04:44:53
MPI Rank 2: 07/13/2016 04:44:53: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:44:53: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:44:53: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/13/2016 04:44:53: Commands: train
MPI Rank 2: 07/13/2016 04:44:53: Precision = "float"
MPI Rank 2: 07/13/2016 04:44:53: Using 1 CPU threads.
MPI Rank 2: 07/13/2016 04:44:53: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 07/13/2016 04:44:53: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/13/2016 04:44:53: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: ##############################################################################
MPI Rank 2: 07/13/2016 04:44:53: #                                                                            #
MPI Rank 2: 07/13/2016 04:44:53: # Action "train"                                                             #
MPI Rank 2: 07/13/2016 04:44:53: #                                                                            #
MPI Rank 2: 07/13/2016 04:44:53: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: Creating virgin network.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: Created model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:53: Training criterion node(s):
MPI Rank 2: 07/13/2016 04:44:53: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000605A3EC6A0: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000605A3ECEC0: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 000000605A3ED140: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000605A3ED320: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 000000607D7C2C40: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 000000607D7C2CE0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 000000607D7C2EC0: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 000000607D7C3140: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 000000607D7C31E0: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 000000607D7C3280: {[Query Value[49292 x *]] }
MPI Rank 2: 000000607D7C33C0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000607D7C3500: {[Keyword Value[49292 x *]] }
MPI Rank 2: 000000607D7C3640: {[S Value[1 x 1]] }
MPI Rank 2: 000000607D7C36E0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 000000607D7C3780: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 000000607D7C3820: {[ce Value[1]] }
MPI Rank 2: 000000607D7C3960: {[SIM Gradient[51 x *]] }
MPI Rank 2: 000000607D7C3B40: {[N Value[1 x 1]] }
MPI Rank 2: 000000607D7C3C80: {[G Value[1 x 1]] }
MPI Rank 2: 000000607D7C3D20: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 000000607D7C4360: {[ce Gradient[1]] }
MPI Rank 2: 000000607D7C4400: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 000000607D7C44A0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 000000607D7C4540: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 000000607D7C45E0: {[SIM Value[51 x *]] }
MPI Rank 2: 000000607D7C4680: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 000000607D7C4860: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/13/2016 04:44:53: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:55: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:55: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.05 seconds
MPI Rank 2: 07/13/2016 04:45:05:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.44101372 * 10240; time = 9.5618s; samplesPerSecond = 1070.9
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/13/2016 04:45:14:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.39723701 * 10240; time = 9.6260s; samplesPerSecond = 1063.8
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/13/2016 04:45:19: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.2627s
MPI Rank 2: 07/13/2016 04:45:21: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 2: 07/13/2016 04:45:21: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:24: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:24: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.08 seconds
MPI Rank 2: 07/13/2016 04:45:34:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34435501 * 10240; time = 9.7580s; samplesPerSecond = 1049.4
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.08 seconds
MPI Rank 2: 07/13/2016 04:45:44:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.13005295 * 10240; time = 9.9880s; samplesPerSecond = 1025.2
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/13/2016 04:45:49: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.8049s
MPI Rank 2: 07/13/2016 04:45:51: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 2: 07/13/2016 04:45:51: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.10 seconds
MPI Rank 2: 07/13/2016 04:46:04:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95727177 * 10240; time = 10.0588s; samplesPerSecond = 1018.0
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 0.10 seconds
MPI Rank 2: 07/13/2016 04:46:14:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88702602 * 10240; time = 9.9575s; samplesPerSecond = 1028.4
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 2.15 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.15 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.31 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.47 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.47 seconds , average latency = 0.10 seconds
MPI Rank 2: 07/13/2016 04:46:19: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.2336s
MPI Rank 2: 07/13/2016 04:46:21: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 2: 07/13/2016 04:46:21: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 2: 07/13/2016 04:46:24: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:24: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:24: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/13/2016 04:44:53: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 07/13/2016 04:44:53: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:44:53: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: 		Built time: Jul 13 2016 03:55:32
MPI Rank 3: 07/13/2016 04:44:53: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 3: 07/13/2016 04:44:53: 		Build type: Release
MPI Rank 3: 07/13/2016 04:44:53: 		Build target: GPU
MPI Rank 3: 07/13/2016 04:44:53: 		With 1bit-SGD: no
MPI Rank 3: 07/13/2016 04:44:53: 		Math lib: mkl
MPI Rank 3: 07/13/2016 04:44:53: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/13/2016 04:44:53: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/13/2016 04:44:53: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/13/2016 04:44:53: 		Build Branch: HEAD
MPI Rank 3: 07/13/2016 04:44:53: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 3: 07/13/2016 04:44:53: 		Built by svcphil on LIANA-09-w
MPI Rank 3: 07/13/2016 04:44:53: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/13/2016 04:44:53: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:44:53: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:44:53: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 07/13/2016 04:44:53: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: Running on cntk-muc02 at 2016/07/13 04:44:53
MPI Rank 3: 07/13/2016 04:44:53: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:44:53: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:44:53: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/13/2016 04:44:53: Commands: train
MPI Rank 3: 07/13/2016 04:44:53: Precision = "float"
MPI Rank 3: 07/13/2016 04:44:53: Using 1 CPU threads.
MPI Rank 3: 07/13/2016 04:44:53: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 07/13/2016 04:44:53: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/13/2016 04:44:53: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: ##############################################################################
MPI Rank 3: 07/13/2016 04:44:53: #                                                                            #
MPI Rank 3: 07/13/2016 04:44:53: # Action "train"                                                             #
MPI Rank 3: 07/13/2016 04:44:53: #                                                                            #
MPI Rank 3: 07/13/2016 04:44:53: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:53: Creating virgin network.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:54: Created model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:54: Training criterion node(s):
MPI Rank 3: 07/13/2016 04:44:54: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000398C8AC7E0: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000398C8AC920: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000398C8ACBA0: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000398C8ACCE0: {[WD1 Value[64 x 288]] }
MPI Rank 3: 00000039AA2B8D50: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 00000039AA2B8DF0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 00000039AA2B9070: {[N Value[1 x 1]] }
MPI Rank 3: 00000039AA2B9110: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 00000039AA2B91B0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 00000039AA2B92F0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 00000039AA2B9430: {[SIM Gradient[51 x *]] }
MPI Rank 3: 00000039AA2B9570: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 00000039AA2B9890: {[Query Value[49292 x *]] }
MPI Rank 3: 00000039AA2B9930: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 00000039AA2B99D0: {[SIM Value[51 x *]] }
MPI Rank 3: 00000039AA2B9A70: {[ce Value[1]] }
MPI Rank 3: 00000039AA2B9BB0: {[S Value[1 x 1]] }
MPI Rank 3: 00000039AA2B9CF0: {[G Value[1 x 1]] }
MPI Rank 3: 00000039AA2B9ED0: {[ce Gradient[1]] }
MPI Rank 3: 00000039AA2BA0B0: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 00000039AA2BA150: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 00000039AA2BA290: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 00000039AA2BA3D0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 00000039AA2BA470: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 00000039AA2BA5B0: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 00000039AA2BA650: {[Keyword Value[49292 x *]] }
MPI Rank 3: 00000039AA2BA6F0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/13/2016 04:44:54: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:55: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:55: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/13/2016 04:45:05:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.43899651 * 10240; time = 9.5957s; samplesPerSecond = 1067.1
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/13/2016 04:45:15:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38633995 * 10240; time = 9.6272s; samplesPerSecond = 1063.7
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/13/2016 04:45:19: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.2625s
MPI Rank 3: 07/13/2016 04:45:21: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 3: 07/13/2016 04:45:21: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:24: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:24: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.06 seconds
MPI Rank 3: 07/13/2016 04:45:34:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29999523 * 10240; time = 9.7912s; samplesPerSecond = 1045.8
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.07 seconds
MPI Rank 3: 07/13/2016 04:45:44:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11324863 * 10240; time = 9.9303s; samplesPerSecond = 1031.2
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.07 seconds
MPI Rank 3: 07/13/2016 04:45:49: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.8055s
MPI Rank 3: 07/13/2016 04:45:51: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 3: 07/13/2016 04:45:51: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/13/2016 04:46:04:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90144348 * 10240; time = 10.0593s; samplesPerSecond = 1018.0
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/13/2016 04:46:14:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87299423 * 10240; time = 10.0147s; samplesPerSecond = 1022.5
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.19 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.19 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/13/2016 04:46:19: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.232s
MPI Rank 3: 07/13/2016 04:46:21: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 3: 07/13/2016 04:46:21: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 3: 07/13/2016 04:46:24: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:24: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:24: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:55:32
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on LIANA-09-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/13/2016 04:46:27: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 07/13/2016 04:46:27: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:46:27: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: 		Built time: Jul 13 2016 03:55:32
MPI Rank 0: 07/13/2016 04:46:27: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 0: 07/13/2016 04:46:27: 		Build type: Release
MPI Rank 0: 07/13/2016 04:46:27: 		Build target: GPU
MPI Rank 0: 07/13/2016 04:46:27: 		With 1bit-SGD: no
MPI Rank 0: 07/13/2016 04:46:27: 		Math lib: mkl
MPI Rank 0: 07/13/2016 04:46:27: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/13/2016 04:46:27: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/13/2016 04:46:27: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/13/2016 04:46:27: 		Build Branch: HEAD
MPI Rank 0: 07/13/2016 04:46:27: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 0: 07/13/2016 04:46:27: 		Built by svcphil on LIANA-09-w
MPI Rank 0: 07/13/2016 04:46:27: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/13/2016 04:46:27: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:46:27: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:46:27: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 07/13/2016 04:46:27: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: Running on cntk-muc02 at 2016/07/13 04:46:27
MPI Rank 0: 07/13/2016 04:46:27: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:46:27: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:46:27: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/13/2016 04:46:27: Commands: train
MPI Rank 0: 07/13/2016 04:46:27: Precision = "float"
MPI Rank 0: 07/13/2016 04:46:27: Using 1 CPU threads.
MPI Rank 0: 07/13/2016 04:46:27: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 07/13/2016 04:46:27: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/13/2016 04:46:27: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: ##############################################################################
MPI Rank 0: 07/13/2016 04:46:27: #                                                                            #
MPI Rank 0: 07/13/2016 04:46:27: # Action "train"                                                             #
MPI Rank 0: 07/13/2016 04:46:27: #                                                                            #
MPI Rank 0: 07/13/2016 04:46:27: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:27: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:29: Loaded model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:29: Training criterion node(s):
MPI Rank 0: 07/13/2016 04:46:29: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 00000091CE25E8A0: {[G Value[1 x 1]] }
MPI Rank 0: 00000091CE25EB20: {[S Value[1 x 1]] }
MPI Rank 0: 00000091CE25EC60: {[Query Value[49292 x *1]] }
MPI Rank 0: 00000091CE25ED00: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 00000091CE25F160: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 00000091CE25F480: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 00000091CE25F5C0: {[N Value[1 x 1]] }
MPI Rank 0: 00000091DC8A1C50: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 00000091DC8A1CF0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 00000091DC8A1D90: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 00000091DC8A1F70: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000091DC8A2010: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 00000091DC8A20B0: {[SIM Value[51 x *1]] }
MPI Rank 0: 00000091DC8A2150: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 00000091DC8A2330: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 00000091DC8A23D0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 00000091DC8A2470: {[WD1 Value[64 x 288]] }
MPI Rank 0: 00000091DC8A2510: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 00000091DC8A25B0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 00000091DC8A2650: {[ce Value[1]] }
MPI Rank 0: 00000091DC8A2970: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 00000091DC8A2AB0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000091DC8A2B50: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 00000091DC8A2BF0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 00000091DC8A2C90: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 00000091DC8A3370: {[ce Gradient[1]] }
MPI Rank 0: 00000091DC8A3550: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/13/2016 04:46:29: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.10 seconds
MPI Rank 0: 07/13/2016 04:46:42:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87912178 * 10240; time = 10.1392s; samplesPerSecond = 1009.9
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.09 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.20 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.35 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 2.52 seconds , average latency = 0.13 seconds
MPI Rank 0: 07/13/2016 04:46:53:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79176941 * 10240; time = 10.1576s; samplesPerSecond = 1008.1
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.52 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.52 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.53 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 2.69 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.69 seconds , average latency = 0.11 seconds
MPI Rank 0: 07/13/2016 04:46:57: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.2909s
MPI Rank 0: 07/13/2016 04:46:59: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 0: 07/13/2016 04:46:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 0: 07/13/2016 04:47:01: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 07/13/2016 04:47:03: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:47:03: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:47:03: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/13/2016 04:46:27: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 07/13/2016 04:46:27: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:46:27: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:27: 		Built time: Jul 13 2016 03:55:32
MPI Rank 1: 07/13/2016 04:46:27: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 1: 07/13/2016 04:46:27: 		Build type: Release
MPI Rank 1: 07/13/2016 04:46:27: 		Build target: GPU
MPI Rank 1: 07/13/2016 04:46:27: 		With 1bit-SGD: no
MPI Rank 1: 07/13/2016 04:46:27: 		Math lib: mkl
MPI Rank 1: 07/13/2016 04:46:27: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/13/2016 04:46:27: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/13/2016 04:46:27: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/13/2016 04:46:27: 		Build Branch: HEAD
MPI Rank 1: 07/13/2016 04:46:27: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 1: 07/13/2016 04:46:27: 		Built by svcphil on LIANA-09-w
MPI Rank 1: 07/13/2016 04:46:27: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/13/2016 04:46:27: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:46:28: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: Running on cntk-muc02 at 2016/07/13 04:46:28
MPI Rank 1: 07/13/2016 04:46:28: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:46:28: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:46:28: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/13/2016 04:46:28: Commands: train
MPI Rank 1: 07/13/2016 04:46:28: Precision = "float"
MPI Rank 1: 07/13/2016 04:46:28: Using 1 CPU threads.
MPI Rank 1: 07/13/2016 04:46:28: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 07/13/2016 04:46:28: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/13/2016 04:46:28: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: ##############################################################################
MPI Rank 1: 07/13/2016 04:46:28: #                                                                            #
MPI Rank 1: 07/13/2016 04:46:28: # Action "train"                                                             #
MPI Rank 1: 07/13/2016 04:46:28: #                                                                            #
MPI Rank 1: 07/13/2016 04:46:28: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:28: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:29: Loaded model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:29: Training criterion node(s):
MPI Rank 1: 07/13/2016 04:46:29: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000649B68C740: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000649B68C7E0: {[N Value[1 x 1]] }
MPI Rank 1: 000000649B68C880: {[G Value[1 x 1]] }
MPI Rank 1: 000000649B68C920: {[Query Value[49292 x *1]] }
MPI Rank 1: 000000649B68C9C0: {[S Value[1 x 1]] }
MPI Rank 1: 000000649B68CD80: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 000000649B68CE20: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 00000064BBA3D020: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 00000064BBA3D0C0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 00000064BBA3D160: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 00000064BBA3D200: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 00000064BBA3D340: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 00000064BBA3D3E0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 00000064BBA3D5C0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 00000064BBA3D7A0: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 00000064BBA3D8E0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 00000064BBA3DC00: {[WD1 Value[64 x 288]] }
MPI Rank 1: 00000064BBA3DCA0: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 00000064BBA3DE80: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 00000064BBA3E100: {[ce Gradient[1]] }
MPI Rank 1: 00000064BBA3E380: {[SIM Value[51 x *1]] }
MPI Rank 1: 00000064BBA3E560: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 00000064BBA3E600: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 00000064BBA3E740: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 00000064BBA3E9C0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 00000064BBA3EA60: {[ce Value[1]] }
MPI Rank 1: 00000064BBA3ED80: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/13/2016 04:46:30: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.90 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.96 seconds , average latency = 0.10 seconds
MPI Rank 1: 07/13/2016 04:46:42:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.94872780 * 10240; time = 10.1859s; samplesPerSecond = 1005.3
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.09 seconds
MPI Rank 1: 07/13/2016 04:46:53:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89873848 * 10240; time = 10.1565s; samplesPerSecond = 1008.2
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 0.08 seconds
MPI Rank 1: 07/13/2016 04:46:57: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.2793s
MPI Rank 1: 07/13/2016 04:46:59: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 1: 07/13/2016 04:46:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 1: 07/13/2016 04:47:03: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:47:03: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:47:03: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/13/2016 04:46:28: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:46:28: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: 		Built time: Jul 13 2016 03:55:32
MPI Rank 2: 07/13/2016 04:46:28: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 2: 07/13/2016 04:46:28: 		Build type: Release
MPI Rank 2: 07/13/2016 04:46:28: 		Build target: GPU
MPI Rank 2: 07/13/2016 04:46:28: 		With 1bit-SGD: no
MPI Rank 2: 07/13/2016 04:46:28: 		Math lib: mkl
MPI Rank 2: 07/13/2016 04:46:28: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/13/2016 04:46:28: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/13/2016 04:46:28: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/13/2016 04:46:28: 		Build Branch: HEAD
MPI Rank 2: 07/13/2016 04:46:28: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 2: 07/13/2016 04:46:28: 		Built by svcphil on LIANA-09-w
MPI Rank 2: 07/13/2016 04:46:28: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:46:28: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: Running on cntk-muc02 at 2016/07/13 04:46:28
MPI Rank 2: 07/13/2016 04:46:28: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:46:28: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:46:28: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/13/2016 04:46:28: Commands: train
MPI Rank 2: 07/13/2016 04:46:28: Precision = "float"
MPI Rank 2: 07/13/2016 04:46:28: Using 1 CPU threads.
MPI Rank 2: 07/13/2016 04:46:28: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 07/13/2016 04:46:28: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/13/2016 04:46:28: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: ##############################################################################
MPI Rank 2: 07/13/2016 04:46:28: #                                                                            #
MPI Rank 2: 07/13/2016 04:46:28: # Action "train"                                                             #
MPI Rank 2: 07/13/2016 04:46:28: #                                                                            #
MPI Rank 2: 07/13/2016 04:46:28: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:28: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:30: Loaded model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:30: Training criterion node(s):
MPI Rank 2: 07/13/2016 04:46:30: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 0000007F17C7C7E0: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0000007F17C7CA60: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 0000007F17C7CCE0: {[Query Value[49292 x *1]] }
MPI Rank 2: 0000007F17C7CEC0: {[N Value[1 x 1]] }
MPI Rank 2: 0000007F17C7D000: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 0000007F17C7D460: {[S Value[1 x 1]] }
MPI Rank 2: 0000007F17C7D5A0: {[G Value[1 x 1]] }
MPI Rank 2: 0000007F26663D50: {[SIM Value[51 x *1]] }
MPI Rank 2: 0000007F26663DF0: {[ce Value[1]] }
MPI Rank 2: 0000007F26663E90: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 0000007F266641B0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0000007F26664390: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 0000007F26664570: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 0000007F26664610: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0000007F266647F0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 0000007F26664B10: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0000007F26664CF0: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 0000007F26664D90: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 0000007F26664ED0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 0000007F26664F70: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0000007F266650B0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 0000007F26665330: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 0000007F26665510: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 0000007F26665650: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0000007F26665970: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 0000007F26665A10: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0000007F26665AB0: {[ce Gradient[1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/13/2016 04:46:30: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/13/2016 04:46:42:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96648235 * 10240; time = 10.1856s; samplesPerSecond = 1005.3
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/13/2016 04:46:53:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91512318 * 10240; time = 10.1576s; samplesPerSecond = 1008.1
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/13/2016 04:46:57: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.2802s
MPI Rank 2: 07/13/2016 04:46:59: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 2: 07/13/2016 04:46:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 2: 07/13/2016 04:47:03: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:47:03: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:47:03: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/13/2016 04:46:28: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:46:28: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:28: 		Built time: Jul 13 2016 03:55:32
MPI Rank 3: 07/13/2016 04:46:28: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 3: 07/13/2016 04:46:28: 		Build type: Release
MPI Rank 3: 07/13/2016 04:46:28: 		Build target: GPU
MPI Rank 3: 07/13/2016 04:46:28: 		With 1bit-SGD: no
MPI Rank 3: 07/13/2016 04:46:28: 		Math lib: mkl
MPI Rank 3: 07/13/2016 04:46:28: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/13/2016 04:46:28: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/13/2016 04:46:28: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/13/2016 04:46:28: 		Build Branch: HEAD
MPI Rank 3: 07/13/2016 04:46:28: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 3: 07/13/2016 04:46:28: 		Built by svcphil on LIANA-09-w
MPI Rank 3: 07/13/2016 04:46:28: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/13/2016 04:46:28: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:46:29: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:46:29: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 07/13/2016 04:46:29: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: Running on cntk-muc02 at 2016/07/13 04:46:29
MPI Rank 3: 07/13/2016 04:46:29: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:46:29: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:46:29: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/13/2016 04:46:29: Commands: train
MPI Rank 3: 07/13/2016 04:46:29: Precision = "float"
MPI Rank 3: 07/13/2016 04:46:29: Using 1 CPU threads.
MPI Rank 3: 07/13/2016 04:46:29: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 07/13/2016 04:46:29: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/13/2016 04:46:29: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: ##############################################################################
MPI Rank 3: 07/13/2016 04:46:29: #                                                                            #
MPI Rank 3: 07/13/2016 04:46:29: # Action "train"                                                             #
MPI Rank 3: 07/13/2016 04:46:29: #                                                                            #
MPI Rank 3: 07/13/2016 04:46:29: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:29: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713044220.475327\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:30: Loaded model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:30: Training criterion node(s):
MPI Rank 3: 07/13/2016 04:46:30: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000083E5DB640: {[N Value[1 x 1]] }
MPI Rank 3: 000000083E5DBA00: {[Query Value[49292 x *1]] }
MPI Rank 3: 000000083E5DBBE0: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000083E5DBDC0: {[G Value[1 x 1]] }
MPI Rank 3: 000000083E5DC0E0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 000000083E5DC220: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 000000083E5DC360: {[S Value[1 x 1]] }
MPI Rank 3: 000000084CD3BBC0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 000000084CD3BEE0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000084CD3C020: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 000000084CD3C160: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000084CD3C2A0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 000000084CD3C5C0: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000084CD3C700: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 000000084CD3C840: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 000000084CD3C980: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 000000084CD3CA20: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000084CD3CAC0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 000000084CD3CB60: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 000000084CD3CC00: {[SIM Value[51 x *1]] }
MPI Rank 3: 000000084CD3CDE0: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 000000084CD3CF20: {[ce Value[1]] }
MPI Rank 3: 000000084CD3D060: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000084CD3D240: {[ce Gradient[1]] }
MPI Rank 3: 000000084CD3D380: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000084CD3D600: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 000000084CD3D9C0: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/13/2016 04:46:30: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.08 seconds
MPI Rank 3: 07/13/2016 04:46:42:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91057034 * 10240; time = 10.2008s; samplesPerSecond = 1003.8
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/13/2016 04:46:53:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90474224 * 10240; time = 10.1565s; samplesPerSecond = 1008.2
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 2.12 seconds , average latency = 0.08 seconds
MPI Rank 3: 07/13/2016 04:46:57: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.2793s
MPI Rank 3: 07/13/2016 04:46:59: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 3: 07/13/2016 04:46:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 3: 07/13/2016 04:47:03: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:47:03: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:47:03: __COMPLETED__
MPI Rank 3: ~MPIWrapper