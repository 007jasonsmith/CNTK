CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3550 @ 3.07GHz
    Hardware threads: 4
    Total Memory: 12580388 kB
-------------------------------------------------------------------
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/14/2016 05:43:00: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 07/14/2016 05:43:00: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 05:43:00: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: 		Built time: Jul 14 2016 05:09:49
MPI Rank 0: 07/14/2016 05:43:00: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 0: 07/14/2016 05:43:00: 		Build type: Release
MPI Rank 0: 07/14/2016 05:43:00: 		Build target: GPU
MPI Rank 0: 07/14/2016 05:43:00: 		With 1bit-SGD: no
MPI Rank 0: 07/14/2016 05:43:00: 		Math lib: mkl
MPI Rank 0: 07/14/2016 05:43:00: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/14/2016 05:43:00: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/14/2016 05:43:00: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/14/2016 05:43:00: 		Build Branch: HEAD
MPI Rank 0: 07/14/2016 05:43:00: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 0: 07/14/2016 05:43:00: 		Built by svcphil on DPHAIM-24
MPI Rank 0: 07/14/2016 05:43:00: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/14/2016 05:43:00: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 05:43:00: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 05:43:00: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 07/14/2016 05:43:00: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: Running on cntk-muc02 at 2016/07/14 05:43:00
MPI Rank 0: 07/14/2016 05:43:00: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 05:43:00: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 05:43:00: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/14/2016 05:43:00: Commands: train
MPI Rank 0: 07/14/2016 05:43:00: Precision = "float"
MPI Rank 0: 07/14/2016 05:43:00: Using 1 CPU threads.
MPI Rank 0: 07/14/2016 05:43:00: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 07/14/2016 05:43:00: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/14/2016 05:43:00: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: ##############################################################################
MPI Rank 0: 07/14/2016 05:43:00: #                                                                            #
MPI Rank 0: 07/14/2016 05:43:00: # Action "train"                                                             #
MPI Rank 0: 07/14/2016 05:43:00: #                                                                            #
MPI Rank 0: 07/14/2016 05:43:00: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:00: Creating virgin network.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:01: Created model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:01: Training criterion node(s):
MPI Rank 0: 07/14/2016 05:43:01: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 00000009A5EFE650: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 00000009A5EFE790: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 00000009A5EFE830: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 00000009A5EFE970: {[ce Value[1]] }
MPI Rank 0: 00000009A5EFED30: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 00000009A5EFEE70: {[S Value[1 x 1]] }
MPI Rank 0: 00000009A5EFEF10: {[SIM Value[51 x *]] }
MPI Rank 0: 00000009A5EFEFB0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 00000009A5EFF050: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 00000009A5EFF0F0: {[ce Gradient[1]] }
MPI Rank 0: 00000009A5EFF230: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 00000009A5EFF2D0: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 00000009A5EFF410: {[N Value[1 x 1]] }
MPI Rank 0: 00000009A5EFF550: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 00000009A5EFF5F0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 00000009A5EFF7D0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 00000009A5EFFB90: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 00000009A5EFFC30: {[G Value[1 x 1]] }
MPI Rank 0: 00000009A5EFFE10: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 00000009A5EFFF50: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 00000009A5EFFFF0: {[SIM Gradient[51 x *]] }
MPI Rank 0: 00000009A5F003B0: {[Keyword Value[49292 x *]] }
MPI Rank 0: 00000009B4960E80: {[WD1 Value[64 x 288]] }
MPI Rank 0: 00000009B4960FC0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 00000009B4961740: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 00000009B49619C0: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 00000009B4961BA0: {[Query Value[49292 x *]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/14/2016 05:43:01: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:04: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/14/2016 05:43:14:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.41944122 * 10240; time = 10.1571s; samplesPerSecond = 1008.2
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.96 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/14/2016 05:43:24:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38406754 * 10240; time = 10.1313s; samplesPerSecond = 1010.7
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.92 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.92 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.92 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/14/2016 05:43:29: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=25.5652s
MPI Rank 0: 07/14/2016 05:43:32: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 0: 07/14/2016 05:43:32: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 0: 07/14/2016 05:43:33: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:34: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:43:34: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/14/2016 05:43:44:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29922523 * 10240; time = 10.0842s; samplesPerSecond = 1015.4
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/14/2016 05:43:54:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.08742409 * 10240; time = 10.0127s; samplesPerSecond = 1022.7
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.05 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 2.06 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/14/2016 05:44:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=25.3005s
MPI Rank 0: 07/14/2016 05:44:02: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 0: 07/14/2016 05:44:02: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 0: 07/14/2016 05:44:03: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:05: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.77 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/14/2016 05:44:15:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90042439 * 10240; time = 9.9819s; samplesPerSecond = 1025.9
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/14/2016 05:44:25:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.85719700 * 10240; time = 10.0251s; samplesPerSecond = 1021.4
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/14/2016 05:44:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.9582s
MPI Rank 0: 07/14/2016 05:44:32: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 0: 07/14/2016 05:44:32: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 0: 07/14/2016 05:44:33: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 07/14/2016 05:44:34: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:34: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:34: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/14/2016 05:43:00: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 07/14/2016 05:43:00: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 05:43:00: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:00: 		Built time: Jul 14 2016 05:09:49
MPI Rank 1: 07/14/2016 05:43:00: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 1: 07/14/2016 05:43:00: 		Build type: Release
MPI Rank 1: 07/14/2016 05:43:00: 		Build target: GPU
MPI Rank 1: 07/14/2016 05:43:00: 		With 1bit-SGD: no
MPI Rank 1: 07/14/2016 05:43:00: 		Math lib: mkl
MPI Rank 1: 07/14/2016 05:43:00: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/14/2016 05:43:00: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/14/2016 05:43:00: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/14/2016 05:43:00: 		Build Branch: HEAD
MPI Rank 1: 07/14/2016 05:43:00: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 1: 07/14/2016 05:43:00: 		Built by svcphil on DPHAIM-24
MPI Rank 1: 07/14/2016 05:43:00: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/14/2016 05:43:00: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 05:43:01: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: Running on cntk-muc02 at 2016/07/14 05:43:01
MPI Rank 1: 07/14/2016 05:43:01: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 05:43:01: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 05:43:01: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/14/2016 05:43:01: Commands: train
MPI Rank 1: 07/14/2016 05:43:01: Precision = "float"
MPI Rank 1: 07/14/2016 05:43:01: Using 1 CPU threads.
MPI Rank 1: 07/14/2016 05:43:01: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 07/14/2016 05:43:01: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/14/2016 05:43:01: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: ##############################################################################
MPI Rank 1: 07/14/2016 05:43:01: #                                                                            #
MPI Rank 1: 07/14/2016 05:43:01: # Action "train"                                                             #
MPI Rank 1: 07/14/2016 05:43:01: #                                                                            #
MPI Rank 1: 07/14/2016 05:43:01: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: Creating virgin network.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: Created model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:01: Training criterion node(s):
MPI Rank 1: 07/14/2016 05:43:01: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 0000002FD49BFE60: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 0000002FD49BFF00: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 0000002FD49BFFA0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 0000002FD49C0040: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 0000002FD49C0220: {[ce Gradient[1]] }
MPI Rank 1: 0000002FD49C02C0: {[SIM Gradient[51 x *]] }
MPI Rank 1: 0000002FD49C0540: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0000002FD49C0720: {[S Value[1 x 1]] }
MPI Rank 1: 0000002FD49C07C0: {[G Value[1 x 1]] }
MPI Rank 1: 0000002FD49C09A0: {[Keyword Value[49292 x *]] }
MPI Rank 1: 0000002FD49C0A40: {[SIM Value[51 x *]] }
MPI Rank 1: 0000002FD49C0AE0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 0000002FD49C0E00: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 0000002FD49C0EA0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0000002FD49C0FE0: {[N Value[1 x 1]] }
MPI Rank 1: 0000002FD49C1080: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 0000002FD49C1440: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 0000002FD49C14E0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 0000002FD49C16C0: {[ce Value[1]] }
MPI Rank 1: 0000002FD49C1760: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 0000002FD49C1800: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 0000002FD49C18A0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 0000002FD7807330: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0000002FD7807470: {[Query Value[49292 x *]] }
MPI Rank 1: 0000002FD7807AB0: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0000002FD7807BF0: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0000002FD7807FB0: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/14/2016 05:43:01: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:04: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.10 seconds
MPI Rank 1: 07/14/2016 05:43:14:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.42829742 * 10240; time = 10.2261s; samplesPerSecond = 1001.4
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 0.10 seconds
MPI Rank 1: 07/14/2016 05:43:24:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38771057 * 10240; time = 10.1455s; samplesPerSecond = 1009.3
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 2.15 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 2.32 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 2.49 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 2.55 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.55 seconds , average latency = 0.10 seconds
MPI Rank 1: 07/14/2016 05:43:29: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=25.5645s
MPI Rank 1: 07/14/2016 05:43:32: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 1: 07/14/2016 05:43:32: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:34: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:43:34: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.09 seconds
MPI Rank 1: 07/14/2016 05:43:44:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34065857 * 10240; time = 10.0605s; samplesPerSecond = 1017.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 0.08 seconds
MPI Rank 1: 07/14/2016 05:43:54:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11009483 * 10240; time = 10.0126s; samplesPerSecond = 1022.7
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.07 seconds
MPI Rank 1: 07/14/2016 05:44:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=25.3003s
MPI Rank 1: 07/14/2016 05:44:02: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 1: 07/14/2016 05:44:02: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:05: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.06 seconds
MPI Rank 1: 07/14/2016 05:44:15:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93978710 * 10240; time = 9.9571s; samplesPerSecond = 1028.4
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 0.06 seconds
MPI Rank 1: 07/14/2016 05:44:25:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86772938 * 10240; time = 10.0838s; samplesPerSecond = 1015.5
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.05 seconds
MPI Rank 1: 07/14/2016 05:44:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.9579s
MPI Rank 1: 07/14/2016 05:44:32: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 1: 07/14/2016 05:44:32: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 1: 07/14/2016 05:44:34: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:34: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:34: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/14/2016 05:43:01: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 05:43:01: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: 		Built time: Jul 14 2016 05:09:49
MPI Rank 2: 07/14/2016 05:43:01: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 2: 07/14/2016 05:43:01: 		Build type: Release
MPI Rank 2: 07/14/2016 05:43:01: 		Build target: GPU
MPI Rank 2: 07/14/2016 05:43:01: 		With 1bit-SGD: no
MPI Rank 2: 07/14/2016 05:43:01: 		Math lib: mkl
MPI Rank 2: 07/14/2016 05:43:01: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/14/2016 05:43:01: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/14/2016 05:43:01: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/14/2016 05:43:01: 		Build Branch: HEAD
MPI Rank 2: 07/14/2016 05:43:01: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 2: 07/14/2016 05:43:01: 		Built by svcphil on DPHAIM-24
MPI Rank 2: 07/14/2016 05:43:01: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 05:43:01: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: Running on cntk-muc02 at 2016/07/14 05:43:01
MPI Rank 2: 07/14/2016 05:43:01: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 05:43:01: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 05:43:01: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/14/2016 05:43:01: Commands: train
MPI Rank 2: 07/14/2016 05:43:01: Precision = "float"
MPI Rank 2: 07/14/2016 05:43:01: Using 1 CPU threads.
MPI Rank 2: 07/14/2016 05:43:01: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 07/14/2016 05:43:01: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/14/2016 05:43:01: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: ##############################################################################
MPI Rank 2: 07/14/2016 05:43:01: #                                                                            #
MPI Rank 2: 07/14/2016 05:43:01: # Action "train"                                                             #
MPI Rank 2: 07/14/2016 05:43:01: #                                                                            #
MPI Rank 2: 07/14/2016 05:43:01: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:01: Creating virgin network.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:02: Created model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:02: Training criterion node(s):
MPI Rank 2: 07/14/2016 05:43:02: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000E0D5EFF0E0: {[G Value[1 x 1]] }
MPI Rank 2: 000000E0D5EFF5E0: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 000000E0D5EFF680: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000E0D5EFF860: {[Keyword Value[49292 x *]] }
MPI Rank 2: 000000E0D5EFFC20: {[SIM Value[51 x *]] }
MPI Rank 2: 000000E0D5EFFCC0: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 000000E0D5EFFD60: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 000000E0D5EFFE00: {[Query Value[49292 x *]] }
MPI Rank 2: 000000E0D5EFFF40: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 000000E0D5EFFFE0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 000000E0D5F00080: {[ce Value[1]] }
MPI Rank 2: 000000E0D5F00120: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000E0D5F001C0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 000000E0D5F00260: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 000000E0D5F00300: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 000000E0D5F00800: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 000000E0D5F00940: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 000000E0D5F00A80: {[S Value[1 x 1]] }
MPI Rank 2: 000000E0D5F00E40: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 000000E0D5F00F80: {[N Value[1 x 1]] }
MPI Rank 2: 000000E0D5F01410: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 000000E0D5F014B0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 000000E0D5F01870: {[ce Gradient[1]] }
MPI Rank 2: 000000E0D5F01AF0: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 000000E0D5F01B90: {[SIM Gradient[51 x *]] }
MPI Rank 2: 000000E0D5F01D70: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000E0D5F01EB0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/14/2016 05:43:02: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:04: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 05:43:14:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.44101372 * 10240; time = 10.1966s; samplesPerSecond = 1004.3
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/14/2016 05:43:24:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.39723701 * 10240; time = 10.2024s; samplesPerSecond = 1003.7
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 05:43:29: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=25.5648s
MPI Rank 2: 07/14/2016 05:43:32: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 2: 07/14/2016 05:43:32: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:34: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:43:34: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.05 seconds
MPI Rank 2: 07/14/2016 05:43:44:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34435501 * 10240; time = 10.0849s; samplesPerSecond = 1015.4
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.96 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 05:43:54:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.13005295 * 10240; time = 10.0127s; samplesPerSecond = 1022.7
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 05:44:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=25.2988s
MPI Rank 2: 07/14/2016 05:44:02: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 2: 07/14/2016 05:44:02: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:05: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 05:44:15:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95727177 * 10240; time = 9.9692s; samplesPerSecond = 1027.2
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/14/2016 05:44:25:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88702602 * 10240; time = 10.0821s; samplesPerSecond = 1015.7
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 05:44:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.956s
MPI Rank 2: 07/14/2016 05:44:32: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 2: 07/14/2016 05:44:32: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 2: 07/14/2016 05:44:34: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:34: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:34: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/14/2016 05:43:01: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 05:43:01: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:01: 		Built time: Jul 14 2016 05:09:49
MPI Rank 3: 07/14/2016 05:43:01: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 3: 07/14/2016 05:43:01: 		Build type: Release
MPI Rank 3: 07/14/2016 05:43:01: 		Build target: GPU
MPI Rank 3: 07/14/2016 05:43:01: 		With 1bit-SGD: no
MPI Rank 3: 07/14/2016 05:43:01: 		Math lib: mkl
MPI Rank 3: 07/14/2016 05:43:01: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/14/2016 05:43:01: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/14/2016 05:43:01: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/14/2016 05:43:01: 		Build Branch: HEAD
MPI Rank 3: 07/14/2016 05:43:01: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 3: 07/14/2016 05:43:01: 		Built by svcphil on DPHAIM-24
MPI Rank 3: 07/14/2016 05:43:01: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/14/2016 05:43:01: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 05:43:02: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 05:43:02: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 07/14/2016 05:43:02: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: Running on cntk-muc02 at 2016/07/14 05:43:02
MPI Rank 3: 07/14/2016 05:43:02: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 05:43:02: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 05:43:02: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/14/2016 05:43:02: Commands: train
MPI Rank 3: 07/14/2016 05:43:02: Precision = "float"
MPI Rank 3: 07/14/2016 05:43:02: Using 1 CPU threads.
MPI Rank 3: 07/14/2016 05:43:02: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 07/14/2016 05:43:02: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/14/2016 05:43:02: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: ##############################################################################
MPI Rank 3: 07/14/2016 05:43:02: #                                                                            #
MPI Rank 3: 07/14/2016 05:43:02: # Action "train"                                                             #
MPI Rank 3: 07/14/2016 05:43:02: #                                                                            #
MPI Rank 3: 07/14/2016 05:43:02: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: Creating virgin network.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: Created model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:02: Training criterion node(s):
MPI Rank 3: 07/14/2016 05:43:02: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000CD60290FD0: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000CD60291070: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000CD60291250: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000CD60291430: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000CD60291570: {[Query Value[49292 x *]] }
MPI Rank 3: 000000CD60465B50: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 000000CD60465D30: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000CD60466050: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 000000CD604660F0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 000000CD60466190: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 000000CD60466230: {[G Value[1 x 1]] }
MPI Rank 3: 000000CD604662D0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 000000CD60466550: {[ce Gradient[1]] }
MPI Rank 3: 000000CD604665F0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000CD60466690: {[SIM Gradient[51 x *]] }
MPI Rank 3: 000000CD60466730: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000CD604667D0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000CD60466C30: {[S Value[1 x 1]] }
MPI Rank 3: 000000CD60466CD0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 000000CD60466D70: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 000000CD60466EB0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 000000CD60467270: {[Keyword Value[49292 x *]] }
MPI Rank 3: 000000CD604673B0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 000000CD60467590: {[N Value[1 x 1]] }
MPI Rank 3: 000000CD604676D0: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 000000CD60467950: {[SIM Value[51 x *]] }
MPI Rank 3: 000000CD604679F0: {[ce Value[1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/14/2016 05:43:02: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:04: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.16 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/14/2016 05:43:14:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.43899651 * 10240; time = 10.2502s; samplesPerSecond = 999.0
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 0.10 seconds
MPI Rank 3: 07/14/2016 05:43:24:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38633995 * 10240; time = 10.1455s; samplesPerSecond = 1009.3
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.15 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/14/2016 05:43:29: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=25.5642s
MPI Rank 3: 07/14/2016 05:43:32: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 3: 07/14/2016 05:43:32: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:34: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:43:34: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.11 seconds
MPI Rank 3: 07/14/2016 05:43:44:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29999523 * 10240; time = 10.0847s; samplesPerSecond = 1015.4
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/14/2016 05:43:54:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11324863 * 10240; time = 9.9557s; samplesPerSecond = 1028.6
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 2.12 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 2.29 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.45 seconds , average latency = 0.10 seconds
MPI Rank 3: 07/14/2016 05:44:00: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=25.2986s
MPI Rank 3: 07/14/2016 05:44:02: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 3: 07/14/2016 05:44:02: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:05: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:05: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.90 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/14/2016 05:44:15:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90144348 * 10240; time = 9.9821s; samplesPerSecond = 1025.8
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 0.11 seconds
MPI Rank 3: 07/14/2016 05:44:25:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87299423 * 10240; time = 10.1376s; samplesPerSecond = 1010.1
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.14 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 2.18 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 2.33 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/14/2016 05:44:29: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.9569s
MPI Rank 3: 07/14/2016 05:44:32: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 3: 07/14/2016 05:44:32: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 3: 07/14/2016 05:44:34: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:34: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:34: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:09:49
		Last modified date: Fri Jul  8 10:29:39 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on DPHAIM-24
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/14/2016 05:44:37: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 07/14/2016 05:44:37: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 05:44:37: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: 		Built time: Jul 14 2016 05:09:49
MPI Rank 0: 07/14/2016 05:44:37: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 0: 07/14/2016 05:44:37: 		Build type: Release
MPI Rank 0: 07/14/2016 05:44:37: 		Build target: GPU
MPI Rank 0: 07/14/2016 05:44:37: 		With 1bit-SGD: no
MPI Rank 0: 07/14/2016 05:44:37: 		Math lib: mkl
MPI Rank 0: 07/14/2016 05:44:37: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/14/2016 05:44:37: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/14/2016 05:44:37: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/14/2016 05:44:37: 		Build Branch: HEAD
MPI Rank 0: 07/14/2016 05:44:37: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 0: 07/14/2016 05:44:37: 		Built by svcphil on DPHAIM-24
MPI Rank 0: 07/14/2016 05:44:37: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/14/2016 05:44:37: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 05:44:37: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 05:44:37: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 07/14/2016 05:44:37: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: Running on cntk-muc02 at 2016/07/14 05:44:37
MPI Rank 0: 07/14/2016 05:44:37: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 05:44:37: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 05:44:37: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/14/2016 05:44:37: Commands: train
MPI Rank 0: 07/14/2016 05:44:37: Precision = "float"
MPI Rank 0: 07/14/2016 05:44:37: Using 1 CPU threads.
MPI Rank 0: 07/14/2016 05:44:37: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 07/14/2016 05:44:37: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/14/2016 05:44:37: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: ##############################################################################
MPI Rank 0: 07/14/2016 05:44:37: #                                                                            #
MPI Rank 0: 07/14/2016 05:44:37: # Action "train"                                                             #
MPI Rank 0: 07/14/2016 05:44:37: #                                                                            #
MPI Rank 0: 07/14/2016 05:44:37: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:37: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:39: Loaded model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:39: Training criterion node(s):
MPI Rank 0: 07/14/2016 05:44:39: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 00000010FB0A9BE0: {[SIM Value[51 x *1]] }
MPI Rank 0: 00000010FB0A9D20: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000010FB0A9E60: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 00000010FB0AA220: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 00000010FB0AA400: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 00000010FB0AA4A0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 00000010FB0AA5E0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 00000010FB0AA720: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 00000010FB0AA860: {[ce Gradient[1]] }
MPI Rank 0: 00000010FB0AAC20: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 00000010FB0AAFE0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 00000010FB0AB580: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 00000010FB0AB620: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 00000010FB0AB6C0: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 00000010FB0AB760: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 00000010FB0AB800: {[ce Value[1]] }
MPI Rank 0: 00000010FB0AB8A0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 00000010FB0AB9E0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000010FB0ABA80: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 00000010FDC4C7E0: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 00000010FDC4CC40: {[G Value[1 x 1]] }
MPI Rank 0: 00000010FDC4CCE0: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 00000010FDC4CF60: {[N Value[1 x 1]] }
MPI Rank 0: 00000010FDC4D0A0: {[S Value[1 x 1]] }
MPI Rank 0: 00000010FDC4D1E0: {[WD1 Value[64 x 288]] }
MPI Rank 0: 00000010FDC4D500: {[Query Value[49292 x *1]] }
MPI Rank 0: 00000010FDC4D6E0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/14/2016 05:44:39: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:42: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:44:42: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.06 seconds
MPI Rank 0: 07/14/2016 05:44:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87912178 * 10240; time = 9.8096s; samplesPerSecond = 1043.9
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.06 seconds
MPI Rank 0: 07/14/2016 05:45:02:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79176941 * 10240; time = 9.6839s; samplesPerSecond = 1057.4
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.06 seconds
MPI Rank 0: 07/14/2016 05:45:07: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.6172s
MPI Rank 0: 07/14/2016 05:45:09: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 0: 07/14/2016 05:45:09: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 0: 07/14/2016 05:45:10: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 07/14/2016 05:45:12: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:45:12: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 05:45:12: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/14/2016 05:44:37: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 07/14/2016 05:44:37: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 05:44:37: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:37: 		Built time: Jul 14 2016 05:09:49
MPI Rank 1: 07/14/2016 05:44:37: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 1: 07/14/2016 05:44:37: 		Build type: Release
MPI Rank 1: 07/14/2016 05:44:37: 		Build target: GPU
MPI Rank 1: 07/14/2016 05:44:37: 		With 1bit-SGD: no
MPI Rank 1: 07/14/2016 05:44:37: 		Math lib: mkl
MPI Rank 1: 07/14/2016 05:44:37: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/14/2016 05:44:37: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/14/2016 05:44:37: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/14/2016 05:44:37: 		Build Branch: HEAD
MPI Rank 1: 07/14/2016 05:44:37: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 1: 07/14/2016 05:44:37: 		Built by svcphil on DPHAIM-24
MPI Rank 1: 07/14/2016 05:44:37: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/14/2016 05:44:37: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 05:44:38: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: Running on cntk-muc02 at 2016/07/14 05:44:38
MPI Rank 1: 07/14/2016 05:44:38: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 05:44:38: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 05:44:38: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/14/2016 05:44:38: Commands: train
MPI Rank 1: 07/14/2016 05:44:38: Precision = "float"
MPI Rank 1: 07/14/2016 05:44:38: Using 1 CPU threads.
MPI Rank 1: 07/14/2016 05:44:38: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 07/14/2016 05:44:38: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/14/2016 05:44:38: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: ##############################################################################
MPI Rank 1: 07/14/2016 05:44:38: #                                                                            #
MPI Rank 1: 07/14/2016 05:44:38: # Action "train"                                                             #
MPI Rank 1: 07/14/2016 05:44:38: #                                                                            #
MPI Rank 1: 07/14/2016 05:44:38: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:38: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:39: Loaded model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:39: Training criterion node(s):
MPI Rank 1: 07/14/2016 05:44:39: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000623D871280: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000623D871320: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 000000623D8715A0: {[S Value[1 x 1]] }
MPI Rank 1: 000000623D871780: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000623D871BE0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 000000623D871DC0: {[G Value[1 x 1]] }
MPI Rank 1: 000000623D871E60: {[N Value[1 x 1]] }
MPI Rank 1: 000000623D872040: {[Query Value[49292 x *1]] }
MPI Rank 1: 000000624C4C8570: {[ce Value[1]] }
MPI Rank 1: 000000624C4C8610: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 000000624C4C8890: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 000000624C4C8930: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 000000624C4C8BB0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 000000624C4C8D90: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 000000624C4C9010: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 000000624C4C9150: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 000000624C4C91F0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 000000624C4C9330: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000624C4C93D0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 000000624C4C9470: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 000000624C4C9510: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 000000624C4C96F0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 000000624C4C9790: {[ce Gradient[1]] }
MPI Rank 1: 000000624C4C9830: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 000000624C4C9AB0: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 000000624C4C9C90: {[SIM Value[51 x *1]] }
MPI Rank 1: 000000624C4C9D30: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/14/2016 05:44:40: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:42: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:44:42: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.06 seconds
MPI Rank 1: 07/14/2016 05:44:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.94872780 * 10240; time = 9.7841s; samplesPerSecond = 1046.6
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.05 seconds
MPI Rank 1: 07/14/2016 05:45:02:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89873848 * 10240; time = 9.7407s; samplesPerSecond = 1051.3
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.06 seconds
MPI Rank 1: 07/14/2016 05:45:07: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.6176s
MPI Rank 1: 07/14/2016 05:45:09: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 1: 07/14/2016 05:45:09: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 1: 07/14/2016 05:45:12: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:45:12: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 05:45:12: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/14/2016 05:44:38: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 05:44:38: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: 		Built time: Jul 14 2016 05:09:49
MPI Rank 2: 07/14/2016 05:44:38: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 2: 07/14/2016 05:44:38: 		Build type: Release
MPI Rank 2: 07/14/2016 05:44:38: 		Build target: GPU
MPI Rank 2: 07/14/2016 05:44:38: 		With 1bit-SGD: no
MPI Rank 2: 07/14/2016 05:44:38: 		Math lib: mkl
MPI Rank 2: 07/14/2016 05:44:38: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/14/2016 05:44:38: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/14/2016 05:44:38: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/14/2016 05:44:38: 		Build Branch: HEAD
MPI Rank 2: 07/14/2016 05:44:38: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 2: 07/14/2016 05:44:38: 		Built by svcphil on DPHAIM-24
MPI Rank 2: 07/14/2016 05:44:38: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 05:44:38: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: Running on cntk-muc02 at 2016/07/14 05:44:38
MPI Rank 2: 07/14/2016 05:44:38: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 05:44:38: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 05:44:38: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/14/2016 05:44:38: Commands: train
MPI Rank 2: 07/14/2016 05:44:38: Precision = "float"
MPI Rank 2: 07/14/2016 05:44:38: Using 1 CPU threads.
MPI Rank 2: 07/14/2016 05:44:38: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 07/14/2016 05:44:38: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/14/2016 05:44:38: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: ##############################################################################
MPI Rank 2: 07/14/2016 05:44:38: #                                                                            #
MPI Rank 2: 07/14/2016 05:44:38: # Action "train"                                                             #
MPI Rank 2: 07/14/2016 05:44:38: #                                                                            #
MPI Rank 2: 07/14/2016 05:44:38: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:38: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:40: Loaded model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:40: Training criterion node(s):
MPI Rank 2: 07/14/2016 05:44:40: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 00000059F9819D30: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 00000059F9819DD0: {[ce Gradient[1]] }
MPI Rank 2: 00000059F981A0F0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 00000059F981A190: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 00000059F981A2D0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 00000059F981A410: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 00000059F981A730: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 00000059F981A7D0: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 00000059F981A910: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 00000059F981A9B0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 00000059F981AA50: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 00000059F981AE10: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 00000059F981AEB0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 00000059F981AFF0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 00000059F981B090: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 00000059F981B270: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 00000059F981B3B0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 00000059F981B950: {[ce Value[1]] }
MPI Rank 2: 00000059F981BA90: {[SIM Value[51 x *1]] }
MPI Rank 2: 00000059F99F8A00: {[Query Value[49292 x *1]] }
MPI Rank 2: 00000059F99F8D20: {[N Value[1 x 1]] }
MPI Rank 2: 00000059F99F8DC0: {[WD1 Value[64 x 288]] }
MPI Rank 2: 00000059F99F8F00: {[S Value[1 x 1]] }
MPI Rank 2: 00000059F99F9180: {[G Value[1 x 1]] }
MPI Rank 2: 00000059F99F92C0: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 00000059F99F9360: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 00000059F99F9720: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/14/2016 05:44:40: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:42: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:44:42: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 05:44:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96648235 * 10240; time = 9.8217s; samplesPerSecond = 1042.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.77 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/14/2016 05:45:02:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91512318 * 10240; time = 9.6840s; samplesPerSecond = 1057.4
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/14/2016 05:45:07: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.617s
MPI Rank 2: 07/14/2016 05:45:09: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 2: 07/14/2016 05:45:09: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 2: 07/14/2016 05:45:12: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:45:12: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 05:45:12: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/14/2016 05:44:38: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 05:44:38: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:38: 		Built time: Jul 14 2016 05:09:49
MPI Rank 3: 07/14/2016 05:44:38: 		Last modified date: Fri Jul  8 10:29:39 2016
MPI Rank 3: 07/14/2016 05:44:38: 		Build type: Release
MPI Rank 3: 07/14/2016 05:44:38: 		Build target: GPU
MPI Rank 3: 07/14/2016 05:44:38: 		With 1bit-SGD: no
MPI Rank 3: 07/14/2016 05:44:38: 		Math lib: mkl
MPI Rank 3: 07/14/2016 05:44:38: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/14/2016 05:44:38: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/14/2016 05:44:38: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/14/2016 05:44:38: 		Build Branch: HEAD
MPI Rank 3: 07/14/2016 05:44:38: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 3: 07/14/2016 05:44:38: 		Built by svcphil on DPHAIM-24
MPI Rank 3: 07/14/2016 05:44:38: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/14/2016 05:44:38: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 05:44:39: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 05:44:39: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 07/14/2016 05:44:39: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: Running on cntk-muc02 at 2016/07/14 05:44:39
MPI Rank 3: 07/14/2016 05:44:39: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 05:44:39: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 05:44:39: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/14/2016 05:44:39: Commands: train
MPI Rank 3: 07/14/2016 05:44:39: Precision = "float"
MPI Rank 3: 07/14/2016 05:44:39: Using 1 CPU threads.
MPI Rank 3: 07/14/2016 05:44:39: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 07/14/2016 05:44:39: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/14/2016 05:44:39: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: ##############################################################################
MPI Rank 3: 07/14/2016 05:44:39: #                                                                            #
MPI Rank 3: 07/14/2016 05:44:39: # Action "train"                                                             #
MPI Rank 3: 07/14/2016 05:44:39: #                                                                            #
MPI Rank 3: 07/14/2016 05:44:39: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:39: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714053622.931850\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:40: Loaded model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:40: Training criterion node(s):
MPI Rank 3: 07/14/2016 05:44:40: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000279EFF2BF0: {[G Value[1 x 1]] }
MPI Rank 3: 000000279EFF2E70: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 000000279EFF2FB0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 00000027ADB1C0A0: {[WD1 Value[64 x 288]] }
MPI Rank 3: 00000027ADB1C3C0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 00000027ADB1C460: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 00000027ADB1C5A0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 00000027ADB1C6E0: {[Query Value[49292 x *1]] }
MPI Rank 3: 00000027ADB1C780: {[N Value[1 x 1]] }
MPI Rank 3: 00000027ADB1C820: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 00000027ADB1CA00: {[SIM Value[51 x *1]] }
MPI Rank 3: 00000027ADB1CAA0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 00000027ADB1CBE0: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 00000027ADB1CE60: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 00000027ADB1CFA0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 00000027ADB1D040: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 00000027ADB1D0E0: {[ce Value[1]] }
MPI Rank 3: 00000027ADB1D220: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 00000027ADB1D360: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 00000027ADB1D4A0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 00000027ADB1D540: {[S Value[1 x 1]] }
MPI Rank 3: 00000027ADB1D720: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 00000027ADB1D9A0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 00000027ADB1DA40: {[ce Gradient[1]] }
MPI Rank 3: 00000027ADB1DAE0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 00000027ADB1DB80: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 00000027ADB1DC20: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/14/2016 05:44:40: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:42: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:44:42: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.09 seconds
MPI Rank 3: 07/14/2016 05:44:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91057034 * 10240; time = 9.8897s; samplesPerSecond = 1035.4
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.90 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.06 seconds
MPI Rank 3: 07/14/2016 05:45:02:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90474224 * 10240; time = 9.6304s; samplesPerSecond = 1063.3
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 0.06 seconds
MPI Rank 3: 07/14/2016 05:45:07: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.6176s
MPI Rank 3: 07/14/2016 05:45:09: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 3: 07/14/2016 05:45:09: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 3: 07/14/2016 05:45:12: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:45:12: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 05:45:12: __COMPLETED__
MPI Rank 3: ~MPIWrapper