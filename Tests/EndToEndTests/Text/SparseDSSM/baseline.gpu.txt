CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu DeviceId=0 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
07/13/2016 12:23:51: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank0
07/13/2016 12:23:52: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank1
07/13/2016 12:23:52: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank2
07/13/2016 12:23:53: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank3
MPI Rank 0: 07/13/2016 12:23:51: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 12:23:51: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:51: 		Built time: Jul 13 2016 11:58:00
MPI Rank 0: 07/13/2016 12:23:51: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 0: 07/13/2016 12:23:51: 		Build type: debug
MPI Rank 0: 07/13/2016 12:23:51: 		Build target: GPU
MPI Rank 0: 07/13/2016 12:23:51: 		With 1bit-SGD: no
MPI Rank 0: 07/13/2016 12:23:51: 		Math lib: mkl
MPI Rank 0: 07/13/2016 12:23:51: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 07/13/2016 12:23:51: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 07/13/2016 12:23:51: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 07/13/2016 12:23:51: 		Build Branch: HEAD
MPI Rank 0: 07/13/2016 12:23:51: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 0: 07/13/2016 12:23:51: 		Built by philly on 2bc22072e267
MPI Rank 0: 07/13/2016 12:23:51: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 07/13/2016 12:23:51: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 12:23:53: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 12:23:53: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:23:53: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:23:53: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:23:53: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:23:53: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: Running on localhost at 2016/07/13 12:23:53
MPI Rank 0: 07/13/2016 12:23:53: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 12:23:53: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 12:23:53: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/13/2016 12:23:53: Commands: train
MPI Rank 0: 07/13/2016 12:23:53: Precision = "float"
MPI Rank 0: 07/13/2016 12:23:53: Using 6 CPU threads.
MPI Rank 0: 07/13/2016 12:23:53: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: 07/13/2016 12:23:53: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/13/2016 12:23:53: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: ##############################################################################
MPI Rank 0: 07/13/2016 12:23:53: #                                                                            #
MPI Rank 0: 07/13/2016 12:23:53: # Action "train"                                                             #
MPI Rank 0: 07/13/2016 12:23:53: #                                                                            #
MPI Rank 0: 07/13/2016 12:23:53: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:53: Creating virgin network.
MPI Rank 0: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:54: Created model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:54: Training criterion node(s):
MPI Rank 0: 07/13/2016 12:23:54: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x7f1b978db1d8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 0x7f1b978db398: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 0x7f1b978db558: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 0x7f1b978dbe18: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x7f1b978dc388: {[ce Gradient[1]] }
MPI Rank 0: 0x7f1b978dc548: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x7f1b978dc708: {[SIM Gradient[51 x *]] }
MPI Rank 0: 0x7f1b978dca88: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 0x7f1b978dce68: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x7f1b978dcf08: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 0x7f1b978e6e98: {[SIM Value[51 x *]] }
MPI Rank 0: 0x7f1b978e7be8: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 0x7f1b978e9058: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 0x7f1b978e9398: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 0x7f1b978e9b38: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 0x7f1b978e9cf8: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 0x7f1b97ad55d8: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x7f1b97ad6188: {[Query Value[49292 x *]] }
MPI Rank 0: 0x7f1b97ad8028: {[Keyword Value[49292 x *]] }
MPI Rank 0: 0x7f1b97ad83a8: {[S Value[1 x 1]] }
MPI Rank 0: 0x7f1b97ada828: {[N Value[1 x 1]] }
MPI Rank 0: 0x7f1b97adb1a8: {[G Value[1 x 1]] }
MPI Rank 0: 0x7f1b97adbc88: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 0x7f1b97c8f5d8: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x7f1b97c903f8: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x7f1baf8a3828: {[ce Value[1]] }
MPI Rank 0: 0x7f1bafadfbb8: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/13/2016 12:23:54: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:23:57: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 07/13/2016 12:24:08:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.34696808 * 10240; time = 11.0727s; samplesPerSecond = 924.8
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 07/13/2016 12:24:19:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.34277344 * 10240; time = 11.0060s; samplesPerSecond = 930.4
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.00 seconds
MPI Rank 0: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=27.6453s
MPI Rank 0: 07/13/2016 12:24:24: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226607
MPI Rank 0: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 0: 07/13/2016 12:24:26: SGD: Saving checkpoint model '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:24:28: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:24:28: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 07/13/2016 12:24:39:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.30270958 * 10240; time = 11.0180s; samplesPerSecond = 929.4
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 07/13/2016 12:24:50:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.09883766 * 10240; time = 10.9991s; samplesPerSecond = 931.0
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=27.5034s
MPI Rank 0: 07/13/2016 12:24:55: Final Results: Minibatch[1-26]: ce = 1.97005577 * 102399; perplexity = 7.17107641
MPI Rank 0: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005577 * 102399
MPI Rank 0: 07/13/2016 12:24:57: SGD: Saving checkpoint model '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:24:59: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:24:59: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 0: 07/13/2016 12:25:10:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.89778175 * 10240; time = 10.9985s; samplesPerSecond = 931.0
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 07/13/2016 12:25:21:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86335983 * 10240; time = 10.9744s; samplesPerSecond = 933.1
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 0: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4384s
MPI Rank 0: 07/13/2016 12:25:26: Final Results: Minibatch[1-26]: ce = 1.80751075 * 102399; perplexity = 6.09525590
MPI Rank 0: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751075 * 102399
MPI Rank 0: 07/13/2016 12:25:28: SGD: Saving checkpoint model '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net'
MPI Rank 0: 07/13/2016 12:25:30: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:30: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:30: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/13/2016 12:23:52: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 12:23:52: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:52: 		Built time: Jul 13 2016 11:58:00
MPI Rank 1: 07/13/2016 12:23:52: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 1: 07/13/2016 12:23:52: 		Build type: debug
MPI Rank 1: 07/13/2016 12:23:52: 		Build target: GPU
MPI Rank 1: 07/13/2016 12:23:52: 		With 1bit-SGD: no
MPI Rank 1: 07/13/2016 12:23:52: 		Math lib: mkl
MPI Rank 1: 07/13/2016 12:23:52: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 07/13/2016 12:23:52: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 07/13/2016 12:23:52: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 07/13/2016 12:23:52: 		Build Branch: HEAD
MPI Rank 1: 07/13/2016 12:23:52: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 1: 07/13/2016 12:23:52: 		Built by philly on 2bc22072e267
MPI Rank 1: 07/13/2016 12:23:52: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 07/13/2016 12:23:52: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 12:23:53: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 12:23:53: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:23:53: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:23:53: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:23:53: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:23:53: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: Running on localhost at 2016/07/13 12:23:53
MPI Rank 1: 07/13/2016 12:23:53: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 12:23:53: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 12:23:53: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/13/2016 12:23:53: Commands: train
MPI Rank 1: 07/13/2016 12:23:53: Precision = "float"
MPI Rank 1: 07/13/2016 12:23:53: Using 6 CPU threads.
MPI Rank 1: 07/13/2016 12:23:53: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: 07/13/2016 12:23:53: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/13/2016 12:23:53: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: ##############################################################################
MPI Rank 1: 07/13/2016 12:23:53: #                                                                            #
MPI Rank 1: 07/13/2016 12:23:53: # Action "train"                                                             #
MPI Rank 1: 07/13/2016 12:23:53: #                                                                            #
MPI Rank 1: 07/13/2016 12:23:53: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:53: Creating virgin network.
MPI Rank 1: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:54: Created model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:54: Training criterion node(s):
MPI Rank 1: 07/13/2016 12:23:54: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x7effffef0bc8: {[SIM Value[51 x *]] }
MPI Rank 1: 0x7effffef21f8: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 0x7effffef2928: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 0x7effffef2a28: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 0x7effffef2eb8: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 0x7effffef30b8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 0x7effffef3278: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 0x7effffef3438: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 0x7effffef3cf8: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x7effffef4268: {[ce Gradient[1]] }
MPI Rank 1: 0x7effffef4428: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x7effffef45e8: {[SIM Gradient[51 x *]] }
MPI Rank 1: 0x7effffef4968: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 0x7effffef4d48: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7effffef4de8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7f00026b7b68: {[Keyword Value[49292 x *]] }
MPI Rank 1: 0x7f00026b7f28: {[S Value[1 x 1]] }
MPI Rank 1: 0x7f00026b81b8: {[N Value[1 x 1]] }
MPI Rank 1: 0x7f00026bad58: {[G Value[1 x 1]] }
MPI Rank 1: 0x7f00026bb7e8: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 0x7f00026ce9a8: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x7f00026cf7c8: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x7f00026d05b8: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x7f00026d0e08: {[Query Value[49292 x *]] }
MPI Rank 1: 0x7f001a9f04c8: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x7f001a9f1a58: {[ce Value[1]] }
MPI Rank 1: 0x7f001a9f1c18: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/13/2016 12:23:54: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:23:57: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 1: 07/13/2016 12:24:08:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32159615 * 10240; time = 11.0850s; samplesPerSecond = 923.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 12:24:19:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.33525505 * 10240; time = 11.0059s; samplesPerSecond = 930.4
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=27.6453s
MPI Rank 1: 07/13/2016 12:24:24: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226607
MPI Rank 1: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:24:28: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:24:28: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 12:24:39:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32732925 * 10240; time = 11.0181s; samplesPerSecond = 929.4
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 12:24:50:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11035995 * 10240; time = 10.9991s; samplesPerSecond = 931.0
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=27.5034s
MPI Rank 1: 07/13/2016 12:24:55: Final Results: Minibatch[1-26]: ce = 1.97005577 * 102399; perplexity = 7.17107641
MPI Rank 1: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005577 * 102399
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:24:59: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:24:59: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 12:25:10:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.92909794 * 10240; time = 10.9985s; samplesPerSecond = 931.0
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 12:25:21:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86598759 * 10240; time = 10.9743s; samplesPerSecond = 933.1
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4384s
MPI Rank 1: 07/13/2016 12:25:26: Final Results: Minibatch[1-26]: ce = 1.80751075 * 102399; perplexity = 6.09525590
MPI Rank 1: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751075 * 102399
MPI Rank 1: 07/13/2016 12:25:30: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:30: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:30: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/13/2016 12:23:52: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 12:23:52: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:52: 		Built time: Jul 13 2016 11:58:00
MPI Rank 2: 07/13/2016 12:23:52: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 2: 07/13/2016 12:23:52: 		Build type: debug
MPI Rank 2: 07/13/2016 12:23:52: 		Build target: GPU
MPI Rank 2: 07/13/2016 12:23:52: 		With 1bit-SGD: no
MPI Rank 2: 07/13/2016 12:23:52: 		Math lib: mkl
MPI Rank 2: 07/13/2016 12:23:52: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 07/13/2016 12:23:52: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 07/13/2016 12:23:52: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 07/13/2016 12:23:52: 		Build Branch: HEAD
MPI Rank 2: 07/13/2016 12:23:52: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 2: 07/13/2016 12:23:52: 		Built by philly on 2bc22072e267
MPI Rank 2: 07/13/2016 12:23:52: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 07/13/2016 12:23:52: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 12:23:54: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 12:23:54: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:23:54: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:23:54: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:23:54: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:23:54: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: Running on localhost at 2016/07/13 12:23:54
MPI Rank 2: 07/13/2016 12:23:54: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 12:23:54: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 12:23:54: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/13/2016 12:23:54: Commands: train
MPI Rank 2: 07/13/2016 12:23:54: Precision = "float"
MPI Rank 2: 07/13/2016 12:23:54: Using 6 CPU threads.
MPI Rank 2: 07/13/2016 12:23:54: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: 07/13/2016 12:23:54: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/13/2016 12:23:54: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: ##############################################################################
MPI Rank 2: 07/13/2016 12:23:54: #                                                                            #
MPI Rank 2: 07/13/2016 12:23:54: # Action "train"                                                             #
MPI Rank 2: 07/13/2016 12:23:54: #                                                                            #
MPI Rank 2: 07/13/2016 12:23:54: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:54: Creating virgin network.
MPI Rank 2: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:55: Created model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:55: Training criterion node(s):
MPI Rank 2: 07/13/2016 12:23:55: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x1d2e908: {[ce Value[1]] }
MPI Rank 2: 0x1dc1538: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x293c608: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x2d932e8: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x2d93cc8: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x2d94858: {[Query Value[49292 x *]] }
MPI Rank 2: 0x2d966f8: {[Keyword Value[49292 x *]] }
MPI Rank 2: 0x2d981d8: {[S Value[1 x 1]] }
MPI Rank 2: 0x2d98f98: {[N Value[1 x 1]] }
MPI Rank 2: 0x2d998c8: {[G Value[1 x 1]] }
MPI Rank 2: 0x2d9a338: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 0x2e91898: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 0x2e91a58: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 0x2e91c18: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 0x2e924d8: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x2e92a48: {[ce Gradient[1]] }
MPI Rank 2: 0x2e92c08: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x2e92dc8: {[SIM Gradient[51 x *]] }
MPI Rank 2: 0x2e93148: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 0x2e93498: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x2e93538: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 0x2e9e018: {[SIM Value[51 x *]] }
MPI Rank 2: 0x2e9e258: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 0x2e9f768: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 0x2e9f9e8: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 0x2e9ff58: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 0x2ea0368: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/13/2016 12:23:55: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:23:57: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.03 seconds
MPI Rank 2: 07/13/2016 12:24:08:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32837563 * 10240; time = 11.0919s; samplesPerSecond = 923.2
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 2: 07/13/2016 12:24:19:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35655479 * 10240; time = 11.0060s; samplesPerSecond = 930.4
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 2: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=27.6453s
MPI Rank 2: 07/13/2016 12:24:24: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226607
MPI Rank 2: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:24:28: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:24:28: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.02 seconds
MPI Rank 2: 07/13/2016 12:24:39:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32893600 * 10240; time = 11.0179s; samplesPerSecond = 929.4
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 07/13/2016 12:24:50:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11646919 * 10240; time = 10.9991s; samplesPerSecond = 931.0
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=27.5034s
MPI Rank 2: 07/13/2016 12:24:55: Final Results: Minibatch[1-26]: ce = 1.97005577 * 102399; perplexity = 7.17107641
MPI Rank 2: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005577 * 102399
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:24:59: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:24:59: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 07/13/2016 12:25:10:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95308418 * 10240; time = 10.9985s; samplesPerSecond = 931.0
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 07/13/2016 12:25:21:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87902641 * 10240; time = 10.9743s; samplesPerSecond = 933.1
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4384s
MPI Rank 2: 07/13/2016 12:25:26: Final Results: Minibatch[1-26]: ce = 1.80751075 * 102399; perplexity = 6.09525590
MPI Rank 2: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751075 * 102399
MPI Rank 2: 07/13/2016 12:25:30: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:30: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:30: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/13/2016 12:23:53: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 12:23:53: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:53: 		Built time: Jul 13 2016 11:58:00
MPI Rank 3: 07/13/2016 12:23:53: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 3: 07/13/2016 12:23:53: 		Build type: debug
MPI Rank 3: 07/13/2016 12:23:53: 		Build target: GPU
MPI Rank 3: 07/13/2016 12:23:53: 		With 1bit-SGD: no
MPI Rank 3: 07/13/2016 12:23:53: 		Math lib: mkl
MPI Rank 3: 07/13/2016 12:23:53: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 07/13/2016 12:23:53: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 07/13/2016 12:23:53: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 07/13/2016 12:23:53: 		Build Branch: HEAD
MPI Rank 3: 07/13/2016 12:23:53: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 3: 07/13/2016 12:23:53: 		Built by philly on 2bc22072e267
MPI Rank 3: 07/13/2016 12:23:53: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 07/13/2016 12:23:53: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 12:23:54: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 12:23:54: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:23:54: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:23:54: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:23:54: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:23:54: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: Running on localhost at 2016/07/13 12:23:54
MPI Rank 3: 07/13/2016 12:23:54: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 12:23:54: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 12:23:54: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/13/2016 12:23:54: Commands: train
MPI Rank 3: 07/13/2016 12:23:54: Precision = "float"
MPI Rank 3: 07/13/2016 12:23:54: Using 6 CPU threads.
MPI Rank 3: 07/13/2016 12:23:54: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: 07/13/2016 12:23:54: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/13/2016 12:23:54: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: ##############################################################################
MPI Rank 3: 07/13/2016 12:23:54: #                                                                            #
MPI Rank 3: 07/13/2016 12:23:54: # Action "train"                                                             #
MPI Rank 3: 07/13/2016 12:23:54: #                                                                            #
MPI Rank 3: 07/13/2016 12:23:54: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:54: Creating virgin network.
MPI Rank 3: SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:55: Created model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:55: Training criterion node(s):
MPI Rank 3: 07/13/2016 12:23:55: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x7f5047ec6698: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 0x7f5047ec69d8: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 0x7f5047ec7178: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 0x7f5047ec72d8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 0x7f5047ec7498: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 0x7f5047ec7658: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 0x7f5047ec7818: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 0x7f5047ec79d8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 0x7f5047ec8298: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x7f5047ec87d8: {[ce Gradient[1]] }
MPI Rank 3: 0x7f5047ec8998: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x7f5047ec8b58: {[SIM Gradient[51 x *]] }
MPI Rank 3: 0x7f5047ec8ed8: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 0x7f5047ec9288: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x7f5047ec9328: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 0x7f504840c308: {[SIM Value[51 x *]] }
MPI Rank 3: 0x7f50487a7cb8: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x7f50487a8b48: {[WD1 Value[64 x 288]] }
MPI Rank 3: 0x7f50487a9368: {[Query Value[49292 x *]] }
MPI Rank 3: 0x7f50487ab1c8: {[Keyword Value[49292 x *]] }
MPI Rank 3: 0x7f50487ab528: {[S Value[1 x 1]] }
MPI Rank 3: 0x7f50487abf78: {[G Value[1 x 1]] }
MPI Rank 3: 0x7f50487adbf8: {[N Value[1 x 1]] }
MPI Rank 3: 0x7f50487aee68: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 0x7f50487c4338: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x7f505fc65458: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x7f505fc66b38: {[ce Value[1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/13/2016 12:23:55: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:56: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:23:57: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.17 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.05 seconds
MPI Rank 3: 07/13/2016 12:24:08:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32287750 * 10240; time = 11.1007s; samplesPerSecond = 922.5
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/13/2016 12:24:19:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35470428 * 10240; time = 11.0059s; samplesPerSecond = 930.4
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=27.6453s
MPI Rank 3: 07/13/2016 12:24:24: Final Results: Minibatch[1-26]: ce = 2.49916009 * 102399; perplexity = 12.17226607
MPI Rank 3: 07/13/2016 12:24:24: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916009 * 102399
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:24:28: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:24:28: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.13 seconds
MPI Rank 3: 07/13/2016 12:24:39:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29653873 * 10240; time = 11.0181s; samplesPerSecond = 929.4
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.85 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.17 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.33 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.50 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.66 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.82 seconds , average latency = 0.14 seconds
MPI Rank 3: 07/13/2016 12:24:50:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11679478 * 10240; time = 10.9991s; samplesPerSecond = 931.0
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.98 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.14 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.30 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.46 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.62 seconds , average latency = 0.14 seconds
MPI Rank 3: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=27.5034s
MPI Rank 3: 07/13/2016 12:24:55: Final Results: Minibatch[1-26]: ce = 1.97005577 * 102399; perplexity = 7.17107641
MPI Rank 3: 07/13/2016 12:24:55: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005577 * 102399
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:24:59: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:24:59: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 0.14 seconds
MPI Rank 3: 07/13/2016 12:25:10:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90347176 * 10240; time = 10.9986s; samplesPerSecond = 931.0
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.07 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.23 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.39 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.55 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.71 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.87 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.03 seconds , average latency = 0.15 seconds
MPI Rank 3: 07/13/2016 12:25:21:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88304138 * 10240; time = 10.9743s; samplesPerSecond = 933.1
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.19 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.35 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.51 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.67 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 3.83 seconds , average latency = 0.15 seconds
MPI Rank 3: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4384s
MPI Rank 3: 07/13/2016 12:25:26: Final Results: Minibatch[1-26]: ce = 1.80751075 * 102399; perplexity = 6.09525590
MPI Rank 3: 07/13/2016 12:25:26: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751075 * 102399
MPI Rank 3: 07/13/2016 12:25:30: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:30: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:30: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu DeviceId=0 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 11:58:00
		Last modified date: Tue Jul 12 04:28:35 2016
		Build type: debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by philly on 2bc22072e267
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
07/13/2016 12:25:34: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank0
07/13/2016 12:25:34: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank1
07/13/2016 12:25:35: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank2
07/13/2016 12:25:35: Redirecting stderr to file /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr_train.logrank3
MPI Rank 0: 07/13/2016 12:25:34: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 12:25:34: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:34: 		Built time: Jul 13 2016 11:58:00
MPI Rank 0: 07/13/2016 12:25:34: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 0: 07/13/2016 12:25:34: 		Build type: debug
MPI Rank 0: 07/13/2016 12:25:34: 		Build target: GPU
MPI Rank 0: 07/13/2016 12:25:34: 		With 1bit-SGD: no
MPI Rank 0: 07/13/2016 12:25:34: 		Math lib: mkl
MPI Rank 0: 07/13/2016 12:25:34: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 07/13/2016 12:25:34: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 07/13/2016 12:25:34: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 07/13/2016 12:25:34: 		Build Branch: HEAD
MPI Rank 0: 07/13/2016 12:25:34: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 0: 07/13/2016 12:25:34: 		Built by philly on 2bc22072e267
MPI Rank 0: 07/13/2016 12:25:34: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 07/13/2016 12:25:34: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 12:25:36: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 12:25:36: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:25:36: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:25:36: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:25:36: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 07/13/2016 12:25:36: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: Running on localhost at 2016/07/13 12:25:36
MPI Rank 0: 07/13/2016 12:25:36: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 12:25:36: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 12:25:36: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/13/2016 12:25:36: Commands: train
MPI Rank 0: 07/13/2016 12:25:36: Precision = "float"
MPI Rank 0: 07/13/2016 12:25:36: Using 6 CPU threads.
MPI Rank 0: 07/13/2016 12:25:36: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: 07/13/2016 12:25:36: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/13/2016 12:25:36: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: ##############################################################################
MPI Rank 0: 07/13/2016 12:25:36: #                                                                            #
MPI Rank 0: 07/13/2016 12:25:36: # Action "train"                                                             #
MPI Rank 0: 07/13/2016 12:25:36: #                                                                            #
MPI Rank 0: 07/13/2016 12:25:36: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:36: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:38: Loaded model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:38: Training criterion node(s):
MPI Rank 0: 07/13/2016 12:25:38: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x7fd1ebe12f58: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0x7fd1ebe14378: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x7fd1ebe574b8: {[G Value[1 x 1]] }
MPI Rank 0: 0x7fd1ebe57678: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 0x7fd1ebe58278: {[Query Value[49292 x *1]] }
MPI Rank 0: 0x7fd1ebe58d28: {[S Value[1 x 1]] }
MPI Rank 0: 0x7fd1ebe58dd8: {[N Value[1 x 1]] }
MPI Rank 0: 0x7fd1ebe58ed8: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x7fd1ec4e6148: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x7fd1ec4e76a8: {[SIM Value[51 x *1]] }
MPI Rank 0: 0x7fd1ec4e78d8: {[ce Value[1]] }
MPI Rank 0: 0x7fd1ec4e91e8: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 0x7fd1ec4e9528: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 0x7fd1ec4e99d8: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 0x7fd1ec4e9b98: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 0x7fd1ec4e9d58: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 0x7fd1ec4e9f18: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 0x7fd1ec4ea0d8: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 0x7fd1ec4ea298: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 0x7fd1ec4eab58: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x7fd1ec4eb098: {[ce Gradient[1]] }
MPI Rank 0: 0x7fd1ec4eb258: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x7fd1ec4eb418: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 0x7fd1ec4eb798: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 0x7fd1ec4ebb78: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x7fd1ec4ebc18: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 0x7fd203fdea48: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/13/2016 12:25:38: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:41: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:25:41: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 07/13/2016 12:25:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87577038 * 10240; time = 11.0232s; samplesPerSecond = 929.0
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 07/13/2016 12:26:03:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79361134 * 10240; time = 10.9680s; samplesPerSecond = 933.6
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 0: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4964s
MPI Rank 0: 07/13/2016 12:26:08: Final Results: Minibatch[1-26]: ce = 1.81846899 * 102399; perplexity = 6.16241650
MPI Rank 0: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846899 * 102399
MPI Rank 0: 07/13/2016 12:26:10: SGD: Saving checkpoint model '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net'
MPI Rank 0: 07/13/2016 12:26:12: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:26:12: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 12:26:12: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/13/2016 12:25:34: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 12:25:34: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:34: 		Built time: Jul 13 2016 11:58:00
MPI Rank 1: 07/13/2016 12:25:34: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 1: 07/13/2016 12:25:34: 		Build type: debug
MPI Rank 1: 07/13/2016 12:25:34: 		Build target: GPU
MPI Rank 1: 07/13/2016 12:25:34: 		With 1bit-SGD: no
MPI Rank 1: 07/13/2016 12:25:34: 		Math lib: mkl
MPI Rank 1: 07/13/2016 12:25:34: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 07/13/2016 12:25:34: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 07/13/2016 12:25:34: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 07/13/2016 12:25:34: 		Build Branch: HEAD
MPI Rank 1: 07/13/2016 12:25:34: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 1: 07/13/2016 12:25:34: 		Built by philly on 2bc22072e267
MPI Rank 1: 07/13/2016 12:25:34: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 07/13/2016 12:25:34: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 12:25:36: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 12:25:36: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:25:36: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:25:36: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:25:36: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 07/13/2016 12:25:36: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: Running on localhost at 2016/07/13 12:25:36
MPI Rank 1: 07/13/2016 12:25:36: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 12:25:36: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 12:25:36: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/13/2016 12:25:36: Commands: train
MPI Rank 1: 07/13/2016 12:25:36: Precision = "float"
MPI Rank 1: 07/13/2016 12:25:36: Using 6 CPU threads.
MPI Rank 1: 07/13/2016 12:25:36: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: 07/13/2016 12:25:36: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/13/2016 12:25:36: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: ##############################################################################
MPI Rank 1: 07/13/2016 12:25:36: #                                                                            #
MPI Rank 1: 07/13/2016 12:25:36: # Action "train"                                                             #
MPI Rank 1: 07/13/2016 12:25:36: #                                                                            #
MPI Rank 1: 07/13/2016 12:25:36: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:36: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:38: Loaded model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:38: Training criterion node(s):
MPI Rank 1: 07/13/2016 12:25:38: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x7f1b23c58308: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 0x7f1b23c58b38: {[N Value[1 x 1]] }
MPI Rank 1: 0x7f1b23c59de8: {[Query Value[49292 x *1]] }
MPI Rank 1: 0x7f1b23ca2258: {[G Value[1 x 1]] }
MPI Rank 1: 0x7f1b23ca2e28: {[S Value[1 x 1]] }
MPI Rank 1: 0x7f1b23ca3d78: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x7f1b23ca5ae8: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x7f1b23ca65e8: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x7f1b256ae378: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x7f1b256b0418: {[SIM Value[51 x *1]] }
MPI Rank 1: 0x7f1b256b0f98: {[ce Value[1]] }
MPI Rank 1: 0x7f1b256b2a08: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 0x7f1b256b2d48: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 0x7f1b256b3138: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 0x7f1b256b32f8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 0x7f1b256b34b8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 0x7f1b256b3678: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 0x7f1b256b3838: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 0x7f1b256b39f8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 0x7f1b256b42b8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x7f1b256b4828: {[ce Gradient[1]] }
MPI Rank 1: 0x7f1b256b49e8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x7f1b256b4ba8: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 0x7f1b256b4f28: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 0x7f1b256b52b8: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7f1b256b5358: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7f1b41202c48: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/13/2016 12:25:38: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:41: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:25:41: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.20-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.20 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 07/13/2016 12:25:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93745022 * 10240; time = 11.0448s; samplesPerSecond = 927.1
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 12:26:03:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89571209 * 10240; time = 10.9680s; samplesPerSecond = 933.6
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4964s
MPI Rank 1: 07/13/2016 12:26:08: Final Results: Minibatch[1-26]: ce = 1.81846899 * 102399; perplexity = 6.16241650
MPI Rank 1: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846899 * 102399
MPI Rank 1: 07/13/2016 12:26:12: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:26:12: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 12:26:12: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/13/2016 12:25:35: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 12:25:35: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:35: 		Built time: Jul 13 2016 11:58:00
MPI Rank 2: 07/13/2016 12:25:35: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 2: 07/13/2016 12:25:35: 		Build type: debug
MPI Rank 2: 07/13/2016 12:25:35: 		Build target: GPU
MPI Rank 2: 07/13/2016 12:25:35: 		With 1bit-SGD: no
MPI Rank 2: 07/13/2016 12:25:35: 		Math lib: mkl
MPI Rank 2: 07/13/2016 12:25:35: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 07/13/2016 12:25:35: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 07/13/2016 12:25:35: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 07/13/2016 12:25:35: 		Build Branch: HEAD
MPI Rank 2: 07/13/2016 12:25:35: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 2: 07/13/2016 12:25:35: 		Built by philly on 2bc22072e267
MPI Rank 2: 07/13/2016 12:25:35: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 07/13/2016 12:25:35: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 12:25:37: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 12:25:37: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:25:37: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:25:37: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:25:37: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 07/13/2016 12:25:37: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: Running on localhost at 2016/07/13 12:25:37
MPI Rank 2: 07/13/2016 12:25:37: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 12:25:37: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 12:25:37: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/13/2016 12:25:37: Commands: train
MPI Rank 2: 07/13/2016 12:25:37: Precision = "float"
MPI Rank 2: 07/13/2016 12:25:37: Using 6 CPU threads.
MPI Rank 2: 07/13/2016 12:25:37: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: 07/13/2016 12:25:37: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/13/2016 12:25:37: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: ##############################################################################
MPI Rank 2: 07/13/2016 12:25:37: #                                                                            #
MPI Rank 2: 07/13/2016 12:25:37: # Action "train"                                                             #
MPI Rank 2: 07/13/2016 12:25:37: #                                                                            #
MPI Rank 2: 07/13/2016 12:25:37: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:37: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:39: Loaded model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:39: Training criterion node(s):
MPI Rank 2: 07/13/2016 12:25:39: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x7f712278e028: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x7f712278f4b8: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x7f7122790478: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x7f71227918d8: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x7f7122794968: {[SIM Value[51 x *1]] }
MPI Rank 2: 0x7f7122794b98: {[ce Value[1]] }
MPI Rank 2: 0x7f7122796418: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 0x7f7122796758: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 0x7f7122796c68: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 0x7f7122796e28: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 0x7f7122796fe8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 0x7f71227971a8: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 0x7f7122797368: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 0x7f7122797528: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 0x7f7122797de8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x7f7122798328: {[ce Gradient[1]] }
MPI Rank 2: 0x7f71227984e8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x7f71227986a8: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 0x7f7122798a28: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 0x7f7122798e08: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x7f7122798ea8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 0x7f71227ab078: {[G Value[1 x 1]] }
MPI Rank 2: 0x7f71227ab278: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 0x7f71227abc28: {[S Value[1 x 1]] }
MPI Rank 2: 0x7f71227abea8: {[Query Value[49292 x *1]] }
MPI Rank 2: 0x7f71227acdb8: {[N Value[1 x 1]] }
MPI Rank 2: 0x7f713aa3e7e8: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/13/2016 12:25:39: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:41: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:25:41: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.14 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 07/13/2016 12:25:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96188030 * 10240; time = 11.0442s; samplesPerSecond = 927.2
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 07/13/2016 12:26:03:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90950069 * 10240; time = 10.9680s; samplesPerSecond = 933.6
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4964s
MPI Rank 2: 07/13/2016 12:26:08: Final Results: Minibatch[1-26]: ce = 1.81846899 * 102399; perplexity = 6.16241650
MPI Rank 2: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846899 * 102399
MPI Rank 2: 07/13/2016 12:26:12: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:26:12: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 12:26:12: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/13/2016 12:25:35: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 12:25:35: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:35: 		Built time: Jul 13 2016 11:58:00
MPI Rank 3: 07/13/2016 12:25:35: 		Last modified date: Tue Jul 12 04:28:35 2016
MPI Rank 3: 07/13/2016 12:25:35: 		Build type: debug
MPI Rank 3: 07/13/2016 12:25:35: 		Build target: GPU
MPI Rank 3: 07/13/2016 12:25:35: 		With 1bit-SGD: no
MPI Rank 3: 07/13/2016 12:25:35: 		Math lib: mkl
MPI Rank 3: 07/13/2016 12:25:35: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 07/13/2016 12:25:35: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 07/13/2016 12:25:35: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 07/13/2016 12:25:35: 		Build Branch: HEAD
MPI Rank 3: 07/13/2016 12:25:35: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 3: 07/13/2016 12:25:35: 		Built by philly on 2bc22072e267
MPI Rank 3: 07/13/2016 12:25:35: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 07/13/2016 12:25:35: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 12:25:37: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 12:25:37: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:25:37: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:25:37: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:25:37: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 07/13/2016 12:25:37: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: Running on localhost at 2016/07/13 12:25:37
MPI Rank 3: 07/13/2016 12:25:37: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/debug/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 12:25:37: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 12:25:37: modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/13/2016 12:25:37: Commands: train
MPI Rank 3: 07/13/2016 12:25:37: Precision = "float"
MPI Rank 3: 07/13/2016 12:25:37: Using 6 CPU threads.
MPI Rank 3: 07/13/2016 12:25:37: CNTKModelPath: /tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: 07/13/2016 12:25:37: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/13/2016 12:25:37: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: ##############################################################################
MPI Rank 3: 07/13/2016 12:25:37: #                                                                            #
MPI Rank 3: 07/13/2016 12:25:37: # Action "train"                                                             #
MPI Rank 3: 07/13/2016 12:25:37: #                                                                            #
MPI Rank 3: 07/13/2016 12:25:37: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:37: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160713121920.930131/Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:38: Loaded model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:38: Training criterion node(s):
MPI Rank 3: 07/13/2016 12:25:38: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x7f024d82d4d8: {[N Value[1 x 1]] }
MPI Rank 3: 0x7f024d82f638: {[Query Value[49292 x *1]] }
MPI Rank 3: 0x7f024d82fd98: {[S Value[1 x 1]] }
MPI Rank 3: 0x7f024d830ce8: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x7f024d832a28: {[WD1 Value[64 x 288]] }
MPI Rank 3: 0x7f024d833a28: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x7f024d834e68: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x7f024d837ec8: {[SIM Value[51 x *1]] }
MPI Rank 3: 0x7f024d8380f8: {[ce Value[1]] }
MPI Rank 3: 0x7f024d839978: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 0x7f024d839cb8: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 0x7f024d83a1c8: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 0x7f024d83a388: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 0x7f024d83a548: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 0x7f024d83a708: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 0x7f024d83a8c8: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 0x7f024d83aa88: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 0x7f024d83b348: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x7f024d83b888: {[ce Gradient[1]] }
MPI Rank 3: 0x7f024d83ba48: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x7f024d83bc08: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 0x7f024d83bf88: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 0x7f024d83c368: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x7f024d83c408: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 0x7f024d849b58: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 0x7f024d849e88: {[G Value[1 x 1]] }
MPI Rank 3: 0x7f02650f6c08: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/13/2016 12:25:38: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:41: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:25:41: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.40-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.40 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.20 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/13/2016 12:25:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91174297 * 10240; time = 11.0599s; samplesPerSecond = 925.9
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/13/2016 12:26:03:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91370487 * 10240; time = 10.9680s; samplesPerSecond = 933.6
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974292 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=27.4964s
MPI Rank 3: 07/13/2016 12:26:08: Final Results: Minibatch[1-26]: ce = 1.81846899 * 102399; perplexity = 6.16241650
MPI Rank 3: 07/13/2016 12:26:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846899 * 102399
MPI Rank 3: 07/13/2016 12:26:12: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:26:12: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 12:26:12: __COMPLETED__
MPI Rank 3: ~MPIWrapper