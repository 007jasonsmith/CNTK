CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3565 @ 3.20GHz
    Hardware threads: 8
    Total Memory: 12580436 kB
-------------------------------------------------------------------
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu DeviceId=0 timestamping=true numCPUThreads=2 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/14/2016 06:31:39: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank0
MPI Rank 0: 07/14/2016 06:31:39: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 06:31:39: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:39: 		Built time: Jul 14 2016 05:11:35
MPI Rank 0: 07/14/2016 06:31:39: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 0: 07/14/2016 06:31:39: 		Build type: Debug
MPI Rank 0: 07/14/2016 06:31:39: 		Build target: GPU
MPI Rank 0: 07/14/2016 06:31:39: 		With 1bit-SGD: no
MPI Rank 0: 07/14/2016 06:31:39: 		Math lib: mkl
MPI Rank 0: 07/14/2016 06:31:39: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/14/2016 06:31:39: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/14/2016 06:31:39: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/14/2016 06:31:39: 		Build Branch: HEAD
MPI Rank 0: 07/14/2016 06:31:39: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 0: 07/14/2016 06:31:39: 		Built by svcphil on liana-08-w
MPI Rank 0: 07/14/2016 06:31:39: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/14/2016 06:31:39: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 06:31:40: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: Running on cntk-muc01 at 2016/07/14 06:31:40
MPI Rank 0: 07/14/2016 06:31:40: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 06:31:40: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=2
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 06:31:40: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=2
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/14/2016 06:31:40: Commands: train
MPI Rank 0: 07/14/2016 06:31:40: Precision = "float"
MPI Rank 0: 07/14/2016 06:31:40: Using 2 CPU threads.
MPI Rank 0: 07/14/2016 06:31:40: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: 07/14/2016 06:31:40: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/14/2016 06:31:40: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: ##############################################################################
MPI Rank 0: 07/14/2016 06:31:40: #                                                                            #
MPI Rank 0: 07/14/2016 06:31:40: # Action "train"                                                             #
MPI Rank 0: 07/14/2016 06:31:40: #                                                                            #
MPI Rank 0: 07/14/2016 06:31:40: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: Creating virgin network.
MPI Rank 0: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: Created model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:40: Training criterion node(s):
MPI Rank 0: 07/14/2016 06:31:40: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 0000009FF0141260: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0000009FFEDDCA50: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0000009FFEDDCB20: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0000009FFEDF8C70: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0000009FFEDF8D40: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 0000009FFEDF8E10: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 0000009FFEDF9080: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 0000009FFEDF9150: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 0000009FFEDF92F0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 0000009FFEDF9490: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 0000009FFEDF9560: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 0000009FFEDF9630: {[Query Value[49292 x *]] }
MPI Rank 0: 0000009FFEDF9700: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 0000009FFEDF97D0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 0000009FFEDF9970: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 0000009FFEDF9A40: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 0000009FFEDF9B10: {[Keyword Value[49292 x *]] }
MPI Rank 0: 0000009FFEDF9D80: {[N Value[1 x 1]] }
MPI Rank 0: 0000009FFEDF9F20: {[ce Gradient[1]] }
MPI Rank 0: 0000009FFEDFA0C0: {[S Value[1 x 1]] }
MPI Rank 0: 0000009FFEDFA190: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0000009FFEDFA260: {[G Value[1 x 1]] }
MPI Rank 0: 0000009FFEDFA330: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 0000009FFEDFA400: {[SIM Gradient[51 x *]] }
MPI Rank 0: 0000009FFEDFA5A0: {[ce Value[1]] }
MPI Rank 0: 0000009FFEDFA8E0: {[SIM Value[51 x *]] }
MPI Rank 0: 0000009FFEDFAB50: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/14/2016 06:31:40: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:46: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:31:47: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.18 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.14 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.90 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.90 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/14/2016 06:31:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.34696770 * 10240; time = 9.4259s; samplesPerSecond = 1086.4
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.06 seconds
MPI Rank 0: 07/14/2016 06:32:06:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.34277382 * 10240; time = 8.9976s; samplesPerSecond = 1138.1
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.06 seconds
MPI Rank 0: 07/14/2016 06:32:10: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.3074s
MPI Rank 0: 07/14/2016 06:32:14: Final Results: Minibatch[1-26]: ce = 2.49916006 * 102399; perplexity = 12.17226569
MPI Rank 0: 07/14/2016 06:32:14: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916006 * 102399
MPI Rank 0: 07/14/2016 06:32:18: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:32:21: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:32:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 1.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 1.10 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.59 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.42 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 0.33 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.28 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.23 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.44 seconds , average latency = 0.21 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 0.18 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 0.17 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 0.15 seconds
MPI Rank 0: 07/14/2016 06:32:32:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.30270958 * 10240; time = 10.3140s; samplesPerSecond = 992.8
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 0.14 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.13 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.13 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/14/2016 06:32:41:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.09883766 * 10240; time = 9.0436s; samplesPerSecond = 1132.3
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/14/2016 06:32:45: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=23.8229s
MPI Rank 0: 07/14/2016 06:32:47: Final Results: Minibatch[1-26]: ce = 1.97005575 * 102399; perplexity = 7.17107623
MPI Rank 0: 07/14/2016 06:32:47: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005575 * 102399
MPI Rank 0: 07/14/2016 06:32:51: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:32:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:32:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.05 seconds
MPI Rank 0: 07/14/2016 06:33:03:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.89778175 * 10240; time = 8.9674s; samplesPerSecond = 1141.9
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 0: 07/14/2016 06:33:12:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86335983 * 10240; time = 8.9169s; samplesPerSecond = 1148.4
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.03 seconds
MPI Rank 0: 07/14/2016 06:33:17: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3287s
MPI Rank 0: 07/14/2016 06:33:19: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 0: 07/14/2016 06:33:19: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 0: 07/14/2016 06:33:23: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net'
MPI Rank 0: 07/14/2016 06:33:26: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:26: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:26: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/14/2016 06:31:40: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank1
MPI Rank 1: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 06:31:40: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: 		Built time: Jul 14 2016 05:11:35
MPI Rank 1: 07/14/2016 06:31:40: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 1: 07/14/2016 06:31:40: 		Build type: Debug
MPI Rank 1: 07/14/2016 06:31:40: 		Build target: GPU
MPI Rank 1: 07/14/2016 06:31:40: 		With 1bit-SGD: no
MPI Rank 1: 07/14/2016 06:31:40: 		Math lib: mkl
MPI Rank 1: 07/14/2016 06:31:40: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/14/2016 06:31:40: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/14/2016 06:31:40: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/14/2016 06:31:40: 		Build Branch: HEAD
MPI Rank 1: 07/14/2016 06:31:40: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 1: 07/14/2016 06:31:40: 		Built by svcphil on liana-08-w
MPI Rank 1: 07/14/2016 06:31:40: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 06:31:40: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: Running on cntk-muc01 at 2016/07/14 06:31:40
MPI Rank 1: 07/14/2016 06:31:40: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 06:31:40: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=2
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 06:31:40: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=2
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/14/2016 06:31:40: Commands: train
MPI Rank 1: 07/14/2016 06:31:40: Precision = "float"
MPI Rank 1: 07/14/2016 06:31:40: Using 2 CPU threads.
MPI Rank 1: 07/14/2016 06:31:40: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: 07/14/2016 06:31:40: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/14/2016 06:31:40: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: ##############################################################################
MPI Rank 1: 07/14/2016 06:31:40: #                                                                            #
MPI Rank 1: 07/14/2016 06:31:40: # Action "train"                                                             #
MPI Rank 1: 07/14/2016 06:31:40: #                                                                            #
MPI Rank 1: 07/14/2016 06:31:40: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:40: Creating virgin network.
MPI Rank 1: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:41: Created model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:41: Training criterion node(s):
MPI Rank 1: 07/14/2016 06:31:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000320DDDF690: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000321ADC1740: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 000000321ADC1810: {[G Value[1 x 1]] }
MPI Rank 1: 000000321ADC18E0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 000000321ADC19B0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 000000321ADC1A80: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 000000321ADC1B50: {[SIM Gradient[51 x *]] }
MPI Rank 1: 000000321ADC1DC0: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 000000321ADC1F60: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 000000321ADC2100: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000321ADC21D0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000321ADC2370: {[S Value[1 x 1]] }
MPI Rank 1: 000000321ADC2440: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 000000321ADC25E0: {[SIM Value[51 x *]] }
MPI Rank 1: 000000321ADC26B0: {[Query Value[49292 x *]] }
MPI Rank 1: 000000321ADC2780: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 000000321ADC2920: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000321ADC2C60: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 000000321ADC2D30: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 000000321ADC2ED0: {[ce Value[1]] }
MPI Rank 1: 000000321ADC3070: {[N Value[1 x 1]] }
MPI Rank 1: 000000321ADC3140: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 000000321ADC3210: {[Keyword Value[49292 x *]] }
MPI Rank 1: 000000321ADC3480: {[ce Gradient[1]] }
MPI Rank 1: 000000321ADC3620: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 000000321CF46990: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 000000321CF47830: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/14/2016 06:31:41: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:46: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:31:47: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.05 seconds
MPI Rank 1: 07/14/2016 06:31:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32159615 * 10240; time = 9.4043s; samplesPerSecond = 1088.9
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 1: 07/14/2016 06:32:06:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.33525505 * 10240; time = 8.9976s; samplesPerSecond = 1138.1
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.04 seconds
MPI Rank 1: 07/14/2016 06:32:10: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.3073s
MPI Rank 1: 07/14/2016 06:32:14: Final Results: Minibatch[1-26]: ce = 2.49916006 * 102399; perplexity = 12.17226569
MPI Rank 1: 07/14/2016 06:32:14: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916006 * 102399
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:32:21: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:32:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.73 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.48 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.39 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 0.32 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.28 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.24 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.21 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 0.19 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 0.17 seconds
MPI Rank 1: 07/14/2016 06:32:32:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32732925 * 10240; time = 10.3141s; samplesPerSecond = 992.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 0.16 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 0.15 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 0.13 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.13 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.85 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 0.10 seconds
MPI Rank 1: 07/14/2016 06:32:41:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11035995 * 10240; time = 9.0436s; samplesPerSecond = 1132.3
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 2.15 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 2.19 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 2.23 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.23 seconds , average latency = 0.09 seconds
MPI Rank 1: 07/14/2016 06:32:45: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=23.823s
MPI Rank 1: 07/14/2016 06:32:47: Final Results: Minibatch[1-26]: ce = 1.97005575 * 102399; perplexity = 7.17107623
MPI Rank 1: 07/14/2016 06:32:47: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005575 * 102399
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:32:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:32:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 07/14/2016 06:33:03:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.92909813 * 10240; time = 8.9684s; samplesPerSecond = 1141.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.02 seconds
MPI Rank 1: 07/14/2016 06:33:12:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86598778 * 10240; time = 8.9166s; samplesPerSecond = 1148.4
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.02 seconds
MPI Rank 1: 07/14/2016 06:33:17: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3287s
MPI Rank 1: 07/14/2016 06:33:19: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 1: 07/14/2016 06:33:19: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 1: 07/14/2016 06:33:26: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:26: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:26: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/14/2016 06:31:40: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank2
MPI Rank 2: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 06:31:40: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:40: 		Built time: Jul 14 2016 05:11:35
MPI Rank 2: 07/14/2016 06:31:40: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 2: 07/14/2016 06:31:40: 		Build type: Debug
MPI Rank 2: 07/14/2016 06:31:40: 		Build target: GPU
MPI Rank 2: 07/14/2016 06:31:40: 		With 1bit-SGD: no
MPI Rank 2: 07/14/2016 06:31:40: 		Math lib: mkl
MPI Rank 2: 07/14/2016 06:31:40: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/14/2016 06:31:40: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/14/2016 06:31:40: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/14/2016 06:31:40: 		Build Branch: HEAD
MPI Rank 2: 07/14/2016 06:31:40: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 2: 07/14/2016 06:31:40: 		Built by svcphil on liana-08-w
MPI Rank 2: 07/14/2016 06:31:40: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/14/2016 06:31:40: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 06:31:41: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 06:31:41: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 07/14/2016 06:31:41: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: Running on cntk-muc01 at 2016/07/14 06:31:41
MPI Rank 2: 07/14/2016 06:31:41: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 06:31:41: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=2
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 06:31:41: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=2
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/14/2016 06:31:41: Commands: train
MPI Rank 2: 07/14/2016 06:31:41: Precision = "float"
MPI Rank 2: 07/14/2016 06:31:41: Using 2 CPU threads.
MPI Rank 2: 07/14/2016 06:31:41: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: 07/14/2016 06:31:41: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/14/2016 06:31:41: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: ##############################################################################
MPI Rank 2: 07/14/2016 06:31:41: #                                                                            #
MPI Rank 2: 07/14/2016 06:31:41: # Action "train"                                                             #
MPI Rank 2: 07/14/2016 06:31:41: #                                                                            #
MPI Rank 2: 07/14/2016 06:31:41: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: Creating virgin network.
MPI Rank 2: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: Created model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:41: Training criterion node(s):
MPI Rank 2: 07/14/2016 06:31:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000E71F974DA0: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 000000E71F974E70: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000E71F974F40: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 000000E71F975010: {[G Value[1 x 1]] }
MPI Rank 2: 000000E71F9750E0: {[ce Gradient[1]] }
MPI Rank 2: 000000E71F9751B0: {[SIM Gradient[51 x *]] }
MPI Rank 2: 000000E71F975350: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 000000E71F975900: {[Query Value[49292 x *]] }
MPI Rank 2: 000000E71F9759D0: {[S Value[1 x 1]] }
MPI Rank 2: 000000E71F975AA0: {[N Value[1 x 1]] }
MPI Rank 2: 000000E71F975C40: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 000000E71F975DE0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 000000E71F975EB0: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000E71F975F80: {[Keyword Value[49292 x *]] }
MPI Rank 2: 000000E71F976050: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 000000E71F976390: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 000000E71F976460: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 000000E71F976530: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 000000E71F976600: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 000000E71F9766D0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 000000E71F9767A0: {[ce Value[1]] }
MPI Rank 2: 000000E71F976940: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 000000E71F976A10: {[SIM Value[51 x *]] }
MPI Rank 2: 000000E71F976C80: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 000000E76B915600: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 000000E77AE520A0: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000E77AE52170: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/14/2016 06:31:41: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:46: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:31:47: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/14/2016 06:31:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32837563 * 10240; time = 9.4289s; samplesPerSecond = 1086.0
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/14/2016 06:32:06:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35655479 * 10240; time = 8.9975s; samplesPerSecond = 1138.1
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/14/2016 06:32:10: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.3073s
MPI Rank 2: 07/14/2016 06:32:14: Final Results: Minibatch[1-26]: ce = 2.49916006 * 102399; perplexity = 12.17226569
MPI Rank 2: 07/14/2016 06:32:14: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916006 * 102399
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:32:21: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:32:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.64 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.43 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.32 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.26 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.21 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 0.19 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 0.17 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 0.15 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.14 seconds
MPI Rank 2: 07/14/2016 06:32:32:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32893620 * 10240; time = 10.3137s; samplesPerSecond = 992.9
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 0.14 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 0.10 seconds
MPI Rank 2: 07/14/2016 06:32:41:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11646900 * 10240; time = 9.0438s; samplesPerSecond = 1132.3
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 2.08 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.08 seconds , average latency = 0.08 seconds
MPI Rank 2: 07/14/2016 06:32:45: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=23.8229s
MPI Rank 2: 07/14/2016 06:32:47: Final Results: Minibatch[1-26]: ce = 1.97005575 * 102399; perplexity = 7.17107623
MPI Rank 2: 07/14/2016 06:32:47: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005575 * 102399
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:32:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:32:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.05 seconds
MPI Rank 2: 07/14/2016 06:33:03:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95308418 * 10240; time = 8.9620s; samplesPerSecond = 1142.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.05 seconds
MPI Rank 2: 07/14/2016 06:33:12:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87902641 * 10240; time = 8.9167s; samplesPerSecond = 1148.4
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.04 seconds
MPI Rank 2: 07/14/2016 06:33:17: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3287s
MPI Rank 2: 07/14/2016 06:33:19: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 2: 07/14/2016 06:33:19: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 2: 07/14/2016 06:33:26: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:26: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:26: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/14/2016 06:31:41: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank3
MPI Rank 3: 07/14/2016 06:31:41: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 06:31:41: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: 		Built time: Jul 14 2016 05:11:35
MPI Rank 3: 07/14/2016 06:31:41: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 3: 07/14/2016 06:31:41: 		Build type: Debug
MPI Rank 3: 07/14/2016 06:31:41: 		Build target: GPU
MPI Rank 3: 07/14/2016 06:31:41: 		With 1bit-SGD: no
MPI Rank 3: 07/14/2016 06:31:41: 		Math lib: mkl
MPI Rank 3: 07/14/2016 06:31:41: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/14/2016 06:31:41: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/14/2016 06:31:41: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/14/2016 06:31:41: 		Build Branch: HEAD
MPI Rank 3: 07/14/2016 06:31:41: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 3: 07/14/2016 06:31:41: 		Built by svcphil on liana-08-w
MPI Rank 3: 07/14/2016 06:31:41: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/14/2016 06:31:41: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 06:31:41: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 06:31:41: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 07/14/2016 06:31:41: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: Running on cntk-muc01 at 2016/07/14 06:31:41
MPI Rank 3: 07/14/2016 06:31:41: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 06:31:41: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=2
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 06:31:41: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=2
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/14/2016 06:31:41: Commands: train
MPI Rank 3: 07/14/2016 06:31:41: Precision = "float"
MPI Rank 3: 07/14/2016 06:31:41: Using 2 CPU threads.
MPI Rank 3: 07/14/2016 06:31:41: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: 07/14/2016 06:31:41: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/14/2016 06:31:41: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: ##############################################################################
MPI Rank 3: 07/14/2016 06:31:41: #                                                                            #
MPI Rank 3: 07/14/2016 06:31:41: # Action "train"                                                             #
MPI Rank 3: 07/14/2016 06:31:41: #                                                                            #
MPI Rank 3: 07/14/2016 06:31:41: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:41: Creating virgin network.
MPI Rank 3: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:42: Created model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:42: Training criterion node(s):
MPI Rank 3: 07/14/2016 06:31:42: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000C98ED55600: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000C9A3EF8880: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000C9A3EF8950: {[Query Value[49292 x *]] }
MPI Rank 3: 000000C9A3EF8A20: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000C9A3EF8C90: {[ce Gradient[1]] }
MPI Rank 3: 000000C9A3EF8D60: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 000000C9A3EF8E30: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 000000C9A3EF90A0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000C9A3EF9170: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000C9A3EF9310: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000C9A3EF9650: {[SIM Value[51 x *]] }
MPI Rank 3: 000000C9A3EF9720: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 000000C9A3EF97F0: {[G Value[1 x 1]] }
MPI Rank 3: 000000C9A3EF98C0: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 000000C9A3EF9A60: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 000000C9A3EF9C00: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 000000C9A3EF9CD0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 000000C9A3EF9E70: {[N Value[1 x 1]] }
MPI Rank 3: 000000C9A3EFA010: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 000000C9A3EFA1B0: {[S Value[1 x 1]] }
MPI Rank 3: 000000C9A3EFA280: {[SIM Gradient[51 x *]] }
MPI Rank 3: 000000C9A3EFA4F0: {[Keyword Value[49292 x *]] }
MPI Rank 3: 000000C9A3EFA5C0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 000000C9A3EFA690: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 000000C9A3EFA760: {[ce Value[1]] }
MPI Rank 3: 000000C9A44FC190: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000C9A44FC260: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/14/2016 06:31:42: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:46: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:31:48: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.03 seconds
MPI Rank 3: 07/14/2016 06:31:57:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32287788 * 10240; time = 9.3812s; samplesPerSecond = 1091.5
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.03 seconds
MPI Rank 3: 07/14/2016 06:32:06:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35470390 * 10240; time = 8.9975s; samplesPerSecond = 1138.1
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/14/2016 06:32:10: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.3073s
MPI Rank 3: 07/14/2016 06:32:14: Final Results: Minibatch[1-26]: ce = 2.49916006 * 102399; perplexity = 12.17226569
MPI Rank 3: 07/14/2016 06:32:14: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916006 * 102399
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:32:21: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:32:21: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/14/2016 06:32:32:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29653873 * 10240; time = 10.3136s; samplesPerSecond = 992.9
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.03 seconds
MPI Rank 3: 07/14/2016 06:32:41:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11679478 * 10240; time = 9.0437s; samplesPerSecond = 1132.3
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.03 seconds
MPI Rank 3: 07/14/2016 06:32:45: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=23.8229s
MPI Rank 3: 07/14/2016 06:32:47: Final Results: Minibatch[1-26]: ce = 1.97005575 * 102399; perplexity = 7.17107623
MPI Rank 3: 07/14/2016 06:32:47: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005575 * 102399
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:32:54: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:32:54: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.05 seconds
MPI Rank 3: 07/14/2016 06:33:03:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90347157 * 10240; time = 8.9683s; samplesPerSecond = 1141.8
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.90 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/14/2016 06:33:12:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88304119 * 10240; time = 8.9166s; samplesPerSecond = 1148.4
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/14/2016 06:33:17: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563937 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3287s
MPI Rank 3: 07/14/2016 06:33:19: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 3: 07/14/2016 06:33:19: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 3: 07/14/2016 06:33:26: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:26: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:26: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu DeviceId=0 timestamping=true numCPUThreads=2 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 14 2016 05:11:35
		Last modified date: Thu Jul 14 03:20:47 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/14/2016 06:33:33: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank0
MPI Rank 0: 07/14/2016 06:33:33: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 06:33:33: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:33: 		Built time: Jul 14 2016 05:11:35
MPI Rank 0: 07/14/2016 06:33:33: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 0: 07/14/2016 06:33:33: 		Build type: Debug
MPI Rank 0: 07/14/2016 06:33:33: 		Build target: GPU
MPI Rank 0: 07/14/2016 06:33:33: 		With 1bit-SGD: no
MPI Rank 0: 07/14/2016 06:33:33: 		Math lib: mkl
MPI Rank 0: 07/14/2016 06:33:33: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/14/2016 06:33:33: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/14/2016 06:33:33: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/14/2016 06:33:33: 		Build Branch: HEAD
MPI Rank 0: 07/14/2016 06:33:33: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 0: 07/14/2016 06:33:33: 		Built by svcphil on liana-08-w
MPI Rank 0: 07/14/2016 06:33:33: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/14/2016 06:33:33: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 06:33:34: -------------------------------------------------------------------
MPI Rank 0: 07/14/2016 06:33:34: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 07/14/2016 06:33:34: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: Running on cntk-muc01 at 2016/07/14 06:33:34
MPI Rank 0: 07/14/2016 06:33:34: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 06:33:34: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=2
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/14/2016 06:33:34: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=2
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:34: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/14/2016 06:33:34: Commands: train
MPI Rank 0: 07/14/2016 06:33:34: Precision = "float"
MPI Rank 0: 07/14/2016 06:33:35: Using 2 CPU threads.
MPI Rank 0: 07/14/2016 06:33:35: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: 07/14/2016 06:33:35: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/14/2016 06:33:35: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:35: ##############################################################################
MPI Rank 0: 07/14/2016 06:33:35: #                                                                            #
MPI Rank 0: 07/14/2016 06:33:35: # Action "train"                                                             #
MPI Rank 0: 07/14/2016 06:33:35: #                                                                            #
MPI Rank 0: 07/14/2016 06:33:35: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:35: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:35: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:41: Loaded model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:41: Training criterion node(s):
MPI Rank 0: 07/14/2016 06:33:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 000000A4989D5EF0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 000000A4ADB4B8A0: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 000000A4ADB4D6B0: {[G Value[1 x 1]] }
MPI Rank 0: 000000A4AE051B60: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 000000A4AE051C30: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 000000A4AE051DD0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 000000A4AE051EA0: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 000000A4AE051F70: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 000000A4AE052040: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 000000A4AE0522B0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 000000A4AE052380: {[Query Value[49292 x *1]] }
MPI Rank 0: 000000A4AE052520: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 000000A4AE0525F0: {[ce Value[1]] }
MPI Rank 0: 000000A4AE0526C0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 000000A4AE052790: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 000000A4AE052860: {[SIM Value[51 x *1]] }
MPI Rank 0: 000000A4AE052930: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 000000A4AE052C70: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 000000A4AE052E10: {[S Value[1 x 1]] }
MPI Rank 0: 000000A4AE052EE0: {[WD1 Value[64 x 288]] }
MPI Rank 0: 000000A4AE053220: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 000000A4AE053560: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 000000A4AE053630: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 000000A4AE053700: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 000000A4AE0537D0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 000000A4AE0538A0: {[ce Gradient[1]] }
MPI Rank 0: 000000A4AE053970: {[N Value[1 x 1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/14/2016 06:33:41: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:49: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:33:50: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.04 seconds
MPI Rank 0: 07/14/2016 06:34:00:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87577038 * 10240; time = 10.0664s; samplesPerSecond = 1017.2
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.03 seconds
MPI Rank 0: 07/14/2016 06:34:09:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79361134 * 10240; time = 8.9404s; samplesPerSecond = 1145.4
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.03 seconds
MPI Rank 0: 07/14/2016 06:34:13: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.0884s
MPI Rank 0: 07/14/2016 06:34:14: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 0: 07/14/2016 06:34:14: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 0: 07/14/2016 06:34:18: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net'
MPI Rank 0: 07/14/2016 06:34:21: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:34:21: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/14/2016 06:34:21: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/14/2016 06:33:33: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank1
MPI Rank 1: 07/14/2016 06:33:33: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 06:33:33: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:33: 		Built time: Jul 14 2016 05:11:35
MPI Rank 1: 07/14/2016 06:33:33: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 1: 07/14/2016 06:33:33: 		Build type: Debug
MPI Rank 1: 07/14/2016 06:33:33: 		Build target: GPU
MPI Rank 1: 07/14/2016 06:33:33: 		With 1bit-SGD: no
MPI Rank 1: 07/14/2016 06:33:33: 		Math lib: mkl
MPI Rank 1: 07/14/2016 06:33:33: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/14/2016 06:33:33: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/14/2016 06:33:33: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/14/2016 06:33:33: 		Build Branch: HEAD
MPI Rank 1: 07/14/2016 06:33:33: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 1: 07/14/2016 06:33:33: 		Built by svcphil on liana-08-w
MPI Rank 1: 07/14/2016 06:33:33: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/14/2016 06:33:33: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 06:33:36: -------------------------------------------------------------------
MPI Rank 1: 07/14/2016 06:33:36: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 07/14/2016 06:33:36: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: Running on cntk-muc01 at 2016/07/14 06:33:36
MPI Rank 1: 07/14/2016 06:33:36: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 06:33:36: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=2
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/14/2016 06:33:36: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=2
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/14/2016 06:33:36: Commands: train
MPI Rank 1: 07/14/2016 06:33:36: Precision = "float"
MPI Rank 1: 07/14/2016 06:33:36: Using 2 CPU threads.
MPI Rank 1: 07/14/2016 06:33:36: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: 07/14/2016 06:33:36: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/14/2016 06:33:36: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: ##############################################################################
MPI Rank 1: 07/14/2016 06:33:36: #                                                                            #
MPI Rank 1: 07/14/2016 06:33:36: # Action "train"                                                             #
MPI Rank 1: 07/14/2016 06:33:36: #                                                                            #
MPI Rank 1: 07/14/2016 06:33:36: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:36: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:41: Loaded model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:41: Training criterion node(s):
MPI Rank 1: 07/14/2016 06:33:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 00000026446020A0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 0000002644602170: {[ce Value[1]] }
MPI Rank 1: 0000002644602310: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 00000026446028C0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 0000002644602990: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0000002644602A60: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 0000002644602CD0: {[N Value[1 x 1]] }
MPI Rank 1: 0000002644602DA0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 0000002644602E70: {[ce Gradient[1]] }
MPI Rank 1: 0000002644602F40: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 00000026446030E0: {[Query Value[49292 x *1]] }
MPI Rank 1: 00000026446031B0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 00000026446034F0: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 00000026446035C0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 0000002644603690: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0000002644603830: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 0000002644603900: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 00000026446039D0: {[S Value[1 x 1]] }
MPI Rank 1: 0000002644603AA0: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0000002644603B70: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0000002644603C40: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 0000002644603DE0: {[SIM Value[51 x *1]] }
MPI Rank 1: 0000002644603EB0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 0000002644603F80: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 0000002644FC99D0: {[G Value[1 x 1]] }
MPI Rank 1: 0000002644FC9AA0: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 00000026469A7D00: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/14/2016 06:33:41: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:49: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:33:50: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.04 seconds
MPI Rank 1: 07/14/2016 06:34:00:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93745022 * 10240; time = 10.1125s; samplesPerSecond = 1012.6
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.63 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.03 seconds
MPI Rank 1: 07/14/2016 06:34:09:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89571209 * 10240; time = 8.9404s; samplesPerSecond = 1145.4
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.03 seconds
MPI Rank 1: 07/14/2016 06:34:13: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.0885s
MPI Rank 1: 07/14/2016 06:34:14: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 1: 07/14/2016 06:34:14: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 1: 07/14/2016 06:34:21: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:34:21: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/14/2016 06:34:21: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/14/2016 06:33:34: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank2
MPI Rank 2: 07/14/2016 06:33:34: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 06:33:34: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:34: 		Built time: Jul 14 2016 05:11:35
MPI Rank 2: 07/14/2016 06:33:34: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 2: 07/14/2016 06:33:34: 		Build type: Debug
MPI Rank 2: 07/14/2016 06:33:34: 		Build target: GPU
MPI Rank 2: 07/14/2016 06:33:34: 		With 1bit-SGD: no
MPI Rank 2: 07/14/2016 06:33:34: 		Math lib: mkl
MPI Rank 2: 07/14/2016 06:33:34: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/14/2016 06:33:34: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/14/2016 06:33:34: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/14/2016 06:33:34: 		Build Branch: HEAD
MPI Rank 2: 07/14/2016 06:33:34: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 2: 07/14/2016 06:33:34: 		Built by svcphil on liana-08-w
MPI Rank 2: 07/14/2016 06:33:34: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/14/2016 06:33:34: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 06:33:35: -------------------------------------------------------------------
MPI Rank 2: 07/14/2016 06:33:35: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 07/14/2016 06:33:35: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: Running on cntk-muc01 at 2016/07/14 06:33:35
MPI Rank 2: 07/14/2016 06:33:35: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 06:33:35: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=2
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/14/2016 06:33:35: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=2
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/14/2016 06:33:35: Commands: train
MPI Rank 2: 07/14/2016 06:33:35: Precision = "float"
MPI Rank 2: 07/14/2016 06:33:35: Using 2 CPU threads.
MPI Rank 2: 07/14/2016 06:33:35: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: 07/14/2016 06:33:35: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/14/2016 06:33:35: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: ##############################################################################
MPI Rank 2: 07/14/2016 06:33:35: #                                                                            #
MPI Rank 2: 07/14/2016 06:33:35: # Action "train"                                                             #
MPI Rank 2: 07/14/2016 06:33:35: #                                                                            #
MPI Rank 2: 07/14/2016 06:33:35: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:35: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:41: Loaded model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:41: Training criterion node(s):
MPI Rank 2: 07/14/2016 06:33:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000CB148D6C20: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 000000CB27BA9680: {[G Value[1 x 1]] }
MPI Rank 2: 000000CB27BA9750: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 000000CB295FB6C0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 000000CB295FB790: {[N Value[1 x 1]] }
MPI Rank 2: 000000CB295FB930: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000CB295FBA00: {[S Value[1 x 1]] }
MPI Rank 2: 000000CB295FBAD0: {[ce Value[1]] }
MPI Rank 2: 000000CB295FBBA0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 000000CB295FBE10: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 000000CB295FC080: {[Query Value[49292 x *1]] }
MPI Rank 2: 000000CB295FC150: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 000000CB295FC3C0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 000000CB295FC490: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 000000CB295FC560: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 000000CB295FC700: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000CB295FC7D0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 000000CB295FC8A0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 000000CB295FC970: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000CB295FCA40: {[ce Gradient[1]] }
MPI Rank 2: 000000CB295FCBE0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 000000CB295FCD80: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 000000CB295FCE50: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 000000CB295FCF20: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000CB295FD0C0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000CB295FD190: {[SIM Value[51 x *1]] }
MPI Rank 2: 000000CB295FD330: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/14/2016 06:33:41: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:49: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:33:50: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.19 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.14 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.63 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.63 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/14/2016 06:34:00:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96188030 * 10240; time = 10.0831s; samplesPerSecond = 1015.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.96 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.05 seconds
MPI Rank 2: 07/14/2016 06:34:09:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90950069 * 10240; time = 8.9402s; samplesPerSecond = 1145.4
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.04 seconds
MPI Rank 2: 07/14/2016 06:34:13: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.0884s
MPI Rank 2: 07/14/2016 06:34:14: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 2: 07/14/2016 06:34:14: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 2: 07/14/2016 06:34:21: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:34:21: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/14/2016 06:34:21: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/14/2016 06:33:34: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr_train.logrank3
MPI Rank 3: 07/14/2016 06:33:34: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 06:33:34: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:34: 		Built time: Jul 14 2016 05:11:35
MPI Rank 3: 07/14/2016 06:33:34: 		Last modified date: Thu Jul 14 03:20:47 2016
MPI Rank 3: 07/14/2016 06:33:34: 		Build type: Debug
MPI Rank 3: 07/14/2016 06:33:34: 		Build target: GPU
MPI Rank 3: 07/14/2016 06:33:34: 		With 1bit-SGD: no
MPI Rank 3: 07/14/2016 06:33:34: 		Math lib: mkl
MPI Rank 3: 07/14/2016 06:33:34: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/14/2016 06:33:34: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/14/2016 06:33:34: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/14/2016 06:33:34: 		Build Branch: HEAD
MPI Rank 3: 07/14/2016 06:33:34: 		Build SHA1: 72bee394bf461e8f6f0feb593a8416c05f481957
MPI Rank 3: 07/14/2016 06:33:34: 		Built by svcphil on liana-08-w
MPI Rank 3: 07/14/2016 06:33:34: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/14/2016 06:33:34: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 06:33:35: -------------------------------------------------------------------
MPI Rank 3: 07/14/2016 06:33:35: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 07/14/2016 06:33:35: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: Running on cntk-muc01 at 2016/07/14 06:33:35
MPI Rank 3: 07/14/2016 06:33:35: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=2  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 06:33:35: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=2
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/14/2016 06:33:35: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=2
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=2
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/14/2016 06:33:35: Commands: train
MPI Rank 3: 07/14/2016 06:33:35: Precision = "float"
MPI Rank 3: 07/14/2016 06:33:35: Using 2 CPU threads.
MPI Rank 3: 07/14/2016 06:33:35: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: 07/14/2016 06:33:35: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/14/2016 06:33:35: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: ##############################################################################
MPI Rank 3: 07/14/2016 06:33:35: #                                                                            #
MPI Rank 3: 07/14/2016 06:33:35: # Action "train"                                                             #
MPI Rank 3: 07/14/2016 06:33:35: #                                                                            #
MPI Rank 3: 07/14/2016 06:33:35: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:35: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160714055016.501646\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:41: Loaded model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:41: Training criterion node(s):
MPI Rank 3: 07/14/2016 06:33:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000366C8A8A00: {[N Value[1 x 1]] }
MPI Rank 3: 000000366C8A8AD0: {[Query Value[49292 x *1]] }
MPI Rank 3: 000000366C8A8BA0: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000366C8A8C70: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 000000366C8A8D40: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 000000366C8A8FB0: {[S Value[1 x 1]] }
MPI Rank 3: 000000366C8A9150: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 000000366C8A93C0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 000000366C8A9560: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 000000366C8A9700: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 000000366C8A97D0: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000366C8A9970: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000366C8A9A40: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 000000366C8A9B10: {[ce Value[1]] }
MPI Rank 3: 000000366C8A9E50: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000366C8A9F20: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 000000366C8A9FF0: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000366C8AA0C0: {[SIM Value[51 x *1]] }
MPI Rank 3: 000000366C8AA260: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000366C8AA330: {[ce Gradient[1]] }
MPI Rank 3: 000000366C8AA4D0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 000000366C8AA5A0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000366C8AA740: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 000000366C8AA8E0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000366E21ADC0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 000000366EA2D5E0: {[G Value[1 x 1]] }
MPI Rank 3: 000000366EA2DD30: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/14/2016 06:33:42: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:49: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:33:50: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.53 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.06 seconds
MPI Rank 3: 07/14/2016 06:34:00:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91174278 * 10240; time = 10.1182s; samplesPerSecond = 1012.0
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.63 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.05 seconds
MPI Rank 3: 07/14/2016 06:34:09:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91370468 * 10240; time = 8.9402s; samplesPerSecond = 1145.4
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.04 seconds
MPI Rank 3: 07/14/2016 06:34:13: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.0885s
MPI Rank 3: 07/14/2016 06:34:14: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 3: 07/14/2016 06:34:14: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 3: 07/14/2016 06:34:21: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:34:21: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/14/2016 06:34:21: __COMPLETED__
MPI Rank 3: ~MPIWrapper