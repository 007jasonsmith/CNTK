CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 268381192 kB
-------------------------------------------------------------------
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu DeviceId=0 timestamping=true numCPUThreads=6 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/13/2016 04:44:34: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank0
MPI Rank 0: 07/13/2016 04:44:34: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:44:34: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:34: 		Built time: Jul 13 2016 03:39:41
MPI Rank 0: 07/13/2016 04:44:34: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 0: 07/13/2016 04:44:34: 		Build type: Debug
MPI Rank 0: 07/13/2016 04:44:34: 		Build target: GPU
MPI Rank 0: 07/13/2016 04:44:34: 		With 1bit-SGD: no
MPI Rank 0: 07/13/2016 04:44:34: 		Math lib: mkl
MPI Rank 0: 07/13/2016 04:44:34: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/13/2016 04:44:34: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/13/2016 04:44:34: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/13/2016 04:44:34: 		Build Branch: HEAD
MPI Rank 0: 07/13/2016 04:44:34: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 0: 07/13/2016 04:44:34: 		Built by svcphil on liana-08-w
MPI Rank 0: 07/13/2016 04:44:34: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/13/2016 04:44:34: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:44:37: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:44:37: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:44:37: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:44:37: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:44:37: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:44:37: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: Running on DPHAIM-24 at 2016/07/13 04:44:37
MPI Rank 0: 07/13/2016 04:44:37: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:44:37: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:44:37: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/13/2016 04:44:37: Commands: train
MPI Rank 0: 07/13/2016 04:44:37: Precision = "float"
MPI Rank 0: 07/13/2016 04:44:37: Using 6 CPU threads.
MPI Rank 0: 07/13/2016 04:44:37: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: 07/13/2016 04:44:37: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/13/2016 04:44:37: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: ##############################################################################
MPI Rank 0: 07/13/2016 04:44:37: #                                                                            #
MPI Rank 0: 07/13/2016 04:44:37: # Action "train"                                                             #
MPI Rank 0: 07/13/2016 04:44:37: #                                                                            #
MPI Rank 0: 07/13/2016 04:44:37: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:37: Creating virgin network.
MPI Rank 0: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:38: Created model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:38: Training criterion node(s):
MPI Rank 0: 07/13/2016 04:44:38: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 000000BFB0F17BC0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 000000BFCC655310: {[S Value[1 x 1]] }
MPI Rank 0: 000000BFCC655C00: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 000000BFCC655DA0: {[Keyword Value[49292 x *]] }
MPI Rank 0: 000000BFCC655E70: {[WD1 Value[64 x 288]] }
MPI Rank 0: 000000BFCC6560E0: {[Query Value[49292 x *]] }
MPI Rank 0: 000000BFCC656350: {[N Value[1 x 1]] }
MPI Rank 0: 000000BFCC656690: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 000000BFCC656830: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 000000BFCC656900: {[SIM Value[51 x *]] }
MPI Rank 0: 000000BFCC6569D0: {[ce Value[1]] }
MPI Rank 0: 000000BFCC656AA0: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 000000BFCC656C40: {[G Value[1 x 1]] }
MPI Rank 0: 000000BFCC656D10: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 000000BFCED74690: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 000000BFCED74900: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 000000BFCED74AA0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 000000BFCED74D10: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 000000BFCED75390: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 000000BFCED75460: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 000000BFCED75530: {[SIM Gradient[51 x *]] }
MPI Rank 0: 000000BFCED75600: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 000000BFCED757A0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 000000BFCED75A10: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 000000BFCED75C80: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 000000BFCED76160: {[ce Gradient[1]] }
MPI Rank 0: 000000BFCED763D0: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/13/2016 04:44:38: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:44: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:44:45: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.32-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.32 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.25 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.23 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.23 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.18 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 0.19 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.18 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.16 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.16 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.16 seconds
MPI Rank 0: 07/13/2016 04:44:54:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.34696808 * 10240; time = 8.7688s; samplesPerSecond = 1167.8
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 0.15 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.14 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 0.14 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.13 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 0.13 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.06 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 2.09 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.20 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.30 seconds , average latency = 0.12 seconds
MPI Rank 0: 07/13/2016 04:45:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.34277344 * 10240; time = 8.3837s; samplesPerSecond = 1221.4
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.41 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.41 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.52 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 2.60 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.60 seconds , average latency = 0.10 seconds
MPI Rank 0: 07/13/2016 04:45:06: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=21.9883s
MPI Rank 0: 07/13/2016 04:45:07: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 0: 07/13/2016 04:45:07: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 0: 07/13/2016 04:45:11: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.18 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.15 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.13 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.11 seconds
MPI Rank 0: 07/13/2016 04:45:23:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.30270958 * 10240; time = 8.3992s; samplesPerSecond = 1219.2
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/13/2016 04:45:32:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.09883766 * 10240; time = 8.3723s; samplesPerSecond = 1223.1
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.88 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.15 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.25 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.25 seconds , average latency = 0.09 seconds
MPI Rank 0: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=20.967s
MPI Rank 0: 07/13/2016 04:45:36: Final Results: Minibatch[1-26]: ce = 1.97005574 * 102399; perplexity = 7.17107619
MPI Rank 0: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005574 * 102399
MPI Rank 0: 07/13/2016 04:45:40: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:44: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:45:44: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.08 seconds
MPI Rank 0: 07/13/2016 04:45:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.89778175 * 10240; time = 8.3754s; samplesPerSecond = 1222.6
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:46:01:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86335983 * 10240; time = 8.3520s; samplesPerSecond = 1226.1
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=20.9067s
MPI Rank 0: 07/13/2016 04:46:05: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 0: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 0: 07/13/2016 04:46:09: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net'
MPI Rank 0: 07/13/2016 04:46:13: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:13: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:13: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/13/2016 04:44:34: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank1
MPI Rank 1: 07/13/2016 04:44:34: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:44:34: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:34: 		Built time: Jul 13 2016 03:39:41
MPI Rank 1: 07/13/2016 04:44:34: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 1: 07/13/2016 04:44:34: 		Build type: Debug
MPI Rank 1: 07/13/2016 04:44:34: 		Build target: GPU
MPI Rank 1: 07/13/2016 04:44:34: 		With 1bit-SGD: no
MPI Rank 1: 07/13/2016 04:44:34: 		Math lib: mkl
MPI Rank 1: 07/13/2016 04:44:34: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/13/2016 04:44:34: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/13/2016 04:44:34: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/13/2016 04:44:34: 		Build Branch: HEAD
MPI Rank 1: 07/13/2016 04:44:34: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 1: 07/13/2016 04:44:34: 		Built by svcphil on liana-08-w
MPI Rank 1: 07/13/2016 04:44:34: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/13/2016 04:44:34: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:44:38: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:44:38: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:44:38: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:44:38: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:44:38: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:44:38: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: Running on DPHAIM-24 at 2016/07/13 04:44:38
MPI Rank 1: 07/13/2016 04:44:38: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:44:38: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:44:38: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/13/2016 04:44:38: Commands: train
MPI Rank 1: 07/13/2016 04:44:38: Precision = "float"
MPI Rank 1: 07/13/2016 04:44:38: Using 6 CPU threads.
MPI Rank 1: 07/13/2016 04:44:38: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: 07/13/2016 04:44:38: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/13/2016 04:44:38: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: ##############################################################################
MPI Rank 1: 07/13/2016 04:44:38: #                                                                            #
MPI Rank 1: 07/13/2016 04:44:38: # Action "train"                                                             #
MPI Rank 1: 07/13/2016 04:44:38: #                                                                            #
MPI Rank 1: 07/13/2016 04:44:38: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:38: Creating virgin network.
MPI Rank 1: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:39: Created model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:39: Training criterion node(s):
MPI Rank 1: 07/13/2016 04:44:39: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000B35FE85480: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000B37B651800: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000B37B6519A0: {[Query Value[49292 x *]] }
MPI Rank 1: 000000B37B651CE0: {[Keyword Value[49292 x *]] }
MPI Rank 1: 000000B37B651DB0: {[S Value[1 x 1]] }
MPI Rank 1: 000000B37B651F50: {[N Value[1 x 1]] }
MPI Rank 1: 000000B37B6520F0: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000B37B6521C0: {[G Value[1 x 1]] }
MPI Rank 1: 000000B37B652290: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 000000B37B652770: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 000000B37B652840: {[SIM Value[51 x *]] }
MPI Rank 1: 000000B37B652910: {[ce Value[1]] }
MPI Rank 1: 000000B37B6529E0: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 000000B37B652C50: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 000000B37DD12810: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 000000B37DD129B0: {[ce Gradient[1]] }
MPI Rank 1: 000000B37DD12A80: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 000000B37DD12C20: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 000000B37DD12CF0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 000000B37DD131D0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 000000B37DD136B0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 000000B37DD13780: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 000000B37DD139F0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000B37DD13FA0: {[SIM Gradient[51 x *]] }
MPI Rank 1: 000000B37DD14140: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 000000B37DD142E0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000B37DD14550: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/13/2016 04:44:39: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:44: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:44:45: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 1: 07/13/2016 04:44:54:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32159653 * 10240; time = 8.7116s; samplesPerSecond = 1175.4
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 04:45:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.33525467 * 10240; time = 8.3836s; samplesPerSecond = 1221.4
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 04:45:06: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=21.9883s
MPI Rank 1: 07/13/2016 04:45:07: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 1: 07/13/2016 04:45:07: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 04:45:23:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32732925 * 10240; time = 8.3983s; samplesPerSecond = 1219.3
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 04:45:32:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11035995 * 10240; time = 8.3724s; samplesPerSecond = 1223.1
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.00 seconds
MPI Rank 1: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=20.9669s
MPI Rank 1: 07/13/2016 04:45:36: Final Results: Minibatch[1-26]: ce = 1.97005574 * 102399; perplexity = 7.17107619
MPI Rank 1: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005574 * 102399
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:44: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:45:44: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 04:45:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.92909813 * 10240; time = 8.3748s; samplesPerSecond = 1222.7
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 04:46:01:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86598778 * 10240; time = 8.3519s; samplesPerSecond = 1226.1
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.01 seconds
MPI Rank 1: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=20.9067s
MPI Rank 1: 07/13/2016 04:46:05: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 1: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 1: 07/13/2016 04:46:13: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:13: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:13: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/13/2016 04:44:35: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank2
MPI Rank 2: 07/13/2016 04:44:35: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:44:35: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:35: 		Built time: Jul 13 2016 03:39:41
MPI Rank 2: 07/13/2016 04:44:35: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 2: 07/13/2016 04:44:35: 		Build type: Debug
MPI Rank 2: 07/13/2016 04:44:35: 		Build target: GPU
MPI Rank 2: 07/13/2016 04:44:35: 		With 1bit-SGD: no
MPI Rank 2: 07/13/2016 04:44:35: 		Math lib: mkl
MPI Rank 2: 07/13/2016 04:44:35: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/13/2016 04:44:35: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/13/2016 04:44:35: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/13/2016 04:44:35: 		Build Branch: HEAD
MPI Rank 2: 07/13/2016 04:44:35: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 2: 07/13/2016 04:44:35: 		Built by svcphil on liana-08-w
MPI Rank 2: 07/13/2016 04:44:35: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/13/2016 04:44:35: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:44:38: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:44:38: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:44:38: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:44:38: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:44:38: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:44:38: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: Running on DPHAIM-24 at 2016/07/13 04:44:38
MPI Rank 2: 07/13/2016 04:44:38: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:44:38: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:44:38: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/13/2016 04:44:38: Commands: train
MPI Rank 2: 07/13/2016 04:44:38: Precision = "float"
MPI Rank 2: 07/13/2016 04:44:38: Using 6 CPU threads.
MPI Rank 2: 07/13/2016 04:44:38: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: 07/13/2016 04:44:38: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/13/2016 04:44:38: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: ##############################################################################
MPI Rank 2: 07/13/2016 04:44:38: #                                                                            #
MPI Rank 2: 07/13/2016 04:44:38: # Action "train"                                                             #
MPI Rank 2: 07/13/2016 04:44:38: #                                                                            #
MPI Rank 2: 07/13/2016 04:44:38: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:38: Creating virgin network.
MPI Rank 2: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:39: Created model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:39: Training criterion node(s):
MPI Rank 2: 07/13/2016 04:44:39: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 00000078AABEFF30: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 00000078B13715B0: {[WD1 Value[64 x 288]] }
MPI Rank 2: 00000078B1371680: {[Keyword Value[49292 x *]] }
MPI Rank 2: 00000078B1371EA0: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 00000078B1371F70: {[Query Value[49292 x *]] }
MPI Rank 2: 00000078B1372110: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 00000078B13721E0: {[S Value[1 x 1]] }
MPI Rank 2: 00000078B13722B0: {[N Value[1 x 1]] }
MPI Rank 2: 00000078B1372450: {[G Value[1 x 1]] }
MPI Rank 2: 00000078B1372520: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 00000078B13725F0: {[SIM Value[51 x *]] }
MPI Rank 2: 00000078B13726C0: {[ce Value[1]] }
MPI Rank 2: 00000078B1372930: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 00000078B1372AD0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 00000078B43D7F60: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 00000078B43D86B0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 00000078B43D8920: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 00000078B43D9070: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 00000078B43D92E0: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 00000078B43D9480: {[SIM Gradient[51 x *]] }
MPI Rank 2: 00000078B43D9620: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 00000078B43D97C0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 00000078B43D9890: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 00000078B43D9A30: {[ce Gradient[1]] }
MPI Rank 2: 00000078B43D9B00: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 00000078B43D9BD0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 00000078B43D9CA0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/13/2016 04:44:40: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:44: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:44:45: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.44-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.44 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.26 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.25 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.24 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.21 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.21 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.19 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.19 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.17 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.16 seconds
MPI Rank 2: 07/13/2016 04:44:54:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32837563 * 10240; time = 8.7667s; samplesPerSecond = 1168.1
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.15 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 0.15 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 0.15 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 0.15 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.24 seconds , average latency = 0.15 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.24 seconds , average latency = 0.14 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.34 seconds , average latency = 0.14 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 2.42 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 2.50 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.61 seconds , average latency = 0.13 seconds
MPI Rank 2: 07/13/2016 04:45:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35655479 * 10240; time = 8.3837s; samplesPerSecond = 1221.4
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 2.66 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.77 seconds , average latency = 0.13 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.77 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 2.91 seconds , average latency = 0.12 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.91 seconds , average latency = 0.12 seconds
MPI Rank 2: 07/13/2016 04:45:06: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=21.9883s
MPI Rank 2: 07/13/2016 04:45:07: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 2: 07/13/2016 04:45:07: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.07 seconds
MPI Rank 2: 07/13/2016 04:45:23:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32893581 * 10240; time = 8.3992s; samplesPerSecond = 1219.2
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.08 seconds
MPI Rank 2: 07/13/2016 04:45:32:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11646938 * 10240; time = 8.3723s; samplesPerSecond = 1223.1
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 0.08 seconds
MPI Rank 2: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=20.9669s
MPI Rank 2: 07/13/2016 04:45:36: Final Results: Minibatch[1-26]: ce = 1.97005574 * 102399; perplexity = 7.17107619
MPI Rank 2: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005574 * 102399
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:44: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:45:44: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/13/2016 04:45:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95308418 * 10240; time = 8.3754s; samplesPerSecond = 1222.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 0.08 seconds
MPI Rank 2: 07/13/2016 04:46:01:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87902641 * 10240; time = 8.3520s; samplesPerSecond = 1226.1
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 0.08 seconds
MPI Rank 2: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=20.9067s
MPI Rank 2: 07/13/2016 04:46:05: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 2: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 2: 07/13/2016 04:46:13: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:13: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:13: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/13/2016 04:44:35: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank3
MPI Rank 3: 07/13/2016 04:44:35: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:44:35: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:35: 		Built time: Jul 13 2016 03:39:41
MPI Rank 3: 07/13/2016 04:44:35: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 3: 07/13/2016 04:44:35: 		Build type: Debug
MPI Rank 3: 07/13/2016 04:44:35: 		Build target: GPU
MPI Rank 3: 07/13/2016 04:44:35: 		With 1bit-SGD: no
MPI Rank 3: 07/13/2016 04:44:35: 		Math lib: mkl
MPI Rank 3: 07/13/2016 04:44:35: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/13/2016 04:44:35: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/13/2016 04:44:35: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/13/2016 04:44:35: 		Build Branch: HEAD
MPI Rank 3: 07/13/2016 04:44:35: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 3: 07/13/2016 04:44:35: 		Built by svcphil on liana-08-w
MPI Rank 3: 07/13/2016 04:44:35: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/13/2016 04:44:35: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:44:39: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:44:39: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:44:39: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:44:39: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:44:39: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:44:39: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: Running on DPHAIM-24 at 2016/07/13 04:44:39
MPI Rank 3: 07/13/2016 04:44:39: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:44:39: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:44:39: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/13/2016 04:44:39: Commands: train
MPI Rank 3: 07/13/2016 04:44:39: Precision = "float"
MPI Rank 3: 07/13/2016 04:44:39: Using 6 CPU threads.
MPI Rank 3: 07/13/2016 04:44:39: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: 07/13/2016 04:44:39: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/13/2016 04:44:39: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: ##############################################################################
MPI Rank 3: 07/13/2016 04:44:39: #                                                                            #
MPI Rank 3: 07/13/2016 04:44:39: # Action "train"                                                             #
MPI Rank 3: 07/13/2016 04:44:39: #                                                                            #
MPI Rank 3: 07/13/2016 04:44:39: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:39: Creating virgin network.
MPI Rank 3: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:40: Created model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:40: Training criterion node(s):
MPI Rank 3: 07/13/2016 04:44:40: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 0000001C88CC3E60: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 0000001C88CC40D0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 0000001C88CC4270: {[ce Gradient[1]] }
MPI Rank 3: 0000001C88CC4B60: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 0000001C88CC4C30: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 0000001C88CC4DD0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 0000001C88CC4EA0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 0000001C88CC4F70: {[SIM Gradient[51 x *]] }
MPI Rank 3: 0000001C88CC5040: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 0000001C88CC51E0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 0000001C88CC5450: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 0000001C88CC55F0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 0000001C88CC5A00: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0000001CE9069610: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0000001CEB20C6B0: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0000001CEB20C920: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 0000001CEB20C9F0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 0000001CEB20D480: {[Query Value[49292 x *]] }
MPI Rank 3: 0000001CEB20D550: {[S Value[1 x 1]] }
MPI Rank 3: 0000001CEB20D6F0: {[WD1 Value[64 x 288]] }
MPI Rank 3: 0000001CEB20D960: {[G Value[1 x 1]] }
MPI Rank 3: 0000001CEB20DB00: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 0000001CEB20DBD0: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0000001CEB20DCA0: {[SIM Value[51 x *]] }
MPI Rank 3: 0000001CEB20DD70: {[ce Value[1]] }
MPI Rank 3: 0000001CEB20E180: {[N Value[1 x 1]] }
MPI Rank 3: 0000001CEB20E4C0: {[Keyword Value[49292 x *]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/13/2016 04:44:40: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:44: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:44:45: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.05 seconds
MPI Rank 3: 07/13/2016 04:44:54:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32287750 * 10240; time = 8.7176s; samplesPerSecond = 1174.6
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.05 seconds
MPI Rank 3: 07/13/2016 04:45:02:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35470428 * 10240; time = 8.3837s; samplesPerSecond = 1221.4
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.05 seconds
MPI Rank 3: 07/13/2016 04:45:06: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601706 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=21.9883s
MPI Rank 3: 07/13/2016 04:45:07: Final Results: Minibatch[1-26]: ce = 2.49916008 * 102399; perplexity = 12.17226592
MPI Rank 3: 07/13/2016 04:45:07: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916008 * 102399
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:15: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:15: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.03 seconds
MPI Rank 3: 07/13/2016 04:45:23:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29653873 * 10240; time = 8.3983s; samplesPerSecond = 1219.3
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.03 seconds
MPI Rank 3: 07/13/2016 04:45:32:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11679478 * 10240; time = 8.3721s; samplesPerSecond = 1223.1
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577526 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=20.9669s
MPI Rank 3: 07/13/2016 04:45:36: Final Results: Minibatch[1-26]: ce = 1.97005574 * 102399; perplexity = 7.17107619
MPI Rank 3: 07/13/2016 04:45:36: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005574 * 102399
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:44: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:45:44: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.03 seconds
MPI Rank 3: 07/13/2016 04:45:52:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90347176 * 10240; time = 8.3746s; samplesPerSecond = 1222.7
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/13/2016 04:46:01:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88304176 * 10240; time = 8.3519s; samplesPerSecond = 1226.1
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.02 seconds
MPI Rank 3: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563945 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=20.9067s
MPI Rank 3: 07/13/2016 04:46:05: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525580
MPI Rank 3: 07/13/2016 04:46:05: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 3: 07/13/2016 04:46:13: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:13: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:13: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu DeviceId=0 timestamping=true numCPUThreads=6 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Jul 13 2016 03:39:41
		Last modified date: Fri Jul  8 10:29:45 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
		Built by svcphil on liana-08-w
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 07/13/2016 04:46:21: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank0
MPI Rank 0: 07/13/2016 04:46:21: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:46:21: Build info: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:21: 		Built time: Jul 13 2016 03:39:41
MPI Rank 0: 07/13/2016 04:46:21: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 0: 07/13/2016 04:46:21: 		Build type: Debug
MPI Rank 0: 07/13/2016 04:46:21: 		Build target: GPU
MPI Rank 0: 07/13/2016 04:46:21: 		With 1bit-SGD: no
MPI Rank 0: 07/13/2016 04:46:21: 		Math lib: mkl
MPI Rank 0: 07/13/2016 04:46:21: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 07/13/2016 04:46:21: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 07/13/2016 04:46:21: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 07/13/2016 04:46:21: 		Build Branch: HEAD
MPI Rank 0: 07/13/2016 04:46:21: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 0: 07/13/2016 04:46:21: 		Built by svcphil on liana-08-w
MPI Rank 0: 07/13/2016 04:46:21: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 0: 07/13/2016 04:46:21: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:46:25: -------------------------------------------------------------------
MPI Rank 0: 07/13/2016 04:46:25: GPU info:
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:46:25: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:46:25: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:46:25: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 0: 07/13/2016 04:46:25: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: Running on DPHAIM-24 at 2016/07/13 04:46:25
MPI Rank 0: 07/13/2016 04:46:25: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:46:25: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 07/13/2016 04:46:25: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 07/13/2016 04:46:25: Commands: train
MPI Rank 0: 07/13/2016 04:46:25: Precision = "float"
MPI Rank 0: 07/13/2016 04:46:25: Using 6 CPU threads.
MPI Rank 0: 07/13/2016 04:46:25: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 0: 07/13/2016 04:46:25: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 07/13/2016 04:46:25: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: ##############################################################################
MPI Rank 0: 07/13/2016 04:46:25: #                                                                            #
MPI Rank 0: 07/13/2016 04:46:25: # Action "train"                                                             #
MPI Rank 0: 07/13/2016 04:46:25: #                                                                            #
MPI Rank 0: 07/13/2016 04:46:25: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:25: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:32: Loaded model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:32: Training criterion node(s):
MPI Rank 0: 07/13/2016 04:46:32: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 00000093FB959340: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 00000093FC7C9F90: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 00000093FC7CA060: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 00000093FC7CA6E0: {[WD1 Value[64 x 288]] }
MPI Rank 0: 00000093FC7CA7B0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 00000093FC7CAA20: {[SIM Value[51 x *1]] }
MPI Rank 0: 00000093FC7CABC0: {[S Value[1 x 1]] }
MPI Rank 0: 00000093FC7CAC90: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 00000093FC7CAD60: {[ce Value[1]] }
MPI Rank 0: 00000093FC7CB4B0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 00000093FC7CB650: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 00000093FC7CB720: {[N Value[1 x 1]] }
MPI Rank 0: 00000093FC7CBB30: {[Query Value[49292 x *1]] }
MPI Rank 0: 00000093FC7CBE70: {[G Value[1 x 1]] }
MPI Rank 0: 00000093FED62F10: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 00000093FED633F0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 00000093FED638D0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 00000093FED639A0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 00000093FED63C10: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 00000093FED63CE0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 00000093FED64020: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 00000093FED640F0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 00000093FED64290: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 00000093FED645D0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 00000093FED646A0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000093FED64840: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000093FED64B80: {[ce Gradient[1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 07/13/2016 04:46:32: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:39: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:46:41: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.14 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.12 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.11 seconds
MPI Rank 0: 07/13/2016 04:46:49:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87577038 * 10240; time = 8.4395s; samplesPerSecond = 1213.3
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.10 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:46:57:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79361134 * 10240; time = 8.2921s; samplesPerSecond = 1234.9
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.07 seconds
MPI Rank 0: 07/13/2016 04:47:02: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3996s
MPI Rank 0: 07/13/2016 04:47:03: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241658
MPI Rank 0: 07/13/2016 04:47:03: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 0: 07/13/2016 04:47:07: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net'
MPI Rank 0: 07/13/2016 04:47:11: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:47:11: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 07/13/2016 04:47:11: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 07/13/2016 04:46:21: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank1
MPI Rank 1: 07/13/2016 04:46:21: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:46:21: Build info: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:21: 		Built time: Jul 13 2016 03:39:41
MPI Rank 1: 07/13/2016 04:46:21: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 1: 07/13/2016 04:46:21: 		Build type: Debug
MPI Rank 1: 07/13/2016 04:46:21: 		Build target: GPU
MPI Rank 1: 07/13/2016 04:46:21: 		With 1bit-SGD: no
MPI Rank 1: 07/13/2016 04:46:21: 		Math lib: mkl
MPI Rank 1: 07/13/2016 04:46:21: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 07/13/2016 04:46:21: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 07/13/2016 04:46:21: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 07/13/2016 04:46:21: 		Build Branch: HEAD
MPI Rank 1: 07/13/2016 04:46:21: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 1: 07/13/2016 04:46:21: 		Built by svcphil on liana-08-w
MPI Rank 1: 07/13/2016 04:46:21: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 1: 07/13/2016 04:46:21: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:46:25: -------------------------------------------------------------------
MPI Rank 1: 07/13/2016 04:46:25: GPU info:
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:46:25: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:46:25: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:46:25: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 1: 07/13/2016 04:46:25: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: Running on DPHAIM-24 at 2016/07/13 04:46:25
MPI Rank 1: 07/13/2016 04:46:25: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:46:25: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 07/13/2016 04:46:25: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 07/13/2016 04:46:25: Commands: train
MPI Rank 1: 07/13/2016 04:46:25: Precision = "float"
MPI Rank 1: 07/13/2016 04:46:25: Using 6 CPU threads.
MPI Rank 1: 07/13/2016 04:46:25: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 1: 07/13/2016 04:46:25: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 07/13/2016 04:46:25: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: ##############################################################################
MPI Rank 1: 07/13/2016 04:46:25: #                                                                            #
MPI Rank 1: 07/13/2016 04:46:25: # Action "train"                                                             #
MPI Rank 1: 07/13/2016 04:46:25: #                                                                            #
MPI Rank 1: 07/13/2016 04:46:25: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:25: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:32: Loaded model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:32: Training criterion node(s):
MPI Rank 1: 07/13/2016 04:46:32: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 0000008FA0A42540: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 0000008FA0A42880: {[N Value[1 x 1]] }
MPI Rank 1: 0000008FA0A43990: {[G Value[1 x 1]] }
MPI Rank 1: 0000008FA0DFDE60: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 0000008FA0DFDF30: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0000008FA0DFE0D0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0000008FA0DFE270: {[ce Gradient[1]] }
MPI Rank 1: 0000008FA0DFE4E0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0000008FA0DFE5B0: {[SIM Value[51 x *1]] }
MPI Rank 1: 0000008FA0DFE680: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 0000008FA0DFE750: {[ce Value[1]] }
MPI Rank 1: 0000008FA0DFE820: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0000008FA0DFE8F0: {[Query Value[49292 x *1]] }
MPI Rank 1: 0000008FA0DFEA90: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 0000008FA0DFEB60: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 0000008FA0DFEC30: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0000008FA0DFEDD0: {[S Value[1 x 1]] }
MPI Rank 1: 0000008FA0DFEEA0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 0000008FA0DFF110: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 0000008FA0DFF1E0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 0000008FA0DFF2B0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0000008FA0DFF450: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0000008FA0DFF860: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0000008FA0DFFA00: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 0000008FA0DFFAD0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 0000008FA0DFFD40: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 0000008FFF3A1FC0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 07/13/2016 04:46:32: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:39: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:46:40: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.60-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.60 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.30 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.25 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.24 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 0.23 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.21 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.20 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.18 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.16 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.15 seconds
MPI Rank 1: 07/13/2016 04:46:49:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93745022 * 10240; time = 9.0011s; samplesPerSecond = 1137.6
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.14 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 0.10 seconds
MPI Rank 1: 07/13/2016 04:46:57:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89571209 * 10240; time = 8.2921s; samplesPerSecond = 1234.9
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.08 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.18 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.18 seconds , average latency = 0.09 seconds
MPI Rank 1: 07/13/2016 04:47:02: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3999s
MPI Rank 1: 07/13/2016 04:47:03: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241658
MPI Rank 1: 07/13/2016 04:47:03: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 1: 07/13/2016 04:47:11: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:47:11: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 07/13/2016 04:47:11: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 07/13/2016 04:46:22: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank2
MPI Rank 2: 07/13/2016 04:46:22: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:46:22: Build info: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:22: 		Built time: Jul 13 2016 03:39:41
MPI Rank 2: 07/13/2016 04:46:22: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 2: 07/13/2016 04:46:22: 		Build type: Debug
MPI Rank 2: 07/13/2016 04:46:22: 		Build target: GPU
MPI Rank 2: 07/13/2016 04:46:22: 		With 1bit-SGD: no
MPI Rank 2: 07/13/2016 04:46:22: 		Math lib: mkl
MPI Rank 2: 07/13/2016 04:46:22: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 07/13/2016 04:46:22: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 07/13/2016 04:46:22: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 07/13/2016 04:46:22: 		Build Branch: HEAD
MPI Rank 2: 07/13/2016 04:46:22: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 2: 07/13/2016 04:46:22: 		Built by svcphil on liana-08-w
MPI Rank 2: 07/13/2016 04:46:22: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 2: 07/13/2016 04:46:22: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:46:26: -------------------------------------------------------------------
MPI Rank 2: 07/13/2016 04:46:26: GPU info:
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:46:26: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:46:26: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:46:26: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 2: 07/13/2016 04:46:26: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: Running on DPHAIM-24 at 2016/07/13 04:46:26
MPI Rank 2: 07/13/2016 04:46:26: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:46:26: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 07/13/2016 04:46:26: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 07/13/2016 04:46:26: Commands: train
MPI Rank 2: 07/13/2016 04:46:26: Precision = "float"
MPI Rank 2: 07/13/2016 04:46:26: Using 6 CPU threads.
MPI Rank 2: 07/13/2016 04:46:26: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 2: 07/13/2016 04:46:26: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 07/13/2016 04:46:26: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: ##############################################################################
MPI Rank 2: 07/13/2016 04:46:26: #                                                                            #
MPI Rank 2: 07/13/2016 04:46:26: # Action "train"                                                             #
MPI Rank 2: 07/13/2016 04:46:26: #                                                                            #
MPI Rank 2: 07/13/2016 04:46:26: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:26: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:33: Loaded model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:33: Training criterion node(s):
MPI Rank 2: 07/13/2016 04:46:33: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000D7AF26E750: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 000000D7B63507E0: {[G Value[1 x 1]] }
MPI Rank 2: 000000D7B63508B0: {[N Value[1 x 1]] }
MPI Rank 2: 000000D7B6350980: {[Query Value[49292 x *1]] }
MPI Rank 2: 000000D7B6350B20: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000D7B6350BF0: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 000000D7B6350CC0: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000D7B6350D90: {[SIM Value[51 x *1]] }
MPI Rank 2: 000000D7B6350E60: {[ce Value[1]] }
MPI Rank 2: 000000D7B6350F30: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 000000D7B6351270: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 000000D7B6352040: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 000000D7B63521E0: {[S Value[1 x 1]] }
MPI Rank 2: 000000D7B63522B0: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 000000D7B8743F90: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000D7B8744060: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 000000D7B8744130: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 000000D7B8744200: {[ce Gradient[1]] }
MPI Rank 2: 000000D7B8744880: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000D7B8744BC0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 000000D7B8744D60: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000D7B8744F00: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 000000D7B87450A0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 000000D7B87454B0: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 000000D7B87457F0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 000000D7B87458C0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 000000D7B8745990: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 07/13/2016 04:46:33: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:39: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:46:41: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/13/2016 04:46:49:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96188030 * 10240; time = 8.4404s; samplesPerSecond = 1213.2
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.06 seconds
MPI Rank 2: 07/13/2016 04:46:57:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90950069 * 10240; time = 8.2921s; samplesPerSecond = 1234.9
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.05 seconds
MPI Rank 2: 07/13/2016 04:47:02: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3997s
MPI Rank 2: 07/13/2016 04:47:03: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241658
MPI Rank 2: 07/13/2016 04:47:03: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 2: 07/13/2016 04:47:11: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:47:11: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 07/13/2016 04:47:11: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 07/13/2016 04:46:22: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr_train.logrank3
MPI Rank 3: 07/13/2016 04:46:22: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:46:22: Build info: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:22: 		Built time: Jul 13 2016 03:39:41
MPI Rank 3: 07/13/2016 04:46:22: 		Last modified date: Fri Jul  8 10:29:45 2016
MPI Rank 3: 07/13/2016 04:46:22: 		Build type: Debug
MPI Rank 3: 07/13/2016 04:46:22: 		Build target: GPU
MPI Rank 3: 07/13/2016 04:46:22: 		With 1bit-SGD: no
MPI Rank 3: 07/13/2016 04:46:22: 		Math lib: mkl
MPI Rank 3: 07/13/2016 04:46:22: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 07/13/2016 04:46:22: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 07/13/2016 04:46:22: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 07/13/2016 04:46:22: 		Build Branch: HEAD
MPI Rank 3: 07/13/2016 04:46:22: 		Build SHA1: 50bb4c8afbc87c14548a5b5f315a064186a5cb5f
MPI Rank 3: 07/13/2016 04:46:22: 		Built by svcphil on liana-08-w
MPI Rank 3: 07/13/2016 04:46:22: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows\Source\CNTK\
MPI Rank 3: 07/13/2016 04:46:22: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:46:26: -------------------------------------------------------------------
MPI Rank 3: 07/13/2016 04:46:26: GPU info:
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:46:26: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:46:26: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:46:26: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3072 MB
MPI Rank 3: 07/13/2016 04:46:26: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: Running on DPHAIM-24 at 2016/07/13 04:46:26
MPI Rank 3: 07/13/2016 04:46:26: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\debug\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu  DeviceId=0  timestamping=true  numCPUThreads=6  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:46:26: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 07/13/2016 04:46:26: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 07/13/2016 04:46:26: Commands: train
MPI Rank 3: 07/13/2016 04:46:26: Precision = "float"
MPI Rank 3: 07/13/2016 04:46:26: Using 6 CPU threads.
MPI Rank 3: 07/13/2016 04:46:26: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net
MPI Rank 3: 07/13/2016 04:46:26: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 07/13/2016 04:46:26: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: ##############################################################################
MPI Rank 3: 07/13/2016 04:46:26: #                                                                            #
MPI Rank 3: 07/13/2016 04:46:26: # Action "train"                                                             #
MPI Rank 3: 07/13/2016 04:46:26: #                                                                            #
MPI Rank 3: 07/13/2016 04:46:26: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:26: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160713043153.833416\Text_SparseDSSM@debug_gpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:32: Loaded model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:32: Training criterion node(s):
MPI Rank 3: 07/13/2016 04:46:32: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000EB41387010: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 000000EB5C48D570: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000EB5C48D640: {[Query Value[49292 x *1]] }
MPI Rank 3: 000000EB5C48D8B0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000EB5C48DBF0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000EB5C48DD90: {[SIM Value[51 x *1]] }
MPI Rank 3: 000000EB5C48DE60: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 000000EB5C48E000: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 000000EB5C48E270: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 000000EB5C48E340: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000EB5C48E5B0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 000000EB5C48E680: {[S Value[1 x 1]] }
MPI Rank 3: 000000EB5C48E820: {[ce Gradient[1]] }
MPI Rank 3: 000000EB5C48E8F0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000EB5C48E9C0: {[ce Value[1]] }
MPI Rank 3: 000000EB5C48EC30: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 000000EB5C48EDD0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 000000EB5C48EEA0: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000EB5C48EF70: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 000000EB5C48F040: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 000000EB5C48F110: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 000000EB5C48F1E0: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000EB5C48F2B0: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000EB5C48F380: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 000000EB5ED86B50: {[G Value[1 x 1]] }
MPI Rank 3: 000000EB5ED86C20: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 000000EB5ED86CF0: {[N Value[1 x 1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 07/13/2016 04:46:32: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:39: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:46:40: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.48-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.48 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.24 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.16 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.14 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 0.10 seconds
MPI Rank 3: 07/13/2016 04:46:49:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91174278 * 10240; time = 9.0095s; samplesPerSecond = 1136.6
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 0.08 seconds
MPI Rank 3: 07/13/2016 04:46:57:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91370468 * 10240; time = 8.2921s; samplesPerSecond = 1234.9
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.07 seconds
MPI Rank 3: 07/13/2016 04:47:02: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=22.3999s
MPI Rank 3: 07/13/2016 04:47:03: Final Results: Minibatch[1-26]: ce = 1.81846900 * 102399; perplexity = 6.16241658
MPI Rank 3: 07/13/2016 04:47:03: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846900 * 102399
MPI Rank 3: 07/13/2016 04:47:11: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:47:11: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 07/13/2016 04:47:11: __COMPLETED__
MPI Rank 3: ~MPIWrapper