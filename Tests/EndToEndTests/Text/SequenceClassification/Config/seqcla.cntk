# Copyright (c) Microsoft. All rights reserved.
# Licensed under the MIT license. See LICENSE file in the project root for full license information.

RootDir = ".."

ConfigDir = "$RootDir$/Config"
DataDir   = "$RootDir$/Data"
OutputDir = "$RootDir$/Output"
ModelDir  = "$OutputDir$/Models"

command=Train #:Write
deviceId = $DeviceId$
modelPath="$ModelDir$/seqcla.dnn"
makeMode = false # set true to enable checkpointing

vocabDim = 2000

Train=[
    action="train"
    
    BrainScriptNetworkBuilder=[
        Layers = [
            EmbeddingLayer(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = GatherPacked(features, embedding)
            ].lookup
            EmbeddingLayerSparse(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = embedding * features
            ].lookup
            DenseLayer(input, inputSize, outputSize, activation) = [
               z = BFF(input, outputSize, inputSize).z
               act = activation(z)
            ].act
            LSTMLayer (input, inputSize, outputSize, cellSize, selector) = [ 
               lstm = BS.RNNs.RecurrentLSTMP (outputSize, cellDim=cellSize, input, inputDim=inputSize).h
               result = selector(lstm)
            ].result
        ]        
        
        // LSTM params
        lstmDim = 25
        cellDim = 25

        // model dims
        numLabels = 5
        vocabDim = $vocabDim$
        embedDim = 50

        # definition without layer composition
        model_macroStyle (features) = {
            // load the pre-learned word embedding matrix
            #l1 = Layers.EmbeddingLayer(features, vocabDim, embedDim, 'embeddingmatrix.txt')
            l1 = Layers.EmbeddingLayerSparse(features, vocabDim, embedDim, 'embeddingmatrix.txt')
            l2 = Layers.LSTMLayer(l1, embedDim, lstmDim, cellDim, BS.Sequences.Last)
            l3 = Layers.DenseLayer(l2, lstmDim, numLabels, Pass)
            z = l3
        }.z

        # definition with layer composition
        model = Sequential
        (
            EmbeddingLayer {embedDim, embeddingPath='embeddingmatrix.txt', transpose=true} :  # load the pre-learned word embedding matrix
            RecurrentLSTMLayer {lstmDim, cellDim=cellDim} :
            BS.Sequences.Last :
            DenseLayer {numLabels}
        )

        # inputs
        t = DynamicAxis{}
        features = SparseInput {$vocabDim$, dynamicAxis=t}   # Input has shape (1,t)
        labels   = Input {numLabels}          # Input has shape (numLabels,*) where all sequences in *=1

        # apply model
        #z = model_macroStyle (features)
        z = model (features)

        # Make sure the trainer understands that the time dimension of l3 is actually the same as that of labels.
        zp = ReconcileDynamicAxis(z, labels)

        # training criteria
        ce  = CrossEntropyWithSoftmax (labels, zp)  // this is the training objective
        err = ErrorPrediction         (labels, zp)  // this also gets tracked

        # connect to system
        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (err)
        outputNodes     = (z)
    ]

    SGD = [	
        epochSize = 0
        minibatchSize = 200
        maxEpochs = 5
        momentumPerMB = 0.9
        learningRatesPerMB = 0.1
        # We are testing checkpointing, keep all checkpoint (.ckp) files
        keepCheckPointFiles = true
    ]

    reader = [
        readerType = "CNTKTextFormatReader"
        #file = "$DataDir$/Train.txt"
        file = "$DataDir$/Train.ctf"
        input = [
            features = [ alias = "x" ; dim = $vocabDim$ ; format = "sparse" ]
            labels =   [ alias = "y" ; dim = 5          ; format = "dense" ]
        ]
   ]    
   outputPath = "$OutputDir$/output.txt"        # dump the output as text?
]

# this is currently not used
Write=[
    action="test"   # TODO: test vs. Write?

    modelFile = "$ModelDir$/seqcla.dnn"    

    format = [
      # %n = minibatch, %x = shape, %d = sequenceId
      sequencePrologue=%d\t|w.shape %x\n%d\t|w\s
      sampleSeparator=\n%d\t|w\s
      elementSeparator=\s
    ]

    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/Train.txt"            
        input = [
            features = [ alias = "x" ; dim = 1 ; format = "dense" ]
            labels   = [ alias = "y" ; dim = 5 ; format = "dense" ]
        ]
   ]    
   outputPath = "$OutputDir$/output.txt"        # dump the output as text?
]
