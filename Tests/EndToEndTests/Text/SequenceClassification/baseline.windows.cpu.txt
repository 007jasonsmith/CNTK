=== Running /cygdrive/c/Users/clemensm/Source/CNTK/x64/debug/cntk.exe configFile=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/seqcla.cntk currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu DeviceId=-1 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Apr 13 2016 17:05:46
		Last modified date: Wed Apr  6 16:26:51 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\Users\clemensm\Source\cub-1.4.1
		CUDNN_PATH: C:\nvidia\cuda
		Built by clemensm on DEV-CLEMENSM5
		Build Path: C:\Users\clemensm\Source\CNTK\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
04/14/2016 15:18:28: -------------------------------------------------------------------
04/14/2016 15:18:28: Build info: 

04/14/2016 15:18:28: 		Built time: Apr 13 2016 17:05:46
04/14/2016 15:18:28: 		Last modified date: Wed Apr  6 16:26:51 2016
04/14/2016 15:18:28: 		Build type: Debug
04/14/2016 15:18:28: 		Build target: GPU
04/14/2016 15:18:28: 		With 1bit-SGD: no
04/14/2016 15:18:28: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
04/14/2016 15:18:28: 		CUB_PATH: C:\Users\clemensm\Source\cub-1.4.1
04/14/2016 15:18:28: 		CUDNN_PATH: C:\nvidia\cuda
04/14/2016 15:18:28: 		Built by clemensm on DEV-CLEMENSM5
04/14/2016 15:18:28: 		Build Path: C:\Users\clemensm\Source\CNTK\Source\CNTK\
04/14/2016 15:18:28: -------------------------------------------------------------------

04/14/2016 15:18:28: Running on DEV-CLEMENSM5 at 2016/04/14 15:18:28
04/14/2016 15:18:28: Command line: 
C:\Users\clemensm\Source\CNTK\x64\debug\cntk.exe  configFile=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/seqcla.cntk  currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification  RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu  DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification  ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification  OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu  DeviceId=-1  timestamping=true



04/14/2016 15:18:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
04/14/2016 15:18:28: command=Train 
deviceId=-1
modelPath="$RunDir$/model"
Train=[
    action="train"
    run=BrainScriptNetworkBuilder
    BrainScriptNetworkBuilder=[
        Macros = [
            // define "last hidden state of sequence" in the LSTM (really for any sequence though)
            TakeRight (N, x) = BS.Sequences._Take(FutureValue, N, x)
            Last(x) = TakeRight(1, x)
        ]
        Layers = [
            EmbeddingLayer(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = GatherPacked(features, embedding)
            ].lookup
            DenseLayer(input, inputSize, outputSize, activation) = [
               z = BFF(input, outputSize, inputSize).z
               act = activation(z)
            ].act
            LSTMLayer(input, inputSize, outputSize, cellSize, selector) = [ 
               lstm = BS.RNNs.RecurrentLSTMP(inputSize, outputSize, cellSize, input)
               result = selector(lstm)
            ].result
        ]        
        // LSTM params
        lstmDim = 25
        cellDim = 25
        // model
        numLabels = 5        
        vocab = 2000
        embedDim = 50        
        // set up features and labels
        t = DynamicAxis()
features = Input(1, dynamicAxis=t)   
labels   = Input(numLabels)          
        // load the pre-learned word embedding matrix
        l1 = Layers.EmbeddingLayer(features, vocab, embedDim, 'embeddingmatrix.txt')
        l2 = Layers.LSTMLayer(l1, embedDim, lstmDim, cellDim, Macros.Last)
        l3 = Layers.DenseLayer(l2, lstmDim, numLabels, Pass)
        out = Pass(l3, tag='output')   
        // Make sure the trainer understands that the time dimension of l3 is actually the same as that of labels.
        l3p = ReconcileDynamicAxis(l3, labels)
        // training criteria
        ce  = CrossEntropyWithSoftmax(labels, l3p, tag='criterion')   // this is the training objective
        wer = ErrorPrediction        (labels, l3p, tag='evaluation')  // this also gets tracked
    ]
    SGD = [	
      epochSize = 0
      minibatchSize = 200
      maxEpochs = 5
      momentumPerMB = 0.9
      learningRatesPerMB = 0.1
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "Train.txt"            
        input = [            
            features=[
                alias = "x"                
                dim = 1               
                format = "dense"
            ]
            labels=[
                alias = "y"                
                dim = 5           
                format = "dense"
            ]
        ]
   ]    
outputPath = "output"        
]
Write=[
    action="test"
    run=BrainScriptNetworkBuilder
    format = [
      sequencePrologue=%d\t|w.shape %x\n%d\t|w\s
      sampleSeparator=\n%d\t|w\s
      elementSeparator=\s
    ]
    modelFile = "model"    
    reader = [
            readerType = "CNTKTextFormatReader"
            file = "$DataDir$/Train.txt"            
            input = [            
                features=[
                    alias = "x"                
                    dim = 1               
                    format = "dense"
                ]
                labels=[
                    alias = "y"                
                    dim = 5           
                    format = "dense"
                ]
            ]
   ]    
outputPath = "output"        
]
currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DeviceId=-1
timestamping=true

04/14/2016 15:18:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

04/14/2016 15:18:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
04/14/2016 15:18:28: command=Train 
deviceId=-1
modelPath="C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model"
Train=[
    action="train"
    run=BrainScriptNetworkBuilder
    BrainScriptNetworkBuilder=[
        Macros = [
            // define "last hidden state of sequence" in the LSTM (really for any sequence though)
            TakeRight (N, x) = BS.Sequences._Take(FutureValue, N, x)
            Last(x) = TakeRight(1, x)
        ]
        Layers = [
            EmbeddingLayer(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = GatherPacked(features, embedding)
            ].lookup
            DenseLayer(input, inputSize, outputSize, activation) = [
               z = BFF(input, outputSize, inputSize).z
               act = activation(z)
            ].act
            LSTMLayer(input, inputSize, outputSize, cellSize, selector) = [ 
               lstm = BS.RNNs.RecurrentLSTMP(inputSize, outputSize, cellSize, input)
               result = selector(lstm)
            ].result
        ]        
        // LSTM params
        lstmDim = 25
        cellDim = 25
        // model
        numLabels = 5        
        vocab = 2000
        embedDim = 50        
        // set up features and labels
        t = DynamicAxis()
features = Input(1, dynamicAxis=t)   
labels   = Input(numLabels)          
        // load the pre-learned word embedding matrix
        l1 = Layers.EmbeddingLayer(features, vocab, embedDim, 'embeddingmatrix.txt')
        l2 = Layers.LSTMLayer(l1, embedDim, lstmDim, cellDim, Macros.Last)
        l3 = Layers.DenseLayer(l2, lstmDim, numLabels, Pass)
        out = Pass(l3, tag='output')   
        // Make sure the trainer understands that the time dimension of l3 is actually the same as that of labels.
        l3p = ReconcileDynamicAxis(l3, labels)
        // training criteria
        ce  = CrossEntropyWithSoftmax(labels, l3p, tag='criterion')   // this is the training objective
        wer = ErrorPrediction        (labels, l3p, tag='evaluation')  // this also gets tracked
    ]
    SGD = [	
      epochSize = 0
      minibatchSize = 200
      maxEpochs = 5
      momentumPerMB = 0.9
      learningRatesPerMB = 0.1
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "Train.txt"            
        input = [            
            features=[
                alias = "x"                
                dim = 1               
                format = "dense"
            ]
            labels=[
                alias = "y"                
                dim = 5           
                format = "dense"
            ]
        ]
   ]    
outputPath = "output"        
]
Write=[
    action="test"
    run=BrainScriptNetworkBuilder
    format = [
      sequencePrologue=%d\t|w.shape %x\n%d\t|w\s
      sampleSeparator=\n%d\t|w\s
      elementSeparator=\s
    ]
    modelFile = "model"    
    reader = [
            readerType = "CNTKTextFormatReader"
            file = "C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/Train.txt"            
            input = [            
                features=[
                    alias = "x"                
                    dim = 1               
                    format = "dense"
                ]
                labels=[
                    alias = "y"                
                    dim = 5           
                    format = "dense"
                ]
            ]
   ]    
outputPath = "output"        
]
currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DeviceId=-1
timestamping=true

04/14/2016 15:18:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

04/14/2016 15:18:28: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: seqcla.cntk:command=Train
configparameters: seqcla.cntk:ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
configparameters: seqcla.cntk:currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
configparameters: seqcla.cntk:DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
configparameters: seqcla.cntk:deviceId=-1
configparameters: seqcla.cntk:modelPath=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model
configparameters: seqcla.cntk:OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
configparameters: seqcla.cntk:RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
configparameters: seqcla.cntk:timestamping=true
configparameters: seqcla.cntk:Train=[
    action="train"
    run=BrainScriptNetworkBuilder
    BrainScriptNetworkBuilder=[
        Macros = [
            // define "last hidden state of sequence" in the LSTM (really for any sequence though)
            TakeRight (N, x) = BS.Sequences._Take(FutureValue, N, x)
            Last(x) = TakeRight(1, x)
        ]
        Layers = [
            EmbeddingLayer(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = GatherPacked(features, embedding)
            ].lookup
            DenseLayer(input, inputSize, outputSize, activation) = [
               z = BFF(input, outputSize, inputSize).z
               act = activation(z)
            ].act
            LSTMLayer(input, inputSize, outputSize, cellSize, selector) = [ 
               lstm = BS.RNNs.RecurrentLSTMP(inputSize, outputSize, cellSize, input)
               result = selector(lstm)
            ].result
        ]        
        // LSTM params
        lstmDim = 25
        cellDim = 25
        // model
        numLabels = 5        
        vocab = 2000
        embedDim = 50        
        // set up features and labels
        t = DynamicAxis()
features = Input(1, dynamicAxis=t)   
labels   = Input(numLabels)          
        // load the pre-learned word embedding matrix
        l1 = Layers.EmbeddingLayer(features, vocab, embedDim, 'embeddingmatrix.txt')
        l2 = Layers.LSTMLayer(l1, embedDim, lstmDim, cellDim, Macros.Last)
        l3 = Layers.DenseLayer(l2, lstmDim, numLabels, Pass)
        out = Pass(l3, tag='output')   
        // Make sure the trainer understands that the time dimension of l3 is actually the same as that of labels.
        l3p = ReconcileDynamicAxis(l3, labels)
        // training criteria
        ce  = CrossEntropyWithSoftmax(labels, l3p, tag='criterion')   // this is the training objective
        wer = ErrorPrediction        (labels, l3p, tag='evaluation')  // this also gets tracked
    ]
    SGD = [	
      epochSize = 0
      minibatchSize = 200
      maxEpochs = 5
      momentumPerMB = 0.9
      learningRatesPerMB = 0.1
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "Train.txt"            
        input = [            
            features=[
                alias = "x"                
                dim = 1               
                format = "dense"
            ]
            labels=[
                alias = "y"                
                dim = 5           
                format = "dense"
            ]
        ]
   ]    
outputPath = "output"        
]

configparameters: seqcla.cntk:Write=[
    action="test"
    run=BrainScriptNetworkBuilder
    format = [
      sequencePrologue=%d\t|w.shape %x\n%d\t|w\s
      sampleSeparator=\n%d\t|w\s
      elementSeparator=\s
    ]
    modelFile = "model"    
    reader = [
            readerType = "CNTKTextFormatReader"
            file = "C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/Train.txt"            
            input = [            
                features=[
                    alias = "x"                
                    dim = 1               
                    format = "dense"
                ]
                labels=[
                    alias = "y"                
                    dim = 5           
                    format = "dense"
                ]
            ]
   ]    
outputPath = "output"        
]

04/14/2016 15:18:28: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
04/14/2016 15:18:28: Commands: Train
04/14/2016 15:18:28: Precision = "float"
04/14/2016 15:18:28: CNTKModelPath: C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model
04/14/2016 15:18:28: CNTKCommandTrainInfo: Train : 5
04/14/2016 15:18:28: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 5

04/14/2016 15:18:28: ##############################################################################
04/14/2016 15:18:28: #                                                                            #
04/14/2016 15:18:28: # Action "train"                                                             #
04/14/2016 15:18:28: #                                                                            #
04/14/2016 15:18:28: ##############################################################################

04/14/2016 15:18:28: CNTKCommandTrainBegin: Train

04/14/2016 15:18:28: Creating virgin network.

Post-processing network...

4 roots:
	ce = CrossEntropyWithSoftmax()
	out = Pass()
	t = DynamicAxis()
	wer = ErrorPrediction()

Loop[0] --> Loop_l2.lstm.lstmState._privateInnards.ht -> 26 nodes

	l2.lstm.prevState.h	l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[1]	l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0]
	l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[1]	l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0]	l2.lstm.prevState.c
	l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[1]	l2.lstm.lstmState._privateInnards.ft.z	l2.lstm.lstmState._privateInnards.ft
	l2.lstm.lstmState._privateInnards.bft	l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[1]	l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0]
	l2.lstm.lstmState._privateInnards.it.z.PlusArgs[1]	l2.lstm.lstmState._privateInnards.it.z	l2.lstm.lstmState._privateInnards.it
	l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1].PlusArgs[0]	l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1]	l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z
	l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1]	l2.lstm.lstmState._privateInnards.bit	l2.lstm.lstmState._privateInnards.ct
	l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[1]	l2.lstm.lstmState._privateInnards.ot.z	l2.lstm.lstmState._privateInnards.ot
	l2.lstm.lstmState._privateInnards.ht.ElementTimesArgs[1]	l2.lstm.lstmState._privateInnards.ht

Validating network. 70 nodes to process in pass 1.


Validating network. 48 nodes to process in pass 2.


Validating network. 14 nodes to process in pass 3.


Validating network, final pass.

Validating --> labels = InputValue() :  -> [5 x *]
Validating --> l3.z.W = LearnableParameter() :  -> [5 x 25]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0].PlusArgs[0].TimesArgs[0] = LearnableParameter() :  -> [25 x 50]
Validating --> features = InputValue() :  -> [1 x t]
Validating --> l1.embedding.x = LearnableParameter() :  -> [2000 x 50]
Validating --> l1.embedding = TransposeDimensions (l1.embedding.x) : [2000 x 50] -> [50 x 2000]
Validating --> l1.lookup = GatherPacked (features, l1.embedding) : [1 x t], [50 x 2000] -> [50 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0].PlusArgs[0] = Times (l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0].PlusArgs[0].TimesArgs[0], l1.lookup) : [25 x 50], [50 x t] -> [25 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0].PlusArgs[1] = LearnableParameter() :  -> [25]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0] = Plus (l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0].PlusArgs[0], l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0].PlusArgs[1]) : [25 x t], [25] -> [25 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[1].TimesArgs[0] = LearnableParameter() :  -> [25 x 25]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[1].ElementTimesArgs[0] = LearnableParameter() :  -> [25 x 1]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0].PlusArgs[0].TimesArgs[0] = LearnableParameter() :  -> [25 x 50]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0].PlusArgs[0] = Times (l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0].PlusArgs[0].TimesArgs[0], l1.lookup) : [25 x 50], [50 x t] -> [25 x t]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0].PlusArgs[1] = LearnableParameter() :  -> [25]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0] = Plus (l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0].PlusArgs[0], l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0].PlusArgs[1]) : [25 x t], [25] -> [25 x t]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[1].TimesArgs[0] = LearnableParameter() :  -> [25 x 25]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[1].ElementTimesArgs[0] = LearnableParameter() :  -> [25 x 1]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0].PlusArgs[0].TimesArgs[0] = LearnableParameter() :  -> [25 x 50]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0].PlusArgs[0] = Times (l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0].PlusArgs[0].TimesArgs[0], l1.lookup) : [25 x 50], [50 x t] -> [25 x t]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0].PlusArgs[1] = LearnableParameter() :  -> [25]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0] = Plus (l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0].PlusArgs[0], l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0].PlusArgs[1]) : [25 x t], [25] -> [25 x t]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[1].TimesArgs[0] = LearnableParameter() :  -> [25 x 25]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[1].ElementTimesArgs[0] = LearnableParameter() :  -> [25 x 1]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[0].TimesArgs[0] = LearnableParameter() :  -> [25 x 50]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[0] = Times (l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[0].TimesArgs[0], l1.lookup) : [25 x 50], [50 x t] -> [25 x t]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1].PlusArgs[0].TimesArgs[0] = LearnableParameter() :  -> [25 x 25]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1].PlusArgs[1] = LearnableParameter() :  -> [25]
Validating --> l2.lstm.prevState.h = PastValue (l2.lstm.lstmState._privateInnards.ht) : [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[1] = Times (l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[1].TimesArgs[0], l2.lstm.prevState.h) : [25 x 25], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0] = Plus (l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[0], l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0].PlusArgs[1]) : [25 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[1] = Times (l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[1].TimesArgs[0], l2.lstm.prevState.h) : [25 x 25], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0] = Plus (l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[0], l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0].PlusArgs[1]) : [25 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.prevState.c = PastValue (l2.lstm.lstmState._privateInnards.ct) : [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[1] = ElementTimes (l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[1].ElementTimesArgs[0], l2.lstm.prevState.c) : [25 x 1], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ft.z = Plus (l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[0], l2.lstm.lstmState._privateInnards.ft.z.PlusArgs[1]) : [25 x 1 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ft = Sigmoid (l2.lstm.lstmState._privateInnards.ft.z) : [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.bft = ElementTimes (l2.lstm.lstmState._privateInnards.ft, l2.lstm.prevState.c) : [25 x 1 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[1] = Times (l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[1].TimesArgs[0], l2.lstm.prevState.h) : [25 x 25], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0] = Plus (l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[0], l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0].PlusArgs[1]) : [25 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.it.z.PlusArgs[1] = ElementTimes (l2.lstm.lstmState._privateInnards.it.z.PlusArgs[1].ElementTimesArgs[0], l2.lstm.prevState.c) : [25 x 1], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.it.z = Plus (l2.lstm.lstmState._privateInnards.it.z.PlusArgs[0], l2.lstm.lstmState._privateInnards.it.z.PlusArgs[1]) : [25 x 1 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.it = Sigmoid (l2.lstm.lstmState._privateInnards.it.z) : [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1].PlusArgs[0] = Times (l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1].PlusArgs[0].TimesArgs[0], l2.lstm.prevState.h) : [25 x 25], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1] = Plus (l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1].PlusArgs[0], l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1].PlusArgs[1]) : [25 x 1 x t], [25] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z = Plus (l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[0], l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z.PlusArgs[1]) : [25 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1] = Tanh (l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1].z) : [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.bit = ElementTimes (l2.lstm.lstmState._privateInnards.it, l2.lstm.lstmState._privateInnards.bit.ElementTimesArgs[1]) : [25 x 1 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ct = Plus (l2.lstm.lstmState._privateInnards.bft, l2.lstm.lstmState._privateInnards.bit) : [25 x 1 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[1] = ElementTimes (l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[1].ElementTimesArgs[0], l2.lstm.lstmState._privateInnards.ct) : [25 x 1], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot.z = Plus (l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[0], l2.lstm.lstmState._privateInnards.ot.z.PlusArgs[1]) : [25 x 1 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ot = Sigmoid (l2.lstm.lstmState._privateInnards.ot.z) : [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ht.ElementTimesArgs[1] = Tanh (l2.lstm.lstmState._privateInnards.ct) : [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.lstm.lstmState._privateInnards.ht = ElementTimes (l2.lstm.lstmState._privateInnards.ot, l2.lstm.lstmState._privateInnards.ht.ElementTimesArgs[1]) : [25 x 1 x t], [25 x 1 x t] -> [25 x 1 x t]
Validating --> l2.result.selected.input.ElementTimesArgs[0] = Slice (l2.lstm.lstmState._privateInnards.ht) : [25 x 1 x t] -> [1 x 1 x t]
Validating --> BS.Constants.Zero = LearnableParameter() :  -> [1]
Validating --> l2.result.selected.input = ElementTimes (l2.result.selected.input.ElementTimesArgs[0], BS.Constants.Zero) : [1 x 1 x t], [1] -> [1 x 1 x t]
Validating --> l2.result.selected = FutureValue (l2.result.selected.input) : [1 x 1 x t] -> [1 x 1 x t]
Validating --> l2.result.out.indexSequence.indexSequence = Where (l2.result.selected) : [1 x 1 x t] -> [1 x WhereNodeAxis]
Validating --> l2.result.out.indexSequence = PackedIndex (l2.lstm.lstmState._privateInnards.ht, l2.result.out.indexSequence.indexSequence) : [25 x 1 x t], [1 x WhereNodeAxis] -> [1 x WhereNodeAxis]
Validating --> l2.result.out = GatherPacked (l2.result.out.indexSequence, l2.lstm.lstmState._privateInnards.ht) : [1 x WhereNodeAxis], [25 x 1 x t] -> [25 x 1 x WhereNodeAxis]
Validating --> l3.z.z.PlusArgs[0] = Times (l3.z.W, l2.result.out) : [5 x 25], [25 x 1 x WhereNodeAxis] -> [5 x 1 x WhereNodeAxis]
Validating --> l3.z.B = LearnableParameter() :  -> [5 x 1]
Validating --> l3.z.z = Plus (l3.z.z.PlusArgs[0], l3.z.B) : [5 x 1 x WhereNodeAxis], [5 x 1] -> [5 x 1 x WhereNodeAxis]
Validating --> l3.act = Pass (l3.z.z) : [5 x 1 x WhereNodeAxis] -> [5 x 1 x WhereNodeAxis]
Validating --> l3p = ReconcileDynamicAxis (l3.act, labels) : [5 x 1 x WhereNodeAxis], [5 x *] -> [5 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, l3p) : [5 x *], [5 x 1 x *] -> [1]
Validating --> out = Pass (l3.act) : [5 x 1 x WhereNodeAxis] -> [5 x 1 x WhereNodeAxis]
Validating --> t = DynamicAxis() :  -> [1 x 1 x t]
Validating --> wer = ErrorPrediction (labels, l3p) : [5 x *], [5 x 1 x *] -> [1]


68 out of 70 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

04/14/2016 15:18:31: Created model with 70 nodes on CPU.

04/14/2016 15:18:31: Training criterion node(s):
04/14/2016 15:18:31: 	ce = CrossEntropyWithSoftmax

04/14/2016 15:18:31: Evaluation criterion node(s):

04/14/2016 15:18:31: 	wer = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
04/14/2016 15:18:31: No PreCompute nodes found, skipping PreCompute step.

04/14/2016 15:18:31: Starting Epoch 1: learning rate per sample = 0.000500  effective momentum = 0.900000  momentum as time constant = 1898.2 samples

04/14/2016 15:18:31: Starting minibatch loop.
04/14/2016 15:18:38: Finished Epoch[ 1 of 5]: [Training Set] TrainLossPerSample = 1.5826238; TotalSamplesSeen = 1247; EvalErrPerSample = 0.48596632; AvgLearningRatePerSample = 0.00050000002; EpochTime=7.73595
04/14/2016 15:18:38: SGD: Saving checkpoint model 'C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model.1'

04/14/2016 15:18:38: Starting Epoch 2: learning rate per sample = 0.000500  effective momentum = 0.900000  momentum as time constant = 1898.2 samples

04/14/2016 15:18:38: Starting minibatch loop.
04/14/2016 15:18:45: Finished Epoch[ 2 of 5]: [Training Set] TrainLossPerSample = 1.497769; TotalSamplesSeen = 2494; EvalErrPerSample = 0.44667202; AvgLearningRatePerSample = 0.00050000002; EpochTime=6.52109
04/14/2016 15:18:45: SGD: Saving checkpoint model 'C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model.2'

04/14/2016 15:18:45: Starting Epoch 3: learning rate per sample = 0.000500  effective momentum = 0.900000  momentum as time constant = 1898.2 samples

04/14/2016 15:18:45: Starting minibatch loop.
04/14/2016 15:18:51: Finished Epoch[ 3 of 5]: [Training Set] TrainLossPerSample = 1.4248083; TotalSamplesSeen = 3741; EvalErrPerSample = 0.44667202; AvgLearningRatePerSample = 0.00050000002; EpochTime=6.27497
04/14/2016 15:18:51: SGD: Saving checkpoint model 'C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model.3'

04/14/2016 15:18:51: Starting Epoch 4: learning rate per sample = 0.000500  effective momentum = 0.900000  momentum as time constant = 1898.2 samples

04/14/2016 15:18:51: Starting minibatch loop.
04/14/2016 15:18:58: Finished Epoch[ 4 of 5]: [Training Set] TrainLossPerSample = 1.3717686; TotalSamplesSeen = 4988; EvalErrPerSample = 0.44667202; AvgLearningRatePerSample = 0.00050000002; EpochTime=6.68203
04/14/2016 15:18:58: SGD: Saving checkpoint model 'C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model.4'

04/14/2016 15:18:58: Starting Epoch 5: learning rate per sample = 0.000500  effective momentum = 0.900000  momentum as time constant = 1898.2 samples

04/14/2016 15:18:58: Starting minibatch loop.
04/14/2016 15:19:06: Finished Epoch[ 5 of 5]: [Training Set] TrainLossPerSample = 1.3331786; TotalSamplesSeen = 6235; EvalErrPerSample = 0.44667202; AvgLearningRatePerSample = 0.00050000002; EpochTime=8.43402
04/14/2016 15:19:06: SGD: Saving checkpoint model 'C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model'
04/14/2016 15:19:06: CNTKCommandTrainEnd: Train

04/14/2016 15:19:06: Action "train" complete.

04/14/2016 15:19:06: __COMPLETED__
=== Deleting last epoch data
rm: cannot remove '/tmp/cntk-test-20160414161828.694305/Text_SequenceClassification@debug_cpu/models/*.dnn': No such file or directory
==== Re-running from checkpoint
=== Running /cygdrive/c/Users/clemensm/Source/CNTK/x64/debug/cntk.exe configFile=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/seqcla.cntk currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu DeviceId=-1 timestamping=true makeMode=true
-------------------------------------------------------------------
Build info: 

		Built time: Apr 13 2016 17:05:46
		Last modified date: Wed Apr  6 16:26:51 2016
		Build type: Debug
		Build target: GPU
		With 1bit-SGD: no
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\Users\clemensm\Source\cub-1.4.1
		CUDNN_PATH: C:\nvidia\cuda
		Built by clemensm on DEV-CLEMENSM5
		Build Path: C:\Users\clemensm\Source\CNTK\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
04/14/2016 15:19:07: -------------------------------------------------------------------
04/14/2016 15:19:07: Build info: 

04/14/2016 15:19:07: 		Built time: Apr 13 2016 17:05:46
04/14/2016 15:19:07: 		Last modified date: Wed Apr  6 16:26:51 2016
04/14/2016 15:19:07: 		Build type: Debug
04/14/2016 15:19:07: 		Build target: GPU
04/14/2016 15:19:07: 		With 1bit-SGD: no
04/14/2016 15:19:07: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
04/14/2016 15:19:07: 		CUB_PATH: C:\Users\clemensm\Source\cub-1.4.1
04/14/2016 15:19:07: 		CUDNN_PATH: C:\nvidia\cuda
04/14/2016 15:19:07: 		Built by clemensm on DEV-CLEMENSM5
04/14/2016 15:19:07: 		Build Path: C:\Users\clemensm\Source\CNTK\Source\CNTK\
04/14/2016 15:19:07: -------------------------------------------------------------------

04/14/2016 15:19:07: Running on DEV-CLEMENSM5 at 2016/04/14 15:19:07
04/14/2016 15:19:07: Command line: 
C:\Users\clemensm\Source\CNTK\x64\debug\cntk.exe  configFile=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/seqcla.cntk  currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification  RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu  DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification  ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification  OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu  DeviceId=-1  timestamping=true  makeMode=true



04/14/2016 15:19:07: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
04/14/2016 15:19:07: command=Train 
deviceId=-1
modelPath="$RunDir$/model"
Train=[
    action="train"
    run=BrainScriptNetworkBuilder
    BrainScriptNetworkBuilder=[
        Macros = [
            // define "last hidden state of sequence" in the LSTM (really for any sequence though)
            TakeRight (N, x) = BS.Sequences._Take(FutureValue, N, x)
            Last(x) = TakeRight(1, x)
        ]
        Layers = [
            EmbeddingLayer(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = GatherPacked(features, embedding)
            ].lookup
            DenseLayer(input, inputSize, outputSize, activation) = [
               z = BFF(input, outputSize, inputSize).z
               act = activation(z)
            ].act
            LSTMLayer(input, inputSize, outputSize, cellSize, selector) = [ 
               lstm = BS.RNNs.RecurrentLSTMP(inputSize, outputSize, cellSize, input)
               result = selector(lstm)
            ].result
        ]        
        // LSTM params
        lstmDim = 25
        cellDim = 25
        // model
        numLabels = 5        
        vocab = 2000
        embedDim = 50        
        // set up features and labels
        t = DynamicAxis()
features = Input(1, dynamicAxis=t)   
labels   = Input(numLabels)          
        // load the pre-learned word embedding matrix
        l1 = Layers.EmbeddingLayer(features, vocab, embedDim, 'embeddingmatrix.txt')
        l2 = Layers.LSTMLayer(l1, embedDim, lstmDim, cellDim, Macros.Last)
        l3 = Layers.DenseLayer(l2, lstmDim, numLabels, Pass)
        out = Pass(l3, tag='output')   
        // Make sure the trainer understands that the time dimension of l3 is actually the same as that of labels.
        l3p = ReconcileDynamicAxis(l3, labels)
        // training criteria
        ce  = CrossEntropyWithSoftmax(labels, l3p, tag='criterion')   // this is the training objective
        wer = ErrorPrediction        (labels, l3p, tag='evaluation')  // this also gets tracked
    ]
    SGD = [	
      epochSize = 0
      minibatchSize = 200
      maxEpochs = 5
      momentumPerMB = 0.9
      learningRatesPerMB = 0.1
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "Train.txt"            
        input = [            
            features=[
                alias = "x"                
                dim = 1               
                format = "dense"
            ]
            labels=[
                alias = "y"                
                dim = 5           
                format = "dense"
            ]
        ]
   ]    
outputPath = "output"        
]
Write=[
    action="test"
    run=BrainScriptNetworkBuilder
    format = [
      sequencePrologue=%d\t|w.shape %x\n%d\t|w\s
      sampleSeparator=\n%d\t|w\s
      elementSeparator=\s
    ]
    modelFile = "model"    
    reader = [
            readerType = "CNTKTextFormatReader"
            file = "$DataDir$/Train.txt"            
            input = [            
                features=[
                    alias = "x"                
                    dim = 1               
                    format = "dense"
                ]
                labels=[
                    alias = "y"                
                    dim = 5           
                    format = "dense"
                ]
            ]
   ]    
outputPath = "output"        
]
currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DeviceId=-1
timestamping=true
makeMode=true

04/14/2016 15:19:07: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

04/14/2016 15:19:07: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
04/14/2016 15:19:07: command=Train 
deviceId=-1
modelPath="C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model"
Train=[
    action="train"
    run=BrainScriptNetworkBuilder
    BrainScriptNetworkBuilder=[
        Macros = [
            // define "last hidden state of sequence" in the LSTM (really for any sequence though)
            TakeRight (N, x) = BS.Sequences._Take(FutureValue, N, x)
            Last(x) = TakeRight(1, x)
        ]
        Layers = [
            EmbeddingLayer(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = GatherPacked(features, embedding)
            ].lookup
            DenseLayer(input, inputSize, outputSize, activation) = [
               z = BFF(input, outputSize, inputSize).z
               act = activation(z)
            ].act
            LSTMLayer(input, inputSize, outputSize, cellSize, selector) = [ 
               lstm = BS.RNNs.RecurrentLSTMP(inputSize, outputSize, cellSize, input)
               result = selector(lstm)
            ].result
        ]        
        // LSTM params
        lstmDim = 25
        cellDim = 25
        // model
        numLabels = 5        
        vocab = 2000
        embedDim = 50        
        // set up features and labels
        t = DynamicAxis()
features = Input(1, dynamicAxis=t)   
labels   = Input(numLabels)          
        // load the pre-learned word embedding matrix
        l1 = Layers.EmbeddingLayer(features, vocab, embedDim, 'embeddingmatrix.txt')
        l2 = Layers.LSTMLayer(l1, embedDim, lstmDim, cellDim, Macros.Last)
        l3 = Layers.DenseLayer(l2, lstmDim, numLabels, Pass)
        out = Pass(l3, tag='output')   
        // Make sure the trainer understands that the time dimension of l3 is actually the same as that of labels.
        l3p = ReconcileDynamicAxis(l3, labels)
        // training criteria
        ce  = CrossEntropyWithSoftmax(labels, l3p, tag='criterion')   // this is the training objective
        wer = ErrorPrediction        (labels, l3p, tag='evaluation')  // this also gets tracked
    ]
    SGD = [	
      epochSize = 0
      minibatchSize = 200
      maxEpochs = 5
      momentumPerMB = 0.9
      learningRatesPerMB = 0.1
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "Train.txt"            
        input = [            
            features=[
                alias = "x"                
                dim = 1               
                format = "dense"
            ]
            labels=[
                alias = "y"                
                dim = 5           
                format = "dense"
            ]
        ]
   ]    
outputPath = "output"        
]
Write=[
    action="test"
    run=BrainScriptNetworkBuilder
    format = [
      sequencePrologue=%d\t|w.shape %x\n%d\t|w\s
      sampleSeparator=\n%d\t|w\s
      elementSeparator=\s
    ]
    modelFile = "model"    
    reader = [
            readerType = "CNTKTextFormatReader"
            file = "C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/Train.txt"            
            input = [            
                features=[
                    alias = "x"                
                    dim = 1               
                    format = "dense"
                ]
                labels=[
                    alias = "y"                
                    dim = 5           
                    format = "dense"
                ]
            ]
   ]    
outputPath = "output"        
]
currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
DeviceId=-1
timestamping=true
makeMode=true

04/14/2016 15:19:07: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

04/14/2016 15:19:07: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: seqcla.cntk:command=Train
configparameters: seqcla.cntk:ConfigDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
configparameters: seqcla.cntk:currentDirectory=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
configparameters: seqcla.cntk:DataDir=C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification
configparameters: seqcla.cntk:deviceId=-1
configparameters: seqcla.cntk:makeMode=true
configparameters: seqcla.cntk:modelPath=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model
configparameters: seqcla.cntk:OutputDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
configparameters: seqcla.cntk:RunDir=C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu
configparameters: seqcla.cntk:timestamping=true
configparameters: seqcla.cntk:Train=[
    action="train"
    run=BrainScriptNetworkBuilder
    BrainScriptNetworkBuilder=[
        Macros = [
            // define "last hidden state of sequence" in the LSTM (really for any sequence though)
            TakeRight (N, x) = BS.Sequences._Take(FutureValue, N, x)
            Last(x) = TakeRight(1, x)
        ]
        Layers = [
            EmbeddingLayer(input, vocabSize, embeddingDim, embeddingPath) = [
                embedding = Transpose(LearnableParameter(vocabSize, embeddingDim, learningRateMultiplier = 0.0, init = 'fromFile', initFromFilePath = embeddingPath))          
                lookup = GatherPacked(features, embedding)
            ].lookup
            DenseLayer(input, inputSize, outputSize, activation) = [
               z = BFF(input, outputSize, inputSize).z
               act = activation(z)
            ].act
            LSTMLayer(input, inputSize, outputSize, cellSize, selector) = [ 
               lstm = BS.RNNs.RecurrentLSTMP(inputSize, outputSize, cellSize, input)
               result = selector(lstm)
            ].result
        ]        
        // LSTM params
        lstmDim = 25
        cellDim = 25
        // model
        numLabels = 5        
        vocab = 2000
        embedDim = 50        
        // set up features and labels
        t = DynamicAxis()
features = Input(1, dynamicAxis=t)   
labels   = Input(numLabels)          
        // load the pre-learned word embedding matrix
        l1 = Layers.EmbeddingLayer(features, vocab, embedDim, 'embeddingmatrix.txt')
        l2 = Layers.LSTMLayer(l1, embedDim, lstmDim, cellDim, Macros.Last)
        l3 = Layers.DenseLayer(l2, lstmDim, numLabels, Pass)
        out = Pass(l3, tag='output')   
        // Make sure the trainer understands that the time dimension of l3 is actually the same as that of labels.
        l3p = ReconcileDynamicAxis(l3, labels)
        // training criteria
        ce  = CrossEntropyWithSoftmax(labels, l3p, tag='criterion')   // this is the training objective
        wer = ErrorPrediction        (labels, l3p, tag='evaluation')  // this also gets tracked
    ]
    SGD = [	
      epochSize = 0
      minibatchSize = 200
      maxEpochs = 5
      momentumPerMB = 0.9
      learningRatesPerMB = 0.1
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "Train.txt"            
        input = [            
            features=[
                alias = "x"                
                dim = 1               
                format = "dense"
            ]
            labels=[
                alias = "y"                
                dim = 5           
                format = "dense"
            ]
        ]
   ]    
outputPath = "output"        
]

configparameters: seqcla.cntk:Write=[
    action="test"
    run=BrainScriptNetworkBuilder
    format = [
      sequencePrologue=%d\t|w.shape %x\n%d\t|w\s
      sampleSeparator=\n%d\t|w\s
      elementSeparator=\s
    ]
    modelFile = "model"    
    reader = [
            readerType = "CNTKTextFormatReader"
            file = "C:\Users\clemensm\Source\CNTK\Tests\EndToEndTests\Text\SequenceClassification/Train.txt"            
            input = [            
                features=[
                    alias = "x"                
                    dim = 1               
                    format = "dense"
                ]
                labels=[
                    alias = "y"                
                    dim = 5           
                    format = "dense"
                ]
            ]
   ]    
outputPath = "output"        
]

04/14/2016 15:19:07: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
04/14/2016 15:19:07: Commands: Train
04/14/2016 15:19:07: Precision = "float"
04/14/2016 15:19:07: CNTKModelPath: C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model
04/14/2016 15:19:07: CNTKCommandTrainInfo: Train : 5
04/14/2016 15:19:07: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 5

04/14/2016 15:19:07: ##############################################################################
04/14/2016 15:19:07: #                                                                            #
04/14/2016 15:19:07: # Action "train"                                                             #
04/14/2016 15:19:07: #                                                                            #
04/14/2016 15:19:07: ##############################################################################

04/14/2016 15:19:07: CNTKCommandTrainBegin: Train
04/14/2016 15:19:07: Final model exists: C:\cygwin64\tmp\cntk-test-20160414161828.694305\Text_SequenceClassification@debug_cpu/model
04/14/2016 15:19:07: No further training is necessary.
04/14/2016 15:19:07: CNTKCommandTrainEnd: Train

04/14/2016 15:19:07: Action "train" complete.

04/14/2016 15:19:07: __COMPLETED__