=== Running /home/mluser/src/cplx_master/build/debug/bin/cntk configFile=/home/mluser/src/cplx_master/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config RunDir=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu DataDir=/home/mluser/src/cplx_master/Tests/Speech/Data DeviceId=0
running on localhost at 2015/10/12 18:49:16
command line options: 
configFile=/home/mluser/src/cplx_master/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config RunDir=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu DataDir=/home/mluser/src/cplx_master/Tests/Speech/Data DeviceId=0 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision=float
deviceId=$DeviceId$
command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain
ndlMacros=$DataDir$/ndl/macros.txt
GlobalMean=GlobalStats/mean.363
GlobalInvStd=GlobalStats/var.363
GlobalPrior=GlobalStats/prior.132
traceLevel=1
SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]
DPT_Pre1=[
    action=train
    modelPath=$RunDir$/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=$DataDir$/ndl/dnn_1layer.txt
    ]
]
AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=$RunDir$/models/Pre1/cntkSpeech
    NewModel=$RunDir$/models/Pre2/cntkSpeech.0
    editPath=$DataDir$/ndl/add_layer.mel
]
DPT_Pre2=[
    action=train
    modelPath=$RunDir$/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=$DataDir$/ndl/dnn_1layer.txt
    ]
]
AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=$RunDir$/models/Pre2/cntkSpeech
    NewModel=$RunDir$/models/cntkSpeech.0
    editPath=$DataDir$/ndl/add_layer.mel
]
speechTrain=[
    action=train
    modelPath=$RunDir$/models/cntkSpeech
    deviceId=$DeviceId$
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=$DataDir$/ndl/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]
reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=$DataDir$/glob_0000.scp
  ]
  labels=[
      mlfFile=$DataDir$/glob_0000.mlf
      labelMappingFile=$DataDir$/state.list
      labelDim=132
      labelType=Category
  ]
]
RunDir=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu
DataDir=/home/mluser/src/cplx_master/Tests/Speech/Data
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision=float
deviceId=0
command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain
ndlMacros=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/macros.txt
GlobalMean=GlobalStats/mean.363
GlobalInvStd=GlobalStats/var.363
GlobalPrior=GlobalStats/prior.132
traceLevel=1
SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]
DPT_Pre1=[
    action=train
    modelPath=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/dnn_1layer.txt
    ]
]
AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
    NewModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/add_layer.mel
]
DPT_Pre2=[
    action=train
    modelPath=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/dnn_1layer.txt
    ]
]
AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
    NewModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/add_layer.mel
]
speechTrain=[
    action=train
    modelPath=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech
    deviceId=0
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]
reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=/home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.scp
  ]
  labels=[
      mlfFile=/home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.mlf
      labelMappingFile=/home/mluser/src/cplx_master/Tests/Speech/Data/state.list
      labelDim=132
      labelType=Category
  ]
]
RunDir=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu
DataDir=/home/mluser/src/cplx_master/Tests/Speech/Data
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.config:AddLayer2=[    
    action=edit
    CurrLayer=1
    NewLayer=2
    CurrModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
    NewModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/add_layer.mel
]

configparameters: cntk_dpt.config:AddLayer3=[    
    action=edit
    CurrLayer=2
    NewLayer=3
    CurrModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
    NewModel=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0
    editPath=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/add_layer.mel
]

configparameters: cntk_dpt.config:command=DPT_Pre1:AddLayer2:DPT_Pre2:AddLayer3:speechTrain
configparameters: cntk_dpt.config:DataDir=/home/mluser/src/cplx_master/Tests/Speech/Data
configparameters: cntk_dpt.config:deviceId=0
configparameters: cntk_dpt.config:DPT_Pre1=[
    action=train
    modelPath=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/dnn_1layer.txt
    ]
]

configparameters: cntk_dpt.config:DPT_Pre2=[
    action=train
    modelPath=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
    NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/dnn_1layer.txt
    ]
]

configparameters: cntk_dpt.config:GlobalInvStd=GlobalStats/var.363
configparameters: cntk_dpt.config:GlobalMean=GlobalStats/mean.363
configparameters: cntk_dpt.config:GlobalPrior=GlobalStats/prior.132
configparameters: cntk_dpt.config:ndlMacros=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/macros.txt
configparameters: cntk_dpt.config:precision=float
configparameters: cntk_dpt.config:reader=[
  readerType=HTKMLFReader
  readMethod=blockRandomize
  miniBatchMode=Partial
  randomize=Auto
  verbosity=0
  features=[
      dim=363
      type=Real
      scpFile=/home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.scp
  ]
  labels=[
      mlfFile=/home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.mlf
      labelMappingFile=/home/mluser/src/cplx_master/Tests/Speech/Data/state.list
      labelDim=132
      labelType=Category
  ]
]

configparameters: cntk_dpt.config:RunDir=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu
configparameters: cntk_dpt.config:SGD=[
    epochSize=81920
    minibatchSize=256
    learningRatesPerMB=0.8
    numMBsToShowResult=10
    momentumPerMB=0.9
    dropoutRate=0.0
    maxEpochs=2
]

configparameters: cntk_dpt.config:speechTrain=[
    action=train
    modelPath=/tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech
    deviceId=0
    traceLevel=1
     NDLNetworkBuilder=[
        networkDescription=/home/mluser/src/cplx_master/Tests/Speech/Data/ndl/dnn.txt
    ]
    SGD=[
        epochSize=81920
        minibatchSize=256:512
        learningRatesPerMB=0.8:1.6
        numMBsToShowResult=10
        momentumPerSample=0.999589
        dropoutRate=0.0
        maxEpochs=4
        gradUpdateType=None
        normWithAveMultiplier=true
        clippingThresholdPerSample=1#INF
    ]
]

configparameters: cntk_dpt.config:traceLevel=1
<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: DPT_Pre1 AddLayer2 DPT_Pre2 AddLayer3 speechTrain 
precision = float
CNTKModelPath: /tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre1/cntkSpeech
CNTKCommandTrainInfo: DPT_Pre1 : 2
CNTKModelPath: /tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech
CNTKCommandTrainInfo: DPT_Pre2 : 2
CNTKModelPath: /tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech
CNTKCommandTrainInfo: speechTrain : 4
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8
CNTKCommandTrainBegin: DPT_Pre1
NDLBuilder Using GPU 0
reading script file /home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /home/mluser/src/cplx_master/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 1], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 1], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 1] = InputValue
HL1.W[512, 363] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 1] = InputValue

Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood. 2 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 1], logPrior[132, 1]) -> [132, MBSize 1]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 1]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 1]) -> [512, MBSize 1]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 1]) -> [132, MBSize 1]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 1], OL.b[132, 1]) -> [132, MBSize 1]
Validating --> Err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 1]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
EnforceOneGPUOnly: WARNING: Ignored attempt to change GPU choice from 0 now 1. This message will be shown only once. 
 Epoch[ 1 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  3.74183846; EvalErr[0]PerSample = 0.80195313; TotalTime = 0.30483s; TotalTimePerSample = 0.11907ms; SamplesPerSecond = 8398
 Epoch[ 1 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.91124763; EvalErr[0]PerSample = 0.70898438; TotalTime = 0.12917s; TotalTimePerSample = 0.05046ms; SamplesPerSecond = 19818
 Epoch[ 1 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.58015976; EvalErr[0]PerSample = 0.66640625; TotalTime = 0.12870s; TotalTimePerSample = 0.05027ms; SamplesPerSecond = 19891
 Epoch[ 1 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.27427139; EvalErr[0]PerSample = 0.58750000; TotalTime = 0.12889s; TotalTimePerSample = 0.05035ms; SamplesPerSecond = 19861
 Epoch[ 1 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.05503616; EvalErr[0]PerSample = 0.56093750; TotalTime = 0.12856s; TotalTimePerSample = 0.05022ms; SamplesPerSecond = 19912
 Epoch[ 1 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.91055145; EvalErr[0]PerSample = 0.52812500; TotalTime = 0.12907s; TotalTimePerSample = 0.05042ms; SamplesPerSecond = 19833
 Epoch[ 1 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.81562653; EvalErr[0]PerSample = 0.51171875; TotalTime = 0.12874s; TotalTimePerSample = 0.05029ms; SamplesPerSecond = 19884
 Epoch[ 1 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.68803253; EvalErr[0]PerSample = 0.48476562; TotalTime = 0.12379s; TotalTimePerSample = 0.04836ms; SamplesPerSecond = 20680
 Epoch[ 1 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.57382050; EvalErr[0]PerSample = 0.45429687; TotalTime = 0.12941s; TotalTimePerSample = 0.05055ms; SamplesPerSecond = 19781
 Epoch[ 1 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.62090302; EvalErr[0]PerSample = 0.47304687; TotalTime = 0.12857s; TotalTimePerSample = 0.05022ms; SamplesPerSecond = 19911
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.59272614; EvalErr[0]PerSample = 0.47500000; TotalTime = 0.12941s; TotalTimePerSample = 0.05055ms; SamplesPerSecond = 19781
 Epoch[ 1 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.51520386; EvalErr[0]PerSample = 0.44531250; TotalTime = 0.12911s; TotalTimePerSample = 0.05043ms; SamplesPerSecond = 19828
 Epoch[ 1 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.49181976; EvalErr[0]PerSample = 0.45039062; TotalTime = 0.10931s; TotalTimePerSample = 0.04270ms; SamplesPerSecond = 23418
 Epoch[ 1 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.53703613; EvalErr[0]PerSample = 0.44804688; TotalTime = 0.09591s; TotalTimePerSample = 0.03746ms; SamplesPerSecond = 26691
 Epoch[ 1 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.43095093; EvalErr[0]PerSample = 0.41640625; TotalTime = 0.09606s; TotalTimePerSample = 0.03753ms; SamplesPerSecond = 26648
 Epoch[ 1 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41503601; EvalErr[0]PerSample = 0.40078125; TotalTime = 0.09662s; TotalTimePerSample = 0.03774ms; SamplesPerSecond = 26494
 Epoch[ 1 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.38912659; EvalErr[0]PerSample = 0.41132812; TotalTime = 0.09588s; TotalTimePerSample = 0.03745ms; SamplesPerSecond = 26700
 Epoch[ 1 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41208191; EvalErr[0]PerSample = 0.42226562; TotalTime = 0.09560s; TotalTimePerSample = 0.03734ms; SamplesPerSecond = 26779
 Epoch[ 1 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.39966125; EvalErr[0]PerSample = 0.40664062; TotalTime = 0.09558s; TotalTimePerSample = 0.03734ms; SamplesPerSecond = 26782
 Epoch[ 1 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.42728577; EvalErr[0]PerSample = 0.42617187; TotalTime = 0.09572s; TotalTimePerSample = 0.03739ms; SamplesPerSecond = 26745
 Epoch[ 1 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.41336365; EvalErr[0]PerSample = 0.42304687; TotalTime = 0.09572s; TotalTimePerSample = 0.03739ms; SamplesPerSecond = 26744
 Epoch[ 1 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33197937; EvalErr[0]PerSample = 0.39960937; TotalTime = 0.09589s; TotalTimePerSample = 0.03746ms; SamplesPerSecond = 26698
 Epoch[ 1 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28578796; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.09591s; TotalTimePerSample = 0.03747ms; SamplesPerSecond = 26691
 Epoch[ 1 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.34131775; EvalErr[0]PerSample = 0.40937500; TotalTime = 0.09552s; TotalTimePerSample = 0.03731ms; SamplesPerSecond = 26800
 Epoch[ 1 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.32666016; EvalErr[0]PerSample = 0.39648438; TotalTime = 0.09573s; TotalTimePerSample = 0.03740ms; SamplesPerSecond = 26741
 Epoch[ 1 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21426086; EvalErr[0]PerSample = 0.37226562; TotalTime = 0.09610s; TotalTimePerSample = 0.03754ms; SamplesPerSecond = 26638
 Epoch[ 1 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23750610; EvalErr[0]PerSample = 0.37382813; TotalTime = 0.10318s; TotalTimePerSample = 0.04031ms; SamplesPerSecond = 24810
 Epoch[ 1 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.29967957; EvalErr[0]PerSample = 0.39062500; TotalTime = 0.12995s; TotalTimePerSample = 0.05076ms; SamplesPerSecond = 19699
 Epoch[ 1 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21233215; EvalErr[0]PerSample = 0.37343750; TotalTime = 0.12914s; TotalTimePerSample = 0.05044ms; SamplesPerSecond = 19823
 Epoch[ 1 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20534973; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.12942s; TotalTimePerSample = 0.05056ms; SamplesPerSecond = 19779
 Epoch[ 1 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23558655; EvalErr[0]PerSample = 0.37187500; TotalTime = 0.12904s; TotalTimePerSample = 0.05041ms; SamplesPerSecond = 19838
 Epoch[ 1 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.25517273; EvalErr[0]PerSample = 0.37890625; TotalTime = 0.11876s; TotalTimePerSample = 0.04639ms; SamplesPerSecond = 21555
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.6294192; EvalErrPerSample = 0.46010742; Ave LearnRatePerSample = 0.003125000047; EpochTime=5.361751
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23276577; EvalErr[0]PerSample = 0.38125000; TotalTime = 0.13037s; TotalTimePerSample = 0.05093ms; SamplesPerSecond = 19635
 Epoch[ 2 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20353279; EvalErr[0]PerSample = 0.37265625; TotalTime = 0.12890s; TotalTimePerSample = 0.05035ms; SamplesPerSecond = 19860
 Epoch[ 2 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28632336; EvalErr[0]PerSample = 0.37734375; TotalTime = 0.12453s; TotalTimePerSample = 0.04864ms; SamplesPerSecond = 20557
 Epoch[ 2 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23058014; EvalErr[0]PerSample = 0.37812500; TotalTime = 0.09562s; TotalTimePerSample = 0.03735ms; SamplesPerSecond = 26772
 Epoch[ 2 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18196945; EvalErr[0]PerSample = 0.35429688; TotalTime = 0.09557s; TotalTimePerSample = 0.03733ms; SamplesPerSecond = 26785
 Epoch[ 2 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28158035; EvalErr[0]PerSample = 0.38007812; TotalTime = 0.09562s; TotalTimePerSample = 0.03735ms; SamplesPerSecond = 26773
 Epoch[ 2 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22469864; EvalErr[0]PerSample = 0.37265625; TotalTime = 0.09554s; TotalTimePerSample = 0.03732ms; SamplesPerSecond = 26795
 Epoch[ 2 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17930145; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.09564s; TotalTimePerSample = 0.03736ms; SamplesPerSecond = 26767
 Epoch[ 2 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23973160; EvalErr[0]PerSample = 0.36328125; TotalTime = 0.09546s; TotalTimePerSample = 0.03729ms; SamplesPerSecond = 26817
 Epoch[ 2 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18514709; EvalErr[0]PerSample = 0.37539062; TotalTime = 0.09566s; TotalTimePerSample = 0.03737ms; SamplesPerSecond = 26762
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20197525; EvalErr[0]PerSample = 0.36171875; TotalTime = 0.09590s; TotalTimePerSample = 0.03746ms; SamplesPerSecond = 26695
 Epoch[ 2 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.18739471; EvalErr[0]PerSample = 0.35312500; TotalTime = 0.09610s; TotalTimePerSample = 0.03754ms; SamplesPerSecond = 26637
 Epoch[ 2 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16798859; EvalErr[0]PerSample = 0.35742188; TotalTime = 0.09603s; TotalTimePerSample = 0.03751ms; SamplesPerSecond = 26657
 Epoch[ 2 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13375397; EvalErr[0]PerSample = 0.35273437; TotalTime = 0.09650s; TotalTimePerSample = 0.03769ms; SamplesPerSecond = 26529
 Epoch[ 2 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09628754; EvalErr[0]PerSample = 0.31992188; TotalTime = 0.09675s; TotalTimePerSample = 0.03779ms; SamplesPerSecond = 26459
 Epoch[ 2 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10226898; EvalErr[0]PerSample = 0.34218750; TotalTime = 0.09621s; TotalTimePerSample = 0.03758ms; SamplesPerSecond = 26608
 Epoch[ 2 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20214386; EvalErr[0]PerSample = 0.36015625; TotalTime = 0.09606s; TotalTimePerSample = 0.03753ms; SamplesPerSecond = 26648
 Epoch[ 2 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17007599; EvalErr[0]PerSample = 0.36015625; TotalTime = 0.09616s; TotalTimePerSample = 0.03756ms; SamplesPerSecond = 26621
 Epoch[ 2 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12343140; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.09620s; TotalTimePerSample = 0.03758ms; SamplesPerSecond = 26611
 Epoch[ 2 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12009735; EvalErr[0]PerSample = 0.34570312; TotalTime = 0.09589s; TotalTimePerSample = 0.03746ms; SamplesPerSecond = 26697
 Epoch[ 2 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.10230255; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.09559s; TotalTimePerSample = 0.03734ms; SamplesPerSecond = 26780
 Epoch[ 2 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12454529; EvalErr[0]PerSample = 0.34179688; TotalTime = 0.09594s; TotalTimePerSample = 0.03748ms; SamplesPerSecond = 26682
 Epoch[ 2 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13382874; EvalErr[0]PerSample = 0.34921875; TotalTime = 0.09603s; TotalTimePerSample = 0.03751ms; SamplesPerSecond = 26657
 Epoch[ 2 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27786255; EvalErr[0]PerSample = 0.39296875; TotalTime = 0.09608s; TotalTimePerSample = 0.03753ms; SamplesPerSecond = 26644
 Epoch[ 2 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16416626; EvalErr[0]PerSample = 0.34960938; TotalTime = 0.09607s; TotalTimePerSample = 0.03753ms; SamplesPerSecond = 26647
 Epoch[ 2 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12371216; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.09599s; TotalTimePerSample = 0.03750ms; SamplesPerSecond = 26668
 Epoch[ 2 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13847351; EvalErr[0]PerSample = 0.34453125; TotalTime = 0.09585s; TotalTimePerSample = 0.03744ms; SamplesPerSecond = 26707
 Epoch[ 2 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14408264; EvalErr[0]PerSample = 0.34414062; TotalTime = 0.09598s; TotalTimePerSample = 0.03749ms; SamplesPerSecond = 26671
 Epoch[ 2 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06380920; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.09587s; TotalTimePerSample = 0.03745ms; SamplesPerSecond = 26702
 Epoch[ 2 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09358521; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.09592s; TotalTimePerSample = 0.03747ms; SamplesPerSecond = 26690
 Epoch[ 2 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08025513; EvalErr[0]PerSample = 0.33046875; TotalTime = 0.09581s; TotalTimePerSample = 0.03743ms; SamplesPerSecond = 26718
 Epoch[ 2 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.05906372; EvalErr[0]PerSample = 0.32968750; TotalTime = 0.08985s; TotalTimePerSample = 0.03510ms; SamplesPerSecond = 28490
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.164273; EvalErrPerSample = 0.35511476; Ave LearnRatePerSample = 0.003125000047; EpochTime=3.177082
CNTKCommandTrainEnd: DPT_Pre1


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 2 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Validating for node Err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.



Printing Gradient Computation Node Order ... 

cr[1, 1] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[132, 0])
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 3 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: DPT_Pre2
NDLBuilder Using GPU 0
reading script file /home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /home/mluser/src/cplx_master/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/Pre2/cntkSpeech.0.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 2 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  4.33646812; EvalErr[0]PerSample = 0.80507812; TotalTime = 0.17076s; TotalTimePerSample = 0.06670ms; SamplesPerSecond = 14991
 Epoch[ 1 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.78729973; EvalErr[0]PerSample = 0.71328125; TotalTime = 0.16588s; TotalTimePerSample = 0.06480ms; SamplesPerSecond = 15432
 Epoch[ 1 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.21825867; EvalErr[0]PerSample = 0.58007812; TotalTime = 0.13480s; TotalTimePerSample = 0.05266ms; SamplesPerSecond = 18991
 Epoch[ 1 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.89405746; EvalErr[0]PerSample = 0.50468750; TotalTime = 0.12949s; TotalTimePerSample = 0.05058ms; SamplesPerSecond = 19769
 Epoch[ 1 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.71779938; EvalErr[0]PerSample = 0.47578125; TotalTime = 0.16601s; TotalTimePerSample = 0.06485ms; SamplesPerSecond = 15420
 Epoch[ 1 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.60265808; EvalErr[0]PerSample = 0.45000000; TotalTime = 0.16532s; TotalTimePerSample = 0.06458ms; SamplesPerSecond = 15484
 Epoch[ 1 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.56439209; EvalErr[0]PerSample = 0.44843750; TotalTime = 0.16557s; TotalTimePerSample = 0.06468ms; SamplesPerSecond = 15461
 Epoch[ 1 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.47621765; EvalErr[0]PerSample = 0.42578125; TotalTime = 0.13552s; TotalTimePerSample = 0.05294ms; SamplesPerSecond = 18890
 Epoch[ 1 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.39409637; EvalErr[0]PerSample = 0.40625000; TotalTime = 0.12883s; TotalTimePerSample = 0.05032ms; SamplesPerSecond = 19871
 Epoch[ 1 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.42145081; EvalErr[0]PerSample = 0.42343750; TotalTime = 0.12926s; TotalTimePerSample = 0.05049ms; SamplesPerSecond = 19805
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.39049683; EvalErr[0]PerSample = 0.42148438; TotalTime = 0.12864s; TotalTimePerSample = 0.05025ms; SamplesPerSecond = 19900
 Epoch[ 1 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.36727448; EvalErr[0]PerSample = 0.41054687; TotalTime = 0.12862s; TotalTimePerSample = 0.05024ms; SamplesPerSecond = 19903
 Epoch[ 1 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33726044; EvalErr[0]PerSample = 0.40703125; TotalTime = 0.15213s; TotalTimePerSample = 0.05943ms; SamplesPerSecond = 16827
 Epoch[ 1 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.40177307; EvalErr[0]PerSample = 0.40781250; TotalTime = 0.12857s; TotalTimePerSample = 0.05022ms; SamplesPerSecond = 19910
 Epoch[ 1 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.33615417; EvalErr[0]PerSample = 0.39570312; TotalTime = 0.12867s; TotalTimePerSample = 0.05026ms; SamplesPerSecond = 19895
 Epoch[ 1 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.34133606; EvalErr[0]PerSample = 0.40273437; TotalTime = 0.12841s; TotalTimePerSample = 0.05016ms; SamplesPerSecond = 19936
 Epoch[ 1 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.26413574; EvalErr[0]PerSample = 0.37304688; TotalTime = 0.12802s; TotalTimePerSample = 0.05001ms; SamplesPerSecond = 19996
 Epoch[ 1 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28038635; EvalErr[0]PerSample = 0.38593750; TotalTime = 0.12841s; TotalTimePerSample = 0.05016ms; SamplesPerSecond = 19936
 Epoch[ 1 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.29767151; EvalErr[0]PerSample = 0.39179687; TotalTime = 0.16430s; TotalTimePerSample = 0.06418ms; SamplesPerSecond = 15581
 Epoch[ 1 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28023682; EvalErr[0]PerSample = 0.39687500; TotalTime = 0.16454s; TotalTimePerSample = 0.06427ms; SamplesPerSecond = 15558
 Epoch[ 1 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.26818542; EvalErr[0]PerSample = 0.38945313; TotalTime = 0.16489s; TotalTimePerSample = 0.06441ms; SamplesPerSecond = 15525
 Epoch[ 1 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21394043; EvalErr[0]PerSample = 0.36250000; TotalTime = 0.16427s; TotalTimePerSample = 0.06417ms; SamplesPerSecond = 15583
 Epoch[ 1 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20627136; EvalErr[0]PerSample = 0.36953125; TotalTime = 0.16384s; TotalTimePerSample = 0.06400ms; SamplesPerSecond = 15624
 Epoch[ 1 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.25008850; EvalErr[0]PerSample = 0.37929687; TotalTime = 0.16415s; TotalTimePerSample = 0.06412ms; SamplesPerSecond = 15595
 Epoch[ 1 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22965393; EvalErr[0]PerSample = 0.37617187; TotalTime = 0.16463s; TotalTimePerSample = 0.06431ms; SamplesPerSecond = 15550
 Epoch[ 1 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.15062561; EvalErr[0]PerSample = 0.34960938; TotalTime = 0.16421s; TotalTimePerSample = 0.06414ms; SamplesPerSecond = 15590
 Epoch[ 1 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16630554; EvalErr[0]PerSample = 0.35390625; TotalTime = 0.13011s; TotalTimePerSample = 0.05082ms; SamplesPerSecond = 19675
 Epoch[ 1 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.22966309; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.12816s; TotalTimePerSample = 0.05006ms; SamplesPerSecond = 19975
 Epoch[ 1 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16364136; EvalErr[0]PerSample = 0.36445312; TotalTime = 0.12827s; TotalTimePerSample = 0.05010ms; SamplesPerSecond = 19958
 Epoch[ 1 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17280579; EvalErr[0]PerSample = 0.35351562; TotalTime = 0.12890s; TotalTimePerSample = 0.05035ms; SamplesPerSecond = 19860
 Epoch[ 1 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16119995; EvalErr[0]PerSample = 0.34687500; TotalTime = 0.12864s; TotalTimePerSample = 0.05025ms; SamplesPerSecond = 19901
 Epoch[ 1 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16999512; EvalErr[0]PerSample = 0.35000000; TotalTime = 0.12263s; TotalTimePerSample = 0.04790ms; SamplesPerSecond = 20875
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.5028688; EvalErrPerSample = 0.42475587; Ave LearnRatePerSample = 0.003125000047; EpochTime=5.763236
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14169836; EvalErr[0]PerSample = 0.35156250; TotalTime = 0.12945s; TotalTimePerSample = 0.05057ms; SamplesPerSecond = 19775
 Epoch[ 2 of 2]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16675386; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.12838s; TotalTimePerSample = 0.05015ms; SamplesPerSecond = 19940
 Epoch[ 2 of 2]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23896408; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.12855s; TotalTimePerSample = 0.05022ms; SamplesPerSecond = 19914
 Epoch[ 2 of 2]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17921028; EvalErr[0]PerSample = 0.36289063; TotalTime = 0.12850s; TotalTimePerSample = 0.05019ms; SamplesPerSecond = 19922
 Epoch[ 2 of 2]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13760986; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.12836s; TotalTimePerSample = 0.05014ms; SamplesPerSecond = 19943
 Epoch[ 2 of 2]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.21572113; EvalErr[0]PerSample = 0.36601563; TotalTime = 0.12828s; TotalTimePerSample = 0.05011ms; SamplesPerSecond = 19956
 Epoch[ 2 of 2]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14051437; EvalErr[0]PerSample = 0.34140625; TotalTime = 0.13201s; TotalTimePerSample = 0.05157ms; SamplesPerSecond = 19392
 Epoch[ 2 of 2]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12286606; EvalErr[0]PerSample = 0.34492187; TotalTime = 0.16368s; TotalTimePerSample = 0.06394ms; SamplesPerSecond = 15640
 Epoch[ 2 of 2]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14243240; EvalErr[0]PerSample = 0.33789062; TotalTime = 0.16444s; TotalTimePerSample = 0.06424ms; SamplesPerSecond = 15567
 Epoch[ 2 of 2]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12677765; EvalErr[0]PerSample = 0.35390625; TotalTime = 0.16509s; TotalTimePerSample = 0.06449ms; SamplesPerSecond = 15506
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14400177; EvalErr[0]PerSample = 0.33984375; TotalTime = 0.16439s; TotalTimePerSample = 0.06422ms; SamplesPerSecond = 15572
 Epoch[ 2 of 2]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12832870; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.16414s; TotalTimePerSample = 0.06412ms; SamplesPerSecond = 15596
 Epoch[ 2 of 2]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11099091; EvalErr[0]PerSample = 0.34414062; TotalTime = 0.16406s; TotalTimePerSample = 0.06409ms; SamplesPerSecond = 15603
 Epoch[ 2 of 2]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06680908; EvalErr[0]PerSample = 0.32304688; TotalTime = 0.16356s; TotalTimePerSample = 0.06389ms; SamplesPerSecond = 15652
 Epoch[ 2 of 2]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.05362549; EvalErr[0]PerSample = 0.30859375; TotalTime = 0.16398s; TotalTimePerSample = 0.06405ms; SamplesPerSecond = 15611
 Epoch[ 2 of 2]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06292725; EvalErr[0]PerSample = 0.32734375; TotalTime = 0.16541s; TotalTimePerSample = 0.06461ms; SamplesPerSecond = 15476
 Epoch[ 2 of 2]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14273834; EvalErr[0]PerSample = 0.34882812; TotalTime = 0.16545s; TotalTimePerSample = 0.06463ms; SamplesPerSecond = 15472
 Epoch[ 2 of 2]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14362183; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.13098s; TotalTimePerSample = 0.05116ms; SamplesPerSecond = 19544
 Epoch[ 2 of 2]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08687897; EvalErr[0]PerSample = 0.33671875; TotalTime = 0.12837s; TotalTimePerSample = 0.05014ms; SamplesPerSecond = 19942
 Epoch[ 2 of 2]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.07546844; EvalErr[0]PerSample = 0.33632812; TotalTime = 0.12850s; TotalTimePerSample = 0.05019ms; SamplesPerSecond = 19922
 Epoch[ 2 of 2]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06579132; EvalErr[0]PerSample = 0.32695313; TotalTime = 0.12887s; TotalTimePerSample = 0.05034ms; SamplesPerSecond = 19864
 Epoch[ 2 of 2]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09530640; EvalErr[0]PerSample = 0.33242187; TotalTime = 0.12837s; TotalTimePerSample = 0.05014ms; SamplesPerSecond = 19942
 Epoch[ 2 of 2]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11944122; EvalErr[0]PerSample = 0.35117188; TotalTime = 0.12823s; TotalTimePerSample = 0.05009ms; SamplesPerSecond = 19963
 Epoch[ 2 of 2]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13388062; EvalErr[0]PerSample = 0.35507813; TotalTime = 0.12828s; TotalTimePerSample = 0.05011ms; SamplesPerSecond = 19955
 Epoch[ 2 of 2]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08914795; EvalErr[0]PerSample = 0.33437500; TotalTime = 0.12840s; TotalTimePerSample = 0.05016ms; SamplesPerSecond = 19937
 Epoch[ 2 of 2]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06987000; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.12851s; TotalTimePerSample = 0.05020ms; SamplesPerSecond = 19920
 Epoch[ 2 of 2]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06095581; EvalErr[0]PerSample = 0.32109375; TotalTime = 0.12857s; TotalTimePerSample = 0.05022ms; SamplesPerSecond = 19911
 Epoch[ 2 of 2]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.09798889; EvalErr[0]PerSample = 0.33085938; TotalTime = 0.12868s; TotalTimePerSample = 0.05027ms; SamplesPerSecond = 19894
 Epoch[ 2 of 2]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.02103271; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.12890s; TotalTimePerSample = 0.05035ms; SamplesPerSecond = 19860
 Epoch[ 2 of 2]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06984253; EvalErr[0]PerSample = 0.33398438; TotalTime = 0.12823s; TotalTimePerSample = 0.05009ms; SamplesPerSecond = 19964
 Epoch[ 2 of 2]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.06397095; EvalErr[0]PerSample = 0.32929687; TotalTime = 0.12842s; TotalTimePerSample = 0.05016ms; SamplesPerSecond = 19935
 Epoch[ 2 of 2]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.05246582; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.12215s; TotalTimePerSample = 0.04772ms; SamplesPerSecond = 20957
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1114886; EvalErrPerSample = 0.34130859; Ave LearnRatePerSample = 0.003125000047; EpochTime=4.486329
CNTKCommandTrainEnd: DPT_Pre2


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 2 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Validating for node Err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.



Printing Gradient Computation Node Order ... 

cr[1, 1] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[132, 0])
OL.z[132, 0] = Plus(OL.t[132, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[132, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[512, 0])
HL2.y[512, 0] = Sigmoid(HL2.z[512, 0])
HL2.z[512, 0] = Plus(HL2.t[512, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[512, 0] = Times(HL2.W[512, 512], HL1.y[512, 0])
HL1.y[512, 0] = Sigmoid(HL1.z[512, 0])
HL1.z[512, 0] = Plus(HL1.t[512, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[512, 0] = Times(HL1.W[512, 363], featNorm[363, 0])
featNorm[363, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 3 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /home/mluser/src/cplx_master/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/mluser/src/cplx_master/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151012184916.181476/Speech/DNN_DiscriminativePreTraining@debug_gpu/models/cntkSpeech.0.


Printing Gradient Computation Node Order ... 

cr[0, 0] = CrossEntropyWithSoftmax(labels[132, 0], OL.z[0, 0])
OL.z[0, 0] = Plus(OL.t[0, 0], OL.b[132, 1])
OL.b[132, 1] = LearnableParameter
OL.t[0, 0] = Times(OL.W[132, 512], HL3.y[0, 0])
HL3.y[0, 0] = Sigmoid(HL3.z[0, 0])
HL3.z[0, 0] = Plus(HL3.t[0, 0], HL3.b[512, 1])
HL3.b[512, 1] = LearnableParameter
HL3.t[0, 0] = Times(HL3.W[512, 512], HL2.y[0, 0])
HL2.y[0, 0] = Sigmoid(HL2.z[0, 0])
HL2.z[0, 0] = Plus(HL2.t[0, 0], HL2.b[512, 1])
HL2.b[512, 1] = LearnableParameter
HL2.t[0, 0] = Times(HL2.W[512, 512], HL1.y[0, 0])
HL1.y[0, 0] = Sigmoid(HL1.z[0, 0])
HL1.z[0, 0] = Plus(HL1.t[0, 0], HL1.b[512, 1])
HL1.b[512, 1] = LearnableParameter
HL1.t[0, 0] = Times(HL1.W[512, 363], featNorm[0, 0])
featNorm[0, 0] = PerDimMeanVarNormalization(features[363, 0], GlobalMean[363, 1], GlobalInvStd[363, 1])
GlobalInvStd[363, 1] = LearnableParameter
GlobalMean[363, 1] = LearnableParameter
features[363, 0] = InputValue
HL1.W[512, 363] = LearnableParameter
HL2.W[512, 512] = LearnableParameter
HL3.W[512, 512] = LearnableParameter
OL.W[132, 512] = LearnableParameter
labels[132, 0] = InputValue

Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 13 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 2 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node ScaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> GlobalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(GlobalPrior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.



Validating for node Err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node Err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> GlobalMean = LearnableParameter -> [363, 1]
Validating --> GlobalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], GlobalMean[363, 1], GlobalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> Err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

GetTrainCriterionNodes  ...
GetEvalCriterionNodes  ...
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117 
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10 of 320]: SamplesSeen = 2560; TrainLossPerSample =  3.96939201; EvalErr[0]PerSample = 0.81250000; TotalTime = 0.20433s; TotalTimePerSample = 0.07982ms; SamplesPerSecond = 12528
 Epoch[ 1 of 4]-Minibatch[  11-  20 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.64767342; EvalErr[0]PerSample = 0.63281250; TotalTime = 0.19765s; TotalTimePerSample = 0.07721ms; SamplesPerSecond = 12952
 Epoch[ 1 of 4]-Minibatch[  21-  30 of 320]: SamplesSeen = 2560; TrainLossPerSample =  2.02707901; EvalErr[0]PerSample = 0.53867188; TotalTime = 0.20620s; TotalTimePerSample = 0.08055ms; SamplesPerSecond = 12415
 Epoch[ 1 of 4]-Minibatch[  31-  40 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.74281921; EvalErr[0]PerSample = 0.47343750; TotalTime = 0.19865s; TotalTimePerSample = 0.07760ms; SamplesPerSecond = 12886
 Epoch[ 1 of 4]-Minibatch[  41-  50 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.58044128; EvalErr[0]PerSample = 0.45156250; TotalTime = 0.19802s; TotalTimePerSample = 0.07735ms; SamplesPerSecond = 12928
 Epoch[ 1 of 4]-Minibatch[  51-  60 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.47565231; EvalErr[0]PerSample = 0.41757813; TotalTime = 0.19878s; TotalTimePerSample = 0.07765ms; SamplesPerSecond = 12878
 Epoch[ 1 of 4]-Minibatch[  61-  70 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.43280945; EvalErr[0]PerSample = 0.41132812; TotalTime = 0.17110s; TotalTimePerSample = 0.06684ms; SamplesPerSecond = 14961
 Epoch[ 1 of 4]-Minibatch[  71-  80 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.35942993; EvalErr[0]PerSample = 0.39531250; TotalTime = 0.16138s; TotalTimePerSample = 0.06304ms; SamplesPerSecond = 15862
 Epoch[ 1 of 4]-Minibatch[  81-  90 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28088837; EvalErr[0]PerSample = 0.37812500; TotalTime = 0.16122s; TotalTimePerSample = 0.06298ms; SamplesPerSecond = 15879
 Epoch[ 1 of 4]-Minibatch[  91- 100 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.29705811; EvalErr[0]PerSample = 0.39570312; TotalTime = 0.16121s; TotalTimePerSample = 0.06297ms; SamplesPerSecond = 15879
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 4]-Minibatch[ 101- 110 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.28361969; EvalErr[0]PerSample = 0.39101562; TotalTime = 0.16147s; TotalTimePerSample = 0.06308ms; SamplesPerSecond = 15853
 Epoch[ 1 of 4]-Minibatch[ 111- 120 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27552490; EvalErr[0]PerSample = 0.38515625; TotalTime = 0.16204s; TotalTimePerSample = 0.06330ms; SamplesPerSecond = 15798
 Epoch[ 1 of 4]-Minibatch[ 121- 130 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.23978882; EvalErr[0]PerSample = 0.37265625; TotalTime = 0.16103s; TotalTimePerSample = 0.06290ms; SamplesPerSecond = 15897
 Epoch[ 1 of 4]-Minibatch[ 131- 140 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.31328888; EvalErr[0]PerSample = 0.38593750; TotalTime = 0.16089s; TotalTimePerSample = 0.06285ms; SamplesPerSecond = 15911
 Epoch[ 1 of 4]-Minibatch[ 141- 150 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.25646362; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.18754s; TotalTimePerSample = 0.07326ms; SamplesPerSecond = 13650
 Epoch[ 1 of 4]-Minibatch[ 151- 160 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.27446442; EvalErr[0]PerSample = 0.38398437; TotalTime = 0.19911s; TotalTimePerSample = 0.07778ms; SamplesPerSecond = 12857
 Epoch[ 1 of 4]-Minibatch[ 161- 170 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20181580; EvalErr[0]PerSample = 0.36289063; TotalTime = 0.19988s; TotalTimePerSample = 0.07808ms; SamplesPerSecond = 12807
 Epoch[ 1 of 4]-Minibatch[ 171- 180 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20729980; EvalErr[0]PerSample = 0.36796875; TotalTime = 0.19928s; TotalTimePerSample = 0.07784ms; SamplesPerSecond = 12846
 Epoch[ 1 of 4]-Minibatch[ 181- 190 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20639648; EvalErr[0]PerSample = 0.36914062; TotalTime = 0.19860s; TotalTimePerSample = 0.07758ms; SamplesPerSecond = 12890
 Epoch[ 1 of 4]-Minibatch[ 191- 200 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20577698; EvalErr[0]PerSample = 0.37539062; TotalTime = 0.17294s; TotalTimePerSample = 0.06755ms; SamplesPerSecond = 14803
 Epoch[ 1 of 4]-Minibatch[ 201- 210 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.20345459; EvalErr[0]PerSample = 0.37265625; TotalTime = 0.16089s; TotalTimePerSample = 0.06285ms; SamplesPerSecond = 15911
 Epoch[ 1 of 4]-Minibatch[ 211- 220 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14157104; EvalErr[0]PerSample = 0.34609375; TotalTime = 0.16185s; TotalTimePerSample = 0.06322ms; SamplesPerSecond = 15817
 Epoch[ 1 of 4]-Minibatch[ 221- 230 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.14772339; EvalErr[0]PerSample = 0.35351562; TotalTime = 0.16116s; TotalTimePerSample = 0.06295ms; SamplesPerSecond = 15884
 Epoch[ 1 of 4]-Minibatch[ 231- 240 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.19301453; EvalErr[0]PerSample = 0.35703125; TotalTime = 0.16144s; TotalTimePerSample = 0.06306ms; SamplesPerSecond = 15857
 Epoch[ 1 of 4]-Minibatch[ 241- 250 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.16928101; EvalErr[0]PerSample = 0.36406250; TotalTime = 0.16115s; TotalTimePerSample = 0.06295ms; SamplesPerSecond = 15885
 Epoch[ 1 of 4]-Minibatch[ 251- 260 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.08552246; EvalErr[0]PerSample = 0.34062500; TotalTime = 0.16084s; TotalTimePerSample = 0.06283ms; SamplesPerSecond = 15916
 Epoch[ 1 of 4]-Minibatch[ 261- 270 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11441040; EvalErr[0]PerSample = 0.33945313; TotalTime = 0.20073s; TotalTimePerSample = 0.07841ms; SamplesPerSecond = 12753
 Epoch[ 1 of 4]-Minibatch[ 271- 280 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.17764893; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.20118s; TotalTimePerSample = 0.07859ms; SamplesPerSecond = 12724
 Epoch[ 1 of 4]-Minibatch[ 281- 290 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.11296692; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.19897s; TotalTimePerSample = 0.07772ms; SamplesPerSecond = 12866
 Epoch[ 1 of 4]-Minibatch[ 291- 300 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.13165283; EvalErr[0]PerSample = 0.34453125; TotalTime = 0.19846s; TotalTimePerSample = 0.07752ms; SamplesPerSecond = 12899
 Epoch[ 1 of 4]-Minibatch[ 301- 310 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12458191; EvalErr[0]PerSample = 0.34570312; TotalTime = 0.19971s; TotalTimePerSample = 0.07801ms; SamplesPerSecond = 12818
 Epoch[ 1 of 4]-Minibatch[ 311- 320 of 320]: SamplesSeen = 2560; TrainLossPerSample =  1.12154541; EvalErr[0]PerSample = 0.33906250; TotalTime = 0.19018s; TotalTimePerSample = 0.07429ms; SamplesPerSecond = 13461
Finished Epoch[ 1 of 4]: [Training Set] TrainLossPerSample = 1.406283; EvalErrPerSample = 0.40246582; Ave LearnRatePerSample = 0.003125000047; EpochTime=7.080416
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.49601746; EvalErr[0]PerSample = 0.41562500; TotalTime = 0.23368s; TotalTimePerSample = 0.04564ms; SamplesPerSecond = 21910
 Epoch[ 2 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.33961754; EvalErr[0]PerSample = 0.39316406; TotalTime = 0.22738s; TotalTimePerSample = 0.04441ms; SamplesPerSecond = 22516
 Epoch[ 2 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.19400368; EvalErr[0]PerSample = 0.36679688; TotalTime = 0.26154s; TotalTimePerSample = 0.05108ms; SamplesPerSecond = 19576
 Epoch[ 2 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11921158; EvalErr[0]PerSample = 0.34023437; TotalTime = 0.29191s; TotalTimePerSample = 0.05701ms; SamplesPerSecond = 17539
 Epoch[ 2 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.12285690; EvalErr[0]PerSample = 0.34140625; TotalTime = 0.29226s; TotalTimePerSample = 0.05708ms; SamplesPerSecond = 17518
 Epoch[ 2 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.13342743; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.27248s; TotalTimePerSample = 0.05322ms; SamplesPerSecond = 18790
 Epoch[ 2 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08950500; EvalErr[0]PerSample = 0.33593750; TotalTime = 0.22818s; TotalTimePerSample = 0.04457ms; SamplesPerSecond = 22438
 Epoch[ 2 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.06079788; EvalErr[0]PerSample = 0.32363281; TotalTime = 0.22761s; TotalTimePerSample = 0.04446ms; SamplesPerSecond = 22494
 Epoch[ 2 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.18579025; EvalErr[0]PerSample = 0.36933594; TotalTime = 0.22761s; TotalTimePerSample = 0.04446ms; SamplesPerSecond = 22494
 Epoch[ 2 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08288193; EvalErr[0]PerSample = 0.34140625; TotalTime = 0.22663s; TotalTimePerSample = 0.04426ms; SamplesPerSecond = 22591
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05400925; EvalErr[0]PerSample = 0.32578125; TotalTime = 0.25990s; TotalTimePerSample = 0.05076ms; SamplesPerSecond = 19700
 Epoch[ 2 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.14049835; EvalErr[0]PerSample = 0.35664062; TotalTime = 0.29239s; TotalTimePerSample = 0.05711ms; SamplesPerSecond = 17510
 Epoch[ 2 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11492462; EvalErr[0]PerSample = 0.34648438; TotalTime = 0.29289s; TotalTimePerSample = 0.05720ms; SamplesPerSecond = 17481
 Epoch[ 2 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.07589722; EvalErr[0]PerSample = 0.32265625; TotalTime = 0.29237s; TotalTimePerSample = 0.05710ms; SamplesPerSecond = 17512
 Epoch[ 2 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04273682; EvalErr[0]PerSample = 0.32871094; TotalTime = 0.28067s; TotalTimePerSample = 0.05482ms; SamplesPerSecond = 18242
 Epoch[ 2 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05914001; EvalErr[0]PerSample = 0.32421875; TotalTime = 0.25271s; TotalTimePerSample = 0.04936ms; SamplesPerSecond = 20260
Finished Epoch[ 2 of 4]: [Training Set] TrainLossPerSample = 1.1444572; EvalErrPerSample = 0.34843752; Ave LearnRatePerSample = 0.003125000047; EpochTime=4.181761
Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.11066093; EvalErr[0]PerSample = 0.33886719; TotalTime = 0.22848s; TotalTimePerSample = 0.04462ms; SamplesPerSecond = 22409
 Epoch[ 3 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.10548515; EvalErr[0]PerSample = 0.34511719; TotalTime = 0.22788s; TotalTimePerSample = 0.04451ms; SamplesPerSecond = 22468
 Epoch[ 3 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.10001144; EvalErr[0]PerSample = 0.34902344; TotalTime = 0.22845s; TotalTimePerSample = 0.04462ms; SamplesPerSecond = 22411
 Epoch[ 3 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.12368736; EvalErr[0]PerSample = 0.33847656; TotalTime = 0.22749s; TotalTimePerSample = 0.04443ms; SamplesPerSecond = 22506
 Epoch[ 3 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.12565804; EvalErr[0]PerSample = 0.34316406; TotalTime = 0.22824s; TotalTimePerSample = 0.04458ms; SamplesPerSecond = 22432
 Epoch[ 3 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08098526; EvalErr[0]PerSample = 0.33652344; TotalTime = 0.25245s; TotalTimePerSample = 0.04931ms; SamplesPerSecond = 20281
 Epoch[ 3 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.09546432; EvalErr[0]PerSample = 0.33964844; TotalTime = 0.29113s; TotalTimePerSample = 0.05686ms; SamplesPerSecond = 17586
 Epoch[ 3 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.07909393; EvalErr[0]PerSample = 0.33242187; TotalTime = 0.29145s; TotalTimePerSample = 0.05692ms; SamplesPerSecond = 17567
 Epoch[ 3 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02718582; EvalErr[0]PerSample = 0.31562500; TotalTime = 0.29116s; TotalTimePerSample = 0.05687ms; SamplesPerSecond = 17584
 Epoch[ 3 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04374771; EvalErr[0]PerSample = 0.31953125; TotalTime = 0.28709s; TotalTimePerSample = 0.05607ms; SamplesPerSecond = 17834
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 3 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05229645; EvalErr[0]PerSample = 0.33457031; TotalTime = 0.29182s; TotalTimePerSample = 0.05700ms; SamplesPerSecond = 17545
 Epoch[ 3 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.08028870; EvalErr[0]PerSample = 0.33769531; TotalTime = 0.23230s; TotalTimePerSample = 0.04537ms; SamplesPerSecond = 22040
 Epoch[ 3 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.05386963; EvalErr[0]PerSample = 0.31933594; TotalTime = 0.22718s; TotalTimePerSample = 0.04437ms; SamplesPerSecond = 22536
 Epoch[ 3 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02473297; EvalErr[0]PerSample = 0.32167969; TotalTime = 0.22772s; TotalTimePerSample = 0.04448ms; SamplesPerSecond = 22483
 Epoch[ 3 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.04503784; EvalErr[0]PerSample = 0.33085938; TotalTime = 0.22719s; TotalTimePerSample = 0.04437ms; SamplesPerSecond = 22536
 Epoch[ 3 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.01943665; EvalErr[0]PerSample = 0.32050781; TotalTime = 0.21543s; TotalTimePerSample = 0.04208ms; SamplesPerSecond = 23766
Finished Epoch[ 3 of 4]: [Training Set] TrainLossPerSample = 1.0729777; EvalErrPerSample = 0.33269045; Ave LearnRatePerSample = 0.003125000047; EpochTime=3.995354
Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210 
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02822218; EvalErr[0]PerSample = 0.31328125; TotalTime = 0.22742s; TotalTimePerSample = 0.04442ms; SamplesPerSecond = 22513
 Epoch[ 4 of 4]-Minibatch[  11-  20 of 160]: SamplesSeen = 4926; TrainLossPerSample =  1.04848684; EvalErr[0]PerSample = 0.32967925; TotalTime = 0.51921s; TotalTimePerSample = 0.10540ms; SamplesPerSecond = 9487
 Epoch[ 4 of 4]-Minibatch[  21-  30 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.01644306; EvalErr[0]PerSample = 0.32148437; TotalTime = 0.29076s; TotalTimePerSample = 0.05679ms; SamplesPerSecond = 17608
 Epoch[ 4 of 4]-Minibatch[  31-  40 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99039593; EvalErr[0]PerSample = 0.31425781; TotalTime = 0.29139s; TotalTimePerSample = 0.05691ms; SamplesPerSecond = 17570
 Epoch[ 4 of 4]-Minibatch[  41-  50 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99446030; EvalErr[0]PerSample = 0.31562500; TotalTime = 0.29187s; TotalTimePerSample = 0.05701ms; SamplesPerSecond = 17541
 Epoch[ 4 of 4]-Minibatch[  51-  60 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00650482; EvalErr[0]PerSample = 0.32382813; TotalTime = 0.29209s; TotalTimePerSample = 0.05705ms; SamplesPerSecond = 17528
 Epoch[ 4 of 4]-Minibatch[  61-  70 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.02417755; EvalErr[0]PerSample = 0.32031250; TotalTime = 0.29194s; TotalTimePerSample = 0.05702ms; SamplesPerSecond = 17537
 Epoch[ 4 of 4]-Minibatch[  71-  80 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.01169128; EvalErr[0]PerSample = 0.31855469; TotalTime = 0.24643s; TotalTimePerSample = 0.04813ms; SamplesPerSecond = 20776
 Epoch[ 4 of 4]-Minibatch[  81-  90 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.99888992; EvalErr[0]PerSample = 0.30937500; TotalTime = 0.22709s; TotalTimePerSample = 0.04435ms; SamplesPerSecond = 22546
 Epoch[ 4 of 4]-Minibatch[  91- 100 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.00544128; EvalErr[0]PerSample = 0.31113281; TotalTime = 0.22708s; TotalTimePerSample = 0.04435ms; SamplesPerSecond = 22547
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 4 of 4]-Minibatch[ 101- 110 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.01687851; EvalErr[0]PerSample = 0.31093750; TotalTime = 0.22702s; TotalTimePerSample = 0.04434ms; SamplesPerSecond = 22553
 Epoch[ 4 of 4]-Minibatch[ 111- 120 of 160]: SamplesSeen = 5120; TrainLossPerSample =  1.03951569; EvalErr[0]PerSample = 0.32851562; TotalTime = 0.26397s; TotalTimePerSample = 0.05156ms; SamplesPerSecond = 19396
 Epoch[ 4 of 4]-Minibatch[ 121- 130 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.98455429; EvalErr[0]PerSample = 0.30234375; TotalTime = 0.28984s; TotalTimePerSample = 0.05661ms; SamplesPerSecond = 17664
 Epoch[ 4 of 4]-Minibatch[ 131- 140 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.96297150; EvalErr[0]PerSample = 0.30136719; TotalTime = 0.29115s; TotalTimePerSample = 0.05687ms; SamplesPerSecond = 17585
 Epoch[ 4 of 4]-Minibatch[ 141- 150 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.98015137; EvalErr[0]PerSample = 0.31054688; TotalTime = 0.29163s; TotalTimePerSample = 0.05696ms; SamplesPerSecond = 17556
 Epoch[ 4 of 4]-Minibatch[ 151- 160 of 160]: SamplesSeen = 5120; TrainLossPerSample =  0.97653656; EvalErr[0]PerSample = 0.29863281; TotalTime = 0.27506s; TotalTimePerSample = 0.05372ms; SamplesPerSecond = 18614
Finished Epoch[ 4 of 4]: [Training Set] TrainLossPerSample = 1.0051814; EvalErrPerSample = 0.31445312; Ave LearnRatePerSample = 0.003125000047; EpochTime=4.579516
CNTKCommandTrainEnd: speechTrain
COMPLETED
