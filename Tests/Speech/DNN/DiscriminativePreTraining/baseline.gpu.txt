=== Running /home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining DeviceId=0
-------------------------------------------------------------------
Build info: 

		Built time: Dec 10 2015 14:48:57
		Last modified date: Tue Dec  8 10:08:43 2015
		Build type: release
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.0
		CUB_PATH: /usr/local/cub-1.4.1
		Build Branch: master
		Build SHA1: 5e0017ac9c55c23d53cb524c8acb7d6d9bfd0269
-------------------------------------------------------------------
running on localhost at 2015/12/10 16:09:46
command line: 
/home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining DeviceId=0 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.config:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.config:ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
configparameters: cntk_dpt.config:currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk_dpt.config:DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk_dpt.config:deviceId=0
configparameters: cntk_dpt.config:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.config:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.config:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.config:ndlMacros=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.config:precision=float
configparameters: cntk_dpt.config:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.config:RunDir=/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.config:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.config:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.config:traceLevel=1
<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain 
precision = float
CNTKModelPath: /tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
CNTKCommandTrainInfo: dptPre1 : 2
CNTKModelPath: /tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
CNTKCommandTrainInfo: dptPre2 : 2
CNTKModelPath: /tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
CNTKCommandTrainInfo: speechTrain : 4
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8
CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for gradient computing
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.1250%]: SamplesSeen = 2560; TrainLossPerSample =  3.77505875; EvalErr[0]PerSample = 0.83125000; TotalTime = 0.1600s; SamplesPerSecond = 16003.3
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.2500%]: SamplesSeen = 2560; TrainLossPerSample =  3.05222931; EvalErr[0]PerSample = 0.72968750; TotalTime = 0.0699s; SamplesPerSecond = 36626.9
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.3750%]: SamplesSeen = 2560; TrainLossPerSample =  2.53188248; EvalErr[0]PerSample = 0.65039062; TotalTime = 0.0696s; SamplesPerSecond = 36783.2
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.5000%]: SamplesSeen = 2560; TrainLossPerSample =  2.25200882; EvalErr[0]PerSample = 0.60039062; TotalTime = 0.0696s; SamplesPerSecond = 36802.2
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.6250%]: SamplesSeen = 2560; TrainLossPerSample =  2.03887405; EvalErr[0]PerSample = 0.54296875; TotalTime = 0.0695s; SamplesPerSecond = 36810.7
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.89382477; EvalErr[0]PerSample = 0.51718750; TotalTime = 0.0696s; SamplesPerSecond = 36790.6
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.78768005; EvalErr[0]PerSample = 0.50195312; TotalTime = 0.0695s; SamplesPerSecond = 36826.1
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.73793335; EvalErr[0]PerSample = 0.50273437; TotalTime = 0.0695s; SamplesPerSecond = 36833.5
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.70590057; EvalErr[0]PerSample = 0.50000000; TotalTime = 0.0697s; SamplesPerSecond = 36733.6
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.55943909; EvalErr[0]PerSample = 0.43984375; TotalTime = 0.0695s; SamplesPerSecond = 36854.7
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.54098206; EvalErr[0]PerSample = 0.45507812; TotalTime = 0.0694s; SamplesPerSecond = 36894.5
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.51198578; EvalErr[0]PerSample = 0.44687500; TotalTime = 0.0694s; SamplesPerSecond = 36900.9
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.50512390; EvalErr[0]PerSample = 0.44726562; TotalTime = 0.0692s; SamplesPerSecond = 36968.0
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.45860596; EvalErr[0]PerSample = 0.44062500; TotalTime = 0.0693s; SamplesPerSecond = 36935.0
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.53634644; EvalErr[0]PerSample = 0.44960937; TotalTime = 0.0694s; SamplesPerSecond = 36885.5
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.47159729; EvalErr[0]PerSample = 0.43359375; TotalTime = 0.0694s; SamplesPerSecond = 36884.4
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.40028687; EvalErr[0]PerSample = 0.42265625; TotalTime = 0.0694s; SamplesPerSecond = 36878.0
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.40258789; EvalErr[0]PerSample = 0.41132812; TotalTime = 0.0696s; SamplesPerSecond = 36801.7
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.39841919; EvalErr[0]PerSample = 0.39960937; TotalTime = 0.0694s; SamplesPerSecond = 36877.0
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.31150208; EvalErr[0]PerSample = 0.39218750; TotalTime = 0.0695s; SamplesPerSecond = 36836.1
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.32054749; EvalErr[0]PerSample = 0.39648438; TotalTime = 0.0695s; SamplesPerSecond = 36837.7
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.28134460; EvalErr[0]PerSample = 0.38203125; TotalTime = 0.0694s; SamplesPerSecond = 36890.8
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.34499512; EvalErr[0]PerSample = 0.41289063; TotalTime = 0.0695s; SamplesPerSecond = 36854.2
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.31982727; EvalErr[0]PerSample = 0.39531250; TotalTime = 0.0697s; SamplesPerSecond = 36749.4
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.27533875; EvalErr[0]PerSample = 0.38203125; TotalTime = 0.0693s; SamplesPerSecond = 36959.5
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.23049927; EvalErr[0]PerSample = 0.37070313; TotalTime = 0.0695s; SamplesPerSecond = 36843.0
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.27904358; EvalErr[0]PerSample = 0.38359375; TotalTime = 0.0695s; SamplesPerSecond = 36823.4
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.30589600; EvalErr[0]PerSample = 0.38476562; TotalTime = 0.0694s; SamplesPerSecond = 36898.2
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.22993774; EvalErr[0]PerSample = 0.37773438; TotalTime = 0.0695s; SamplesPerSecond = 36854.7
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.20908508; EvalErr[0]PerSample = 0.36054687; TotalTime = 0.0695s; SamplesPerSecond = 36846.7
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.28749084; EvalErr[0]PerSample = 0.40898438; TotalTime = 0.0694s; SamplesPerSecond = 36902.0
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.40186768; EvalErr[0]PerSample = 0.40820312; TotalTime = 0.0695s; SamplesPerSecond = 36820.8
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.636192; EvalErrPerSample = 0.46182862; AvgLearningRatePerSample = 0.003125; EpochTime=2.38585
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.30975819; EvalErr[0]PerSample = 0.39531250; TotalTime = 0.0700s; SamplesPerSecond = 36585.5
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.24904690; EvalErr[0]PerSample = 0.38203125; TotalTime = 0.0693s; SamplesPerSecond = 36933.9
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.23808594; EvalErr[0]PerSample = 0.38515625; TotalTime = 0.0693s; SamplesPerSecond = 36941.4
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.26858025; EvalErr[0]PerSample = 0.38476562; TotalTime = 0.0694s; SamplesPerSecond = 36896.6
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.20073395; EvalErr[0]PerSample = 0.36015625; TotalTime = 0.0693s; SamplesPerSecond = 36949.9
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.16229973; EvalErr[0]PerSample = 0.36093750; TotalTime = 0.0695s; SamplesPerSecond = 36845.1
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.26922150; EvalErr[0]PerSample = 0.36796875; TotalTime = 0.0696s; SamplesPerSecond = 36801.2
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.13705521; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.0693s; SamplesPerSecond = 36927.0
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.16430893; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.0695s; SamplesPerSecond = 36851.5
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.15487900; EvalErr[0]PerSample = 0.36132812; TotalTime = 0.0694s; SamplesPerSecond = 36879.1
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.17501450; EvalErr[0]PerSample = 0.36445312; TotalTime = 0.0693s; SamplesPerSecond = 36943.5
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.20395966; EvalErr[0]PerSample = 0.36640625; TotalTime = 0.0695s; SamplesPerSecond = 36836.1
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.17673035; EvalErr[0]PerSample = 0.36054687; TotalTime = 0.0693s; SamplesPerSecond = 36940.3
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.22292175; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.0694s; SamplesPerSecond = 36892.4
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.24486694; EvalErr[0]PerSample = 0.38476562; TotalTime = 0.0694s; SamplesPerSecond = 36894.5
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.21594086; EvalErr[0]PerSample = 0.36132812; TotalTime = 0.0693s; SamplesPerSecond = 36952.6
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.16101379; EvalErr[0]PerSample = 0.35703125; TotalTime = 0.0693s; SamplesPerSecond = 36916.3
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.16903076; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.0693s; SamplesPerSecond = 36960.0
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.17054291; EvalErr[0]PerSample = 0.36250000; TotalTime = 0.0694s; SamplesPerSecond = 36903.6
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.15604858; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.0693s; SamplesPerSecond = 36957.4
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.11155853; EvalErr[0]PerSample = 0.35507813; TotalTime = 0.0695s; SamplesPerSecond = 36836.1
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.15468597; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.0695s; SamplesPerSecond = 36836.7
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.13483276; EvalErr[0]PerSample = 0.34648438; TotalTime = 0.0695s; SamplesPerSecond = 36825.0
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.19299622; EvalErr[0]PerSample = 0.35898438; TotalTime = 0.0694s; SamplesPerSecond = 36870.6
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.15268555; EvalErr[0]PerSample = 0.35976562; TotalTime = 0.0694s; SamplesPerSecond = 36884.4
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.22990112; EvalErr[0]PerSample = 0.37968750; TotalTime = 0.0693s; SamplesPerSecond = 36925.4
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.12927246; EvalErr[0]PerSample = 0.34492187; TotalTime = 0.0695s; SamplesPerSecond = 36834.5
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.09533386; EvalErr[0]PerSample = 0.32617188; TotalTime = 0.0695s; SamplesPerSecond = 36839.3
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.16107788; EvalErr[0]PerSample = 0.34140625; TotalTime = 0.0694s; SamplesPerSecond = 36882.8
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.12488708; EvalErr[0]PerSample = 0.34453125; TotalTime = 0.0695s; SamplesPerSecond = 36857.9
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.08076172; EvalErr[0]PerSample = 0.32539062; TotalTime = 0.0696s; SamplesPerSecond = 36794.3
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.10429687; EvalErr[0]PerSample = 0.34335938; TotalTime = 0.0694s; SamplesPerSecond = 36881.2
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1788229; EvalErrPerSample = 0.36074218; AvgLearningRatePerSample = 0.003125; EpochTime=2.22328
CNTKCommandTrainEnd: dptPre1

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	scaledLogLikelihood = Minus
	err = ErrorPrediction
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation

Post-processing network complete.
CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0.

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for gradient computing
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.1250%]: SamplesSeen = 2560; TrainLossPerSample =  4.18252335; EvalErr[0]PerSample = 0.81875000; TotalTime = 0.1232s; SamplesPerSecond = 20770.8
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.2500%]: SamplesSeen = 2560; TrainLossPerSample =  2.75668793; EvalErr[0]PerSample = 0.68046875; TotalTime = 0.1141s; SamplesPerSecond = 22437.8
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.3750%]: SamplesSeen = 2560; TrainLossPerSample =  2.15866089; EvalErr[0]PerSample = 0.58203125; TotalTime = 0.1142s; SamplesPerSecond = 22412.1
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.84255371; EvalErr[0]PerSample = 0.50898438; TotalTime = 0.1141s; SamplesPerSecond = 22445.1
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.68514862; EvalErr[0]PerSample = 0.47226563; TotalTime = 0.1142s; SamplesPerSecond = 22419.0
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.58679276; EvalErr[0]PerSample = 0.44375000; TotalTime = 0.1144s; SamplesPerSecond = 22385.1
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.51864166; EvalErr[0]PerSample = 0.44062500; TotalTime = 0.1140s; SamplesPerSecond = 22450.8
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.47392883; EvalErr[0]PerSample = 0.45000000; TotalTime = 0.1142s; SamplesPerSecond = 22411.5
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.44582214; EvalErr[0]PerSample = 0.43320313; TotalTime = 0.1139s; SamplesPerSecond = 22472.3
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.36554871; EvalErr[0]PerSample = 0.39882812; TotalTime = 0.1142s; SamplesPerSecond = 22416.2
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.36598816; EvalErr[0]PerSample = 0.41953125; TotalTime = 0.1139s; SamplesPerSecond = 22473.1
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.32212830; EvalErr[0]PerSample = 0.38828125; TotalTime = 0.1142s; SamplesPerSecond = 22420.0
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.29261475; EvalErr[0]PerSample = 0.39140625; TotalTime = 0.1139s; SamplesPerSecond = 22470.1
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.28101196; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.1142s; SamplesPerSecond = 22407.6
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.33432007; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.1140s; SamplesPerSecond = 22462.2
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.31024780; EvalErr[0]PerSample = 0.39335938; TotalTime = 0.1142s; SamplesPerSecond = 22412.9
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.27631836; EvalErr[0]PerSample = 0.38710937; TotalTime = 0.1141s; SamplesPerSecond = 22441.4
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.30263062; EvalErr[0]PerSample = 0.38750000; TotalTime = 0.1142s; SamplesPerSecond = 22415.0
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.30517883; EvalErr[0]PerSample = 0.37343750; TotalTime = 0.1141s; SamplesPerSecond = 22437.4
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.23120728; EvalErr[0]PerSample = 0.37031250; TotalTime = 0.1140s; SamplesPerSecond = 22451.2
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.22390747; EvalErr[0]PerSample = 0.37656250; TotalTime = 0.1141s; SamplesPerSecond = 22431.2
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.19273987; EvalErr[0]PerSample = 0.35703125; TotalTime = 0.1140s; SamplesPerSecond = 22455.0
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.23573608; EvalErr[0]PerSample = 0.37070313; TotalTime = 0.1142s; SamplesPerSecond = 22425.8
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.23114929; EvalErr[0]PerSample = 0.36953125; TotalTime = 0.1141s; SamplesPerSecond = 22445.1
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.19703674; EvalErr[0]PerSample = 0.36132812; TotalTime = 0.1143s; SamplesPerSecond = 22397.6
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.14395752; EvalErr[0]PerSample = 0.33867188; TotalTime = 0.1140s; SamplesPerSecond = 22457.9
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.19253235; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.1142s; SamplesPerSecond = 22408.2
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.21293335; EvalErr[0]PerSample = 0.36054687; TotalTime = 0.1140s; SamplesPerSecond = 22447.7
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.15293884; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.1142s; SamplesPerSecond = 22416.8
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.13847656; EvalErr[0]PerSample = 0.33828125; TotalTime = 0.1141s; SamplesPerSecond = 22436.7
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.16446838; EvalErr[0]PerSample = 0.36914062; TotalTime = 0.1141s; SamplesPerSecond = 22439.6
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.19324036; EvalErr[0]PerSample = 0.36367187; TotalTime = 0.1140s; SamplesPerSecond = 22462.6
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.4786586; EvalErrPerSample = 0.41955566; AvgLearningRatePerSample = 0.003125; EpochTime=3.73278
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.16996117; EvalErr[0]PerSample = 0.36250000; TotalTime = 0.1149s; SamplesPerSecond = 22285.3
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.18648243; EvalErr[0]PerSample = 0.36484375; TotalTime = 0.1140s; SamplesPerSecond = 22464.8
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.15900440; EvalErr[0]PerSample = 0.34687500; TotalTime = 0.1142s; SamplesPerSecond = 22421.1
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.16018105; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.1139s; SamplesPerSecond = 22474.3
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.12293968; EvalErr[0]PerSample = 0.33554688; TotalTime = 0.1142s; SamplesPerSecond = 22416.4
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.08400497; EvalErr[0]PerSample = 0.33867188; TotalTime = 0.1139s; SamplesPerSecond = 22472.7
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.18458557; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.1141s; SamplesPerSecond = 22429.6
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.08141022; EvalErr[0]PerSample = 0.33828125; TotalTime = 0.1139s; SamplesPerSecond = 22474.5
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.10440216; EvalErr[0]PerSample = 0.34023437; TotalTime = 0.1141s; SamplesPerSecond = 22436.5
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.10237198; EvalErr[0]PerSample = 0.33515625; TotalTime = 0.1140s; SamplesPerSecond = 22457.7
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.12764435; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.1141s; SamplesPerSecond = 22442.6
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.16334534; EvalErr[0]PerSample = 0.35117188; TotalTime = 0.1141s; SamplesPerSecond = 22440.4
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.10643768; EvalErr[0]PerSample = 0.32734375; TotalTime = 0.1140s; SamplesPerSecond = 22459.1
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.15814972; EvalErr[0]PerSample = 0.35507813; TotalTime = 0.1141s; SamplesPerSecond = 22439.2
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.14853821; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.1140s; SamplesPerSecond = 22466.0
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.12832794; EvalErr[0]PerSample = 0.33750000; TotalTime = 0.1141s; SamplesPerSecond = 22442.2
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.08540955; EvalErr[0]PerSample = 0.32734375; TotalTime = 0.1140s; SamplesPerSecond = 22449.4
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.09871216; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.1142s; SamplesPerSecond = 22409.4
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.10895844; EvalErr[0]PerSample = 0.34687500; TotalTime = 0.1139s; SamplesPerSecond = 22471.9
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.13967285; EvalErr[0]PerSample = 0.35781250; TotalTime = 0.1142s; SamplesPerSecond = 22414.5
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.09966583; EvalErr[0]PerSample = 0.34218750; TotalTime = 0.1139s; SamplesPerSecond = 22482.4
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.12695770; EvalErr[0]PerSample = 0.34765625; TotalTime = 0.1142s; SamplesPerSecond = 22412.7
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.09246368; EvalErr[0]PerSample = 0.33281250; TotalTime = 0.1140s; SamplesPerSecond = 22460.1
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.13705750; EvalErr[0]PerSample = 0.33984375; TotalTime = 0.1142s; SamplesPerSecond = 22408.4
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.08877258; EvalErr[0]PerSample = 0.33398438; TotalTime = 0.1139s; SamplesPerSecond = 22472.9
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.13309021; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.1142s; SamplesPerSecond = 22421.5
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.06476746; EvalErr[0]PerSample = 0.32500000; TotalTime = 0.1141s; SamplesPerSecond = 22443.5
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.05253601; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.1142s; SamplesPerSecond = 22415.4
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.09331360; EvalErr[0]PerSample = 0.33789062; TotalTime = 0.1139s; SamplesPerSecond = 22466.2
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.08100281; EvalErr[0]PerSample = 0.32773438; TotalTime = 0.1141s; SamplesPerSecond = 22431.5
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.05775757; EvalErr[0]PerSample = 0.32695313; TotalTime = 0.1139s; SamplesPerSecond = 22476.4
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.09493713; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.1142s; SamplesPerSecond = 22425.6
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1169645; EvalErrPerSample = 0.3416504; AvgLearningRatePerSample = 0.003125; EpochTime=3.65247
CNTKCommandTrainEnd: dptPre2

Post-processing network...

3 roots:
	scaledLogLikelihood = Minus
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 11 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation

Post-processing network complete.
CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151210155756.754205/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0.

Post-processing network...

3 roots:
	err = ErrorPrediction
	cr = CrossEntropyWithSoftmax
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 13 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.


Validating for node err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation


Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.


Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node scaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 13 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for gradient computing
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10, 3.1250%]: SamplesSeen = 2560; TrainLossPerSample =  4.19780846; EvalErr[0]PerSample = 0.85117188; TotalTime = 0.1626s; SamplesPerSecond = 15746.1
 Epoch[ 1 of 4]-Minibatch[  11-  20, 6.2500%]: SamplesSeen = 2560; TrainLossPerSample =  2.64861336; EvalErr[0]PerSample = 0.66406250; TotalTime = 0.1589s; SamplesPerSecond = 16107.9
 Epoch[ 1 of 4]-Minibatch[  21-  30, 9.3750%]: SamplesSeen = 2560; TrainLossPerSample =  2.01158600; EvalErr[0]PerSample = 0.53828125; TotalTime = 0.1589s; SamplesPerSecond = 16114.0
 Epoch[ 1 of 4]-Minibatch[  31-  40, 12.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.72842636; EvalErr[0]PerSample = 0.47734375; TotalTime = 0.1592s; SamplesPerSecond = 16084.0
 Epoch[ 1 of 4]-Minibatch[  41-  50, 15.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.58273087; EvalErr[0]PerSample = 0.43437500; TotalTime = 0.1587s; SamplesPerSecond = 16132.1
 Epoch[ 1 of 4]-Minibatch[  51-  60, 18.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.47992554; EvalErr[0]PerSample = 0.41250000; TotalTime = 0.1592s; SamplesPerSecond = 16077.9
 Epoch[ 1 of 4]-Minibatch[  61-  70, 21.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.42155914; EvalErr[0]PerSample = 0.41289063; TotalTime = 0.1588s; SamplesPerSecond = 16121.8
 Epoch[ 1 of 4]-Minibatch[  71-  80, 25.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.36949768; EvalErr[0]PerSample = 0.41445312; TotalTime = 0.1589s; SamplesPerSecond = 16108.3
 Epoch[ 1 of 4]-Minibatch[  81-  90, 28.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.34786224; EvalErr[0]PerSample = 0.40507813; TotalTime = 0.1589s; SamplesPerSecond = 16111.9
 Epoch[ 1 of 4]-Minibatch[  91- 100, 31.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.27002563; EvalErr[0]PerSample = 0.36875000; TotalTime = 0.1590s; SamplesPerSecond = 16098.3
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.28982849; EvalErr[0]PerSample = 0.39882812; TotalTime = 0.1588s; SamplesPerSecond = 16124.2
 Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.24182129; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.1590s; SamplesPerSecond = 16101.0
 Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.20470581; EvalErr[0]PerSample = 0.36835937; TotalTime = 0.1588s; SamplesPerSecond = 16122.3
 Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.20108948; EvalErr[0]PerSample = 0.36289063; TotalTime = 0.1592s; SamplesPerSecond = 16084.4
 Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.26263428; EvalErr[0]PerSample = 0.35898438; TotalTime = 0.1588s; SamplesPerSecond = 16122.6
 Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.24391479; EvalErr[0]PerSample = 0.36289063; TotalTime = 0.1591s; SamplesPerSecond = 16089.6
 Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.21484070; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.1592s; SamplesPerSecond = 16081.3
 Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.24012756; EvalErr[0]PerSample = 0.37304688; TotalTime = 0.1589s; SamplesPerSecond = 16114.2
 Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.24010315; EvalErr[0]PerSample = 0.35664062; TotalTime = 0.1590s; SamplesPerSecond = 16099.8
 Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.16525574; EvalErr[0]PerSample = 0.35156250; TotalTime = 0.1589s; SamplesPerSecond = 16111.8
 Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.16553650; EvalErr[0]PerSample = 0.35625000; TotalTime = 0.1592s; SamplesPerSecond = 16082.2
 Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.13382568; EvalErr[0]PerSample = 0.34414062; TotalTime = 0.1587s; SamplesPerSecond = 16129.7
 Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.18659058; EvalErr[0]PerSample = 0.36367187; TotalTime = 0.1592s; SamplesPerSecond = 16075.8
 Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.18215942; EvalErr[0]PerSample = 0.35390625; TotalTime = 0.1587s; SamplesPerSecond = 16127.7
 Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.1250%]: SamplesSeen = 2560; TrainLossPerSample =  1.13978882; EvalErr[0]PerSample = 0.33906250; TotalTime = 0.1596s; SamplesPerSecond = 16044.3
 Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.2500%]: SamplesSeen = 2560; TrainLossPerSample =  1.09225464; EvalErr[0]PerSample = 0.32539062; TotalTime = 0.1599s; SamplesPerSecond = 16008.5
 Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.3750%]: SamplesSeen = 2560; TrainLossPerSample =  1.13308105; EvalErr[0]PerSample = 0.34375000; TotalTime = 0.1602s; SamplesPerSecond = 15983.9
 Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.5000%]: SamplesSeen = 2560; TrainLossPerSample =  1.16348877; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.1602s; SamplesPerSecond = 15978.6
 Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.6250%]: SamplesSeen = 2560; TrainLossPerSample =  1.11323242; EvalErr[0]PerSample = 0.33984375; TotalTime = 0.1598s; SamplesPerSecond = 16022.6
 Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.7500%]: SamplesSeen = 2560; TrainLossPerSample =  1.09819641; EvalErr[0]PerSample = 0.32617188; TotalTime = 0.1591s; SamplesPerSecond = 16088.8
 Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.8750%]: SamplesSeen = 2560; TrainLossPerSample =  1.12811279; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.1587s; SamplesPerSecond = 16132.5
 Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.0000%]: SamplesSeen = 2560; TrainLossPerSample =  1.14795227; EvalErr[0]PerSample = 0.34726563; TotalTime = 0.1590s; SamplesPerSecond = 16097.5
Finished Epoch[ 1 of 4]: [Training Set] TrainLossPerSample = 1.4077055; EvalErrPerSample = 0.39998779; AvgLearningRatePerSample = 0.003125; EpochTime=5.16599
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10, 6.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.37476654; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.2688s; SamplesPerSecond = 19046.3
 Epoch[ 2 of 4]-Minibatch[  11-  20, 12.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.52002716; EvalErr[0]PerSample = 0.41230469; TotalTime = 0.2652s; SamplesPerSecond = 19302.7
 Epoch[ 2 of 4]-Minibatch[  21-  30, 18.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.16951942; EvalErr[0]PerSample = 0.35957031; TotalTime = 0.2649s; SamplesPerSecond = 19329.7
 Epoch[ 2 of 4]-Minibatch[  31-  40, 25.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.12974701; EvalErr[0]PerSample = 0.34003906; TotalTime = 0.2651s; SamplesPerSecond = 19310.3
 Epoch[ 2 of 4]-Minibatch[  41-  50, 31.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.09153595; EvalErr[0]PerSample = 0.33554688; TotalTime = 0.2650s; SamplesPerSecond = 19318.2
 Epoch[ 2 of 4]-Minibatch[  51-  60, 37.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.13825188; EvalErr[0]PerSample = 0.35371094; TotalTime = 0.2650s; SamplesPerSecond = 19321.4
 Epoch[ 2 of 4]-Minibatch[  61-  70, 43.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.12816467; EvalErr[0]PerSample = 0.34433594; TotalTime = 0.2648s; SamplesPerSecond = 19336.7
 Epoch[ 2 of 4]-Minibatch[  71-  80, 50.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.14659424; EvalErr[0]PerSample = 0.34804687; TotalTime = 0.2651s; SamplesPerSecond = 19314.6
 Epoch[ 2 of 4]-Minibatch[  81-  90, 56.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.10215683; EvalErr[0]PerSample = 0.33964844; TotalTime = 0.2650s; SamplesPerSecond = 19323.5
 Epoch[ 2 of 4]-Minibatch[  91- 100, 62.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.15161133; EvalErr[0]PerSample = 0.36054687; TotalTime = 0.2647s; SamplesPerSecond = 19345.1
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.10864639; EvalErr[0]PerSample = 0.34687500; TotalTime = 0.2652s; SamplesPerSecond = 19307.5
 Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.09600677; EvalErr[0]PerSample = 0.33593750; TotalTime = 0.2648s; SamplesPerSecond = 19333.4
 Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.09834747; EvalErr[0]PerSample = 0.33242187; TotalTime = 0.2650s; SamplesPerSecond = 19317.3
 Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.05377350; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.2648s; SamplesPerSecond = 19335.0
 Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.08458405; EvalErr[0]PerSample = 0.32910156; TotalTime = 0.2650s; SamplesPerSecond = 19322.6
 Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.07811737; EvalErr[0]PerSample = 0.33632812; TotalTime = 0.2649s; SamplesPerSecond = 19326.7
Finished Epoch[ 2 of 4]: [Training Set] TrainLossPerSample = 1.1544907; EvalErrPerSample = 0.34937745; AvgLearningRatePerSample = 0.003125; EpochTime=4.24537
Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10, 6.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.10079184; EvalErr[0]PerSample = 0.33261719; TotalTime = 0.2652s; SamplesPerSecond = 19304.7
 Epoch[ 3 of 4]-Minibatch[  11-  20, 12.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.25790491; EvalErr[0]PerSample = 0.37890625; TotalTime = 0.2651s; SamplesPerSecond = 19316.4
 Epoch[ 3 of 4]-Minibatch[  21-  30, 18.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.18565598; EvalErr[0]PerSample = 0.36230469; TotalTime = 0.2650s; SamplesPerSecond = 19319.7
 Epoch[ 3 of 4]-Minibatch[  31-  40, 25.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.10303345; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.2651s; SamplesPerSecond = 19312.5
 Epoch[ 3 of 4]-Minibatch[  41-  50, 31.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.07204132; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.2647s; SamplesPerSecond = 19339.7
 Epoch[ 3 of 4]-Minibatch[  51-  60, 37.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.08541183; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.2651s; SamplesPerSecond = 19316.9
 Epoch[ 3 of 4]-Minibatch[  61-  70, 43.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.02934418; EvalErr[0]PerSample = 0.32910156; TotalTime = 0.2649s; SamplesPerSecond = 19324.9
 Epoch[ 3 of 4]-Minibatch[  71-  80, 50.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.08052292; EvalErr[0]PerSample = 0.32949219; TotalTime = 0.2651s; SamplesPerSecond = 19314.3
 Epoch[ 3 of 4]-Minibatch[  81-  90, 56.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.04220352; EvalErr[0]PerSample = 0.32636719; TotalTime = 0.2651s; SamplesPerSecond = 19315.1
 Epoch[ 3 of 4]-Minibatch[  91- 100, 62.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.08314514; EvalErr[0]PerSample = 0.33007812; TotalTime = 0.2650s; SamplesPerSecond = 19324.3
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.04539642; EvalErr[0]PerSample = 0.32714844; TotalTime = 0.2648s; SamplesPerSecond = 19333.1
 Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.04447403; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.2651s; SamplesPerSecond = 19311.7
 Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.04384003; EvalErr[0]PerSample = 0.32089844; TotalTime = 0.2650s; SamplesPerSecond = 19320.6
 Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.02437286; EvalErr[0]PerSample = 0.31796875; TotalTime = 0.2649s; SamplesPerSecond = 19325.9
 Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.04496155; EvalErr[0]PerSample = 0.32382813; TotalTime = 0.2651s; SamplesPerSecond = 19313.9
 Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.05646667; EvalErr[0]PerSample = 0.33281250; TotalTime = 0.2648s; SamplesPerSecond = 19332.1
Finished Epoch[ 3 of 4]: [Training Set] TrainLossPerSample = 1.0812229; EvalErrPerSample = 0.33406982; AvgLearningRatePerSample = 0.003125; EpochTime=4.24195
Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10, 6.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.06878595; EvalErr[0]PerSample = 0.32871094; TotalTime = 0.2654s; SamplesPerSecond = 19289.8
 Epoch[ 4 of 4]-Minibatch[  11-  20, 12.5000%]: SamplesSeen = 4926; TrainLossPerSample =  1.04344008; EvalErr[0]PerSample = 0.32947625; TotalTime = 0.2858s; SamplesPerSecond = 17235.8
 Epoch[ 4 of 4]-Minibatch[  21-  30, 18.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.04028511; EvalErr[0]PerSample = 0.32929687; TotalTime = 0.2650s; SamplesPerSecond = 19324.1
 Epoch[ 4 of 4]-Minibatch[  31-  40, 25.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.01357346; EvalErr[0]PerSample = 0.31621094; TotalTime = 0.2651s; SamplesPerSecond = 19315.4
 Epoch[ 4 of 4]-Minibatch[  41-  50, 31.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.04428215; EvalErr[0]PerSample = 0.32070312; TotalTime = 0.2651s; SamplesPerSecond = 19311.7
 Epoch[ 4 of 4]-Minibatch[  51-  60, 37.5000%]: SamplesSeen = 5120; TrainLossPerSample =  0.98517265; EvalErr[0]PerSample = 0.30312500; TotalTime = 0.2649s; SamplesPerSecond = 19325.4
 Epoch[ 4 of 4]-Minibatch[  61-  70, 43.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.01726456; EvalErr[0]PerSample = 0.31210938; TotalTime = 0.2681s; SamplesPerSecond = 19093.9
 Epoch[ 4 of 4]-Minibatch[  71-  80, 50.0000%]: SamplesSeen = 5120; TrainLossPerSample =  0.97967148; EvalErr[0]PerSample = 0.31191406; TotalTime = 0.2694s; SamplesPerSecond = 19003.9
 Epoch[ 4 of 4]-Minibatch[  81-  90, 56.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.07085876; EvalErr[0]PerSample = 0.34003906; TotalTime = 0.2691s; SamplesPerSecond = 19025.3
 Epoch[ 4 of 4]-Minibatch[  91- 100, 62.5000%]: SamplesSeen = 5120; TrainLossPerSample =  1.00917587; EvalErr[0]PerSample = 0.31601563; TotalTime = 0.2693s; SamplesPerSecond = 19014.8
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.7500%]: SamplesSeen = 5120; TrainLossPerSample =  0.99082108; EvalErr[0]PerSample = 0.30644531; TotalTime = 0.2691s; SamplesPerSecond = 19024.1
 Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.0000%]: SamplesSeen = 5120; TrainLossPerSample =  0.96312790; EvalErr[0]PerSample = 0.30351563; TotalTime = 0.2696s; SamplesPerSecond = 18992.4
 Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.2500%]: SamplesSeen = 5120; TrainLossPerSample =  1.00613327; EvalErr[0]PerSample = 0.31699219; TotalTime = 0.2695s; SamplesPerSecond = 18997.6
 Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.5000%]: SamplesSeen = 5120; TrainLossPerSample =  0.98339691; EvalErr[0]PerSample = 0.30292969; TotalTime = 0.2695s; SamplesPerSecond = 18999.4
 Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.7500%]: SamplesSeen = 5120; TrainLossPerSample =  1.00519562; EvalErr[0]PerSample = 0.31250000; TotalTime = 0.2698s; SamplesPerSecond = 18975.7
 Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.0000%]: SamplesSeen = 5120; TrainLossPerSample =  1.01431122; EvalErr[0]PerSample = 0.31816406; TotalTime = 0.2706s; SamplesPerSecond = 18924.3
Finished Epoch[ 4 of 4]: [Training Set] TrainLossPerSample = 1.0146079; EvalErrPerSample = 0.31669924; AvgLearningRatePerSample = 0.003125; EpochTime=4.32286
CNTKCommandTrainEnd: speechTrain
COMPLETED