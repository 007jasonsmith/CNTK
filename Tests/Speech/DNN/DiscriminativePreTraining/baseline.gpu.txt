=== Running /home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining DeviceId=0
-------------------------------------------------------------------
Build info: 

		Built time: Dec 15 2015 16:32:52
		Last modified date: Tue Dec 15 16:31:42 2015
		Build type: release
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.0
		CUB_PATH: /usr/local/cub-1.4.1
		Build Branch: master
		Build SHA1: 5e0017ac9c55c23d53cb524c8acb7d6d9bfd0269
-------------------------------------------------------------------
running on localhost at 2015/12/15 16:49:25
command line: 
/home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining DeviceId=0 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.config:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.config:ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
configparameters: cntk_dpt.config:currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk_dpt.config:DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk_dpt.config:deviceId=0
configparameters: cntk_dpt.config:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.config:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.config:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.config:ndlMacros=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.config:precision=float
configparameters: cntk_dpt.config:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.config:RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.config:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.config:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.config:traceLevel=1
<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain 
precision = float
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
CNTKCommandTrainInfo: dptPre1 : 2
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
CNTKCommandTrainInfo: dptPre2 : 2
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
CNTKCommandTrainInfo: speechTrain : 4
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8
CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  3.80342407; EvalErr[0]PerSample = 0.83125000; TotalTime = 0.2106s; SamplesPerSecond = 12155.5
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.82046280; EvalErr[0]PerSample = 0.68125000; TotalTime = 0.0713s; SamplesPerSecond = 35925.3
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.52124176; EvalErr[0]PerSample = 0.64179688; TotalTime = 0.0702s; SamplesPerSecond = 36458.9
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.20756912; EvalErr[0]PerSample = 0.58085937; TotalTime = 0.0704s; SamplesPerSecond = 36375.0
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  2.04683380; EvalErr[0]PerSample = 0.55976563; TotalTime = 0.0705s; SamplesPerSecond = 36336.8
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.91002655; EvalErr[0]PerSample = 0.52773437; TotalTime = 0.0702s; SamplesPerSecond = 36466.7
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.83807220; EvalErr[0]PerSample = 0.51406250; TotalTime = 0.0702s; SamplesPerSecond = 36447.0
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.75461121; EvalErr[0]PerSample = 0.49648437; TotalTime = 0.0704s; SamplesPerSecond = 36358.5
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.65055847; EvalErr[0]PerSample = 0.49687500; TotalTime = 0.0703s; SamplesPerSecond = 36436.1
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.64387207; EvalErr[0]PerSample = 0.47773437; TotalTime = 0.0702s; SamplesPerSecond = 36443.4
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.58492737; EvalErr[0]PerSample = 0.46914062; TotalTime = 0.0705s; SamplesPerSecond = 36320.3
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.59035645; EvalErr[0]PerSample = 0.46093750; TotalTime = 0.0703s; SamplesPerSecond = 36428.3
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.47140961; EvalErr[0]PerSample = 0.44179687; TotalTime = 0.0703s; SamplesPerSecond = 36400.3
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.43652649; EvalErr[0]PerSample = 0.43789062; TotalTime = 0.0704s; SamplesPerSecond = 36349.2
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.42555847; EvalErr[0]PerSample = 0.42031250; TotalTime = 0.0704s; SamplesPerSecond = 36370.9
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.42143250; EvalErr[0]PerSample = 0.42031250; TotalTime = 0.0702s; SamplesPerSecond = 36476.6
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.37999268; EvalErr[0]PerSample = 0.39921875; TotalTime = 0.0704s; SamplesPerSecond = 36370.9
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.40045471; EvalErr[0]PerSample = 0.41562500; TotalTime = 0.0705s; SamplesPerSecond = 36319.8
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.32805786; EvalErr[0]PerSample = 0.39023438; TotalTime = 0.0703s; SamplesPerSecond = 36424.7
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.36459045; EvalErr[0]PerSample = 0.40898438; TotalTime = 0.0702s; SamplesPerSecond = 36472.4
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.38917236; EvalErr[0]PerSample = 0.42187500; TotalTime = 0.0703s; SamplesPerSecond = 36391.6
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.41898804; EvalErr[0]PerSample = 0.42148438; TotalTime = 0.0703s; SamplesPerSecond = 36416.4
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.32540894; EvalErr[0]PerSample = 0.39335938; TotalTime = 0.0702s; SamplesPerSecond = 36474.0
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.32970886; EvalErr[0]PerSample = 0.39453125; TotalTime = 0.0704s; SamplesPerSecond = 36356.4
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.33233337; EvalErr[0]PerSample = 0.40390625; TotalTime = 0.0702s; SamplesPerSecond = 36455.8
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.33637085; EvalErr[0]PerSample = 0.40000000; TotalTime = 0.0703s; SamplesPerSecond = 36421.6
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.28219604; EvalErr[0]PerSample = 0.40468750; TotalTime = 0.0704s; SamplesPerSecond = 36358.5
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.25918274; EvalErr[0]PerSample = 0.38164063; TotalTime = 0.0702s; SamplesPerSecond = 36453.2
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.26340637; EvalErr[0]PerSample = 0.37304688; TotalTime = 0.0702s; SamplesPerSecond = 36484.9
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.27405396; EvalErr[0]PerSample = 0.38515625; TotalTime = 0.0703s; SamplesPerSecond = 36405.0
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.31404419; EvalErr[0]PerSample = 0.40312500; TotalTime = 0.0701s; SamplesPerSecond = 36494.8
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.30253601; EvalErr[0]PerSample = 0.39296875; TotalTime = 0.0703s; SamplesPerSecond = 36428.8
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.6383556; EvalErrPerSample = 0.46400148; AvgLearningRatePerSample = 0.003125; EpochTime=2.46626
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.24688988; EvalErr[0]PerSample = 0.38398437; TotalTime = 0.0712s; SamplesPerSecond = 35962.6
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.27934895; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.0702s; SamplesPerSecond = 36451.1
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.25806255; EvalErr[0]PerSample = 0.37812500; TotalTime = 0.0702s; SamplesPerSecond = 36467.2
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.24357872; EvalErr[0]PerSample = 0.38398437; TotalTime = 0.0701s; SamplesPerSecond = 36500.0
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.27939835; EvalErr[0]PerSample = 0.38437500; TotalTime = 0.0703s; SamplesPerSecond = 36415.4
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.19105911; EvalErr[0]PerSample = 0.36406250; TotalTime = 0.0703s; SamplesPerSecond = 36435.6
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.19511108; EvalErr[0]PerSample = 0.36367187; TotalTime = 0.0701s; SamplesPerSecond = 36493.7
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.21830215; EvalErr[0]PerSample = 0.38203125; TotalTime = 0.0703s; SamplesPerSecond = 36404.5
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.23375244; EvalErr[0]PerSample = 0.37539062; TotalTime = 0.0702s; SamplesPerSecond = 36454.8
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.20884781; EvalErr[0]PerSample = 0.35429688; TotalTime = 0.0703s; SamplesPerSecond = 36436.1
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.18095932; EvalErr[0]PerSample = 0.36250000; TotalTime = 0.0704s; SamplesPerSecond = 36377.1
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.18072357; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.0702s; SamplesPerSecond = 36477.6
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.18027191; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.0702s; SamplesPerSecond = 36446.0
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.14044342; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.0702s; SamplesPerSecond = 36474.0
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.20970154; EvalErr[0]PerSample = 0.36054687; TotalTime = 0.0701s; SamplesPerSecond = 36538.5
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.10420685; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.0704s; SamplesPerSecond = 36353.8
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.11220551; EvalErr[0]PerSample = 0.34375000; TotalTime = 0.0702s; SamplesPerSecond = 36454.8
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.17280579; EvalErr[0]PerSample = 0.34453125; TotalTime = 0.0702s; SamplesPerSecond = 36452.2
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.12581482; EvalErr[0]PerSample = 0.34492187; TotalTime = 0.0705s; SamplesPerSecond = 36332.7
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.11171570; EvalErr[0]PerSample = 0.33867188; TotalTime = 0.0702s; SamplesPerSecond = 36479.7
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.12338867; EvalErr[0]PerSample = 0.35390625; TotalTime = 0.0702s; SamplesPerSecond = 36478.1
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.12327576; EvalErr[0]PerSample = 0.34687500; TotalTime = 0.0702s; SamplesPerSecond = 36492.2
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.14758911; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.0701s; SamplesPerSecond = 36496.9
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.13156433; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.0704s; SamplesPerSecond = 36359.0
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.06281128; EvalErr[0]PerSample = 0.32226562; TotalTime = 0.0702s; SamplesPerSecond = 36446.0
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.16522217; EvalErr[0]PerSample = 0.34921875; TotalTime = 0.0701s; SamplesPerSecond = 36494.8
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.16606140; EvalErr[0]PerSample = 0.35468750; TotalTime = 0.0704s; SamplesPerSecond = 36368.8
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.15579834; EvalErr[0]PerSample = 0.35664062; TotalTime = 0.0702s; SamplesPerSecond = 36472.4
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.08480530; EvalErr[0]PerSample = 0.33359375; TotalTime = 0.0701s; SamplesPerSecond = 36513.0
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.11228638; EvalErr[0]PerSample = 0.35039063; TotalTime = 0.0702s; SamplesPerSecond = 36473.0
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.14930725; EvalErr[0]PerSample = 0.33554688; TotalTime = 0.0701s; SamplesPerSecond = 36514.0
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.04487610; EvalErr[0]PerSample = 0.32421875; TotalTime = 0.0705s; SamplesPerSecond = 36330.6
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1668808; EvalErrPerSample = 0.35670167; AvgLearningRatePerSample = 0.003125; EpochTime=2.25424
CNTKCommandTrainEnd: dptPre1

Post-processing network...

3 roots:
	err = ErrorPrediction
	cr = CrossEntropyWithSoftmax
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.

Post-processing network complete.
CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0.

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  4.91810837; EvalErr[0]PerSample = 0.82031250; TotalTime = 0.1190s; SamplesPerSecond = 21512.4
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.77993393; EvalErr[0]PerSample = 0.69765625; TotalTime = 0.1152s; SamplesPerSecond = 22222.2
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.43313293; EvalErr[0]PerSample = 0.62500000; TotalTime = 0.1154s; SamplesPerSecond = 22187.2
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.02324753; EvalErr[0]PerSample = 0.56601563; TotalTime = 0.1154s; SamplesPerSecond = 22183.1
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.79592819; EvalErr[0]PerSample = 0.50273437; TotalTime = 0.1153s; SamplesPerSecond = 22207.2
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.63825836; EvalErr[0]PerSample = 0.46445313; TotalTime = 0.1152s; SamplesPerSecond = 22231.7
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.59271851; EvalErr[0]PerSample = 0.46796875; TotalTime = 0.1153s; SamplesPerSecond = 22207.4
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.53959503; EvalErr[0]PerSample = 0.44375000; TotalTime = 0.1152s; SamplesPerSecond = 22228.6
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.44679413; EvalErr[0]PerSample = 0.43554688; TotalTime = 0.1153s; SamplesPerSecond = 22199.1
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.48092804; EvalErr[0]PerSample = 0.45156250; TotalTime = 0.1152s; SamplesPerSecond = 22215.5
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.39338531; EvalErr[0]PerSample = 0.41523437; TotalTime = 0.1152s; SamplesPerSecond = 22229.6
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.37537994; EvalErr[0]PerSample = 0.40195313; TotalTime = 0.1153s; SamplesPerSecond = 22203.7
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.31613159; EvalErr[0]PerSample = 0.39023438; TotalTime = 0.1153s; SamplesPerSecond = 22198.1
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.28457947; EvalErr[0]PerSample = 0.38867188; TotalTime = 0.1152s; SamplesPerSecond = 22225.1
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.30657959; EvalErr[0]PerSample = 0.39296875; TotalTime = 0.1151s; SamplesPerSecond = 22242.9
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.31732483; EvalErr[0]PerSample = 0.39296875; TotalTime = 0.1154s; SamplesPerSecond = 22188.9
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.25568237; EvalErr[0]PerSample = 0.37304688; TotalTime = 0.1153s; SamplesPerSecond = 22203.3
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.28210754; EvalErr[0]PerSample = 0.37695312; TotalTime = 0.1151s; SamplesPerSecond = 22238.6
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.25090332; EvalErr[0]PerSample = 0.38046875; TotalTime = 0.1154s; SamplesPerSecond = 22182.7
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.27268677; EvalErr[0]PerSample = 0.39062500; TotalTime = 0.1152s; SamplesPerSecond = 22216.4
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.25652466; EvalErr[0]PerSample = 0.38007812; TotalTime = 0.1152s; SamplesPerSecond = 22222.2
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.28724365; EvalErr[0]PerSample = 0.37695312; TotalTime = 0.1153s; SamplesPerSecond = 22196.4
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.21982117; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.1152s; SamplesPerSecond = 22229.2
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.25404358; EvalErr[0]PerSample = 0.37031250; TotalTime = 0.1152s; SamplesPerSecond = 22223.8
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.23924561; EvalErr[0]PerSample = 0.37773438; TotalTime = 0.1153s; SamplesPerSecond = 22204.3
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.22186279; EvalErr[0]PerSample = 0.37929687; TotalTime = 0.1163s; SamplesPerSecond = 22016.4
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.20288391; EvalErr[0]PerSample = 0.37812500; TotalTime = 0.1151s; SamplesPerSecond = 22242.1
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.19831848; EvalErr[0]PerSample = 0.38007812; TotalTime = 0.1152s; SamplesPerSecond = 22225.3
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.19112244; EvalErr[0]PerSample = 0.36132812; TotalTime = 0.1153s; SamplesPerSecond = 22194.7
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.17488098; EvalErr[0]PerSample = 0.36210938; TotalTime = 0.1152s; SamplesPerSecond = 22229.0
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.20837708; EvalErr[0]PerSample = 0.36484375; TotalTime = 0.1151s; SamplesPerSecond = 22234.6
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.17091370; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.1152s; SamplesPerSecond = 22216.6
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.5415201; EvalErrPerSample = 0.4324463; AvgLearningRatePerSample = 0.003125; EpochTime=3.80174
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.18974009; EvalErr[0]PerSample = 0.35742188; TotalTime = 0.1161s; SamplesPerSecond = 22058.5
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.21057482; EvalErr[0]PerSample = 0.36992188; TotalTime = 0.1152s; SamplesPerSecond = 22217.6
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.18170261; EvalErr[0]PerSample = 0.35781250; TotalTime = 0.1150s; SamplesPerSecond = 22257.6
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.16365662; EvalErr[0]PerSample = 0.35390625; TotalTime = 0.1153s; SamplesPerSecond = 22204.5
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.19693451; EvalErr[0]PerSample = 0.34570312; TotalTime = 0.1153s; SamplesPerSecond = 22197.4
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.13508606; EvalErr[0]PerSample = 0.34609375; TotalTime = 0.1151s; SamplesPerSecond = 22246.9
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.16446762; EvalErr[0]PerSample = 0.35468750; TotalTime = 0.1151s; SamplesPerSecond = 22239.6
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.17567062; EvalErr[0]PerSample = 0.36054687; TotalTime = 0.1151s; SamplesPerSecond = 22249.5
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.19057159; EvalErr[0]PerSample = 0.36835937; TotalTime = 0.1153s; SamplesPerSecond = 22210.5
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.16688461; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.1150s; SamplesPerSecond = 22252.0
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.15200882; EvalErr[0]PerSample = 0.34765625; TotalTime = 0.1151s; SamplesPerSecond = 22238.8
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.14322815; EvalErr[0]PerSample = 0.34023437; TotalTime = 0.1152s; SamplesPerSecond = 22219.3
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.14062195; EvalErr[0]PerSample = 0.34062500; TotalTime = 0.1150s; SamplesPerSecond = 22254.5
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.11449127; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.1152s; SamplesPerSecond = 22222.2
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.09423523; EvalErr[0]PerSample = 0.32265625; TotalTime = 0.1151s; SamplesPerSecond = 22235.0
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.08985901; EvalErr[0]PerSample = 0.34335938; TotalTime = 0.1158s; SamplesPerSecond = 22102.9
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.09220886; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.1171s; SamplesPerSecond = 21867.1
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.09957428; EvalErr[0]PerSample = 0.32734375; TotalTime = 0.1173s; SamplesPerSecond = 21833.1
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.08864288; EvalErr[0]PerSample = 0.33281250; TotalTime = 0.1169s; SamplesPerSecond = 21900.4
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.08203583; EvalErr[0]PerSample = 0.32382813; TotalTime = 0.1169s; SamplesPerSecond = 21896.6
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.07147827; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.1169s; SamplesPerSecond = 21907.3
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.06768951; EvalErr[0]PerSample = 0.31992188; TotalTime = 0.1172s; SamplesPerSecond = 21844.3
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.07778778; EvalErr[0]PerSample = 0.32812500; TotalTime = 0.1169s; SamplesPerSecond = 21892.9
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.08068237; EvalErr[0]PerSample = 0.33320312; TotalTime = 0.1171s; SamplesPerSecond = 21865.4
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.04742737; EvalErr[0]PerSample = 0.32343750; TotalTime = 0.1167s; SamplesPerSecond = 21928.5
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.11270142; EvalErr[0]PerSample = 0.34140625; TotalTime = 0.1169s; SamplesPerSecond = 21900.9
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.11357422; EvalErr[0]PerSample = 0.33906250; TotalTime = 0.1172s; SamplesPerSecond = 21834.6
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.11867676; EvalErr[0]PerSample = 0.34218750; TotalTime = 0.1172s; SamplesPerSecond = 21836.1
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.06671143; EvalErr[0]PerSample = 0.31992188; TotalTime = 0.1169s; SamplesPerSecond = 21894.2
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.07430115; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.1173s; SamplesPerSecond = 21832.0
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.13439331; EvalErr[0]PerSample = 0.33437500; TotalTime = 0.1170s; SamplesPerSecond = 21874.4
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.03737488; EvalErr[0]PerSample = 0.32695313; TotalTime = 0.1170s; SamplesPerSecond = 21885.4
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1210936; EvalErrPerSample = 0.34122315; AvgLearningRatePerSample = 0.003125; EpochTime=3.72221
CNTKCommandTrainEnd: dptPre2

Post-processing network...

3 roots:
	err = ErrorPrediction
	cr = CrossEntropyWithSoftmax
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.

Post-processing network complete.
CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0.

Post-processing network...

3 roots:
	scaledLogLikelihood = Minus
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node scaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 14 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.


Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.


Validating for node err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  4.20781937; EvalErr[0]PerSample = 0.83476562; TotalTime = 0.1647s; SamplesPerSecond = 15546.5
 Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.55169449; EvalErr[0]PerSample = 0.63945312; TotalTime = 0.1601s; SamplesPerSecond = 15992.7
 Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.11914444; EvalErr[0]PerSample = 0.56718750; TotalTime = 0.1605s; SamplesPerSecond = 15948.3
 Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.78704681; EvalErr[0]PerSample = 0.50195312; TotalTime = 0.1601s; SamplesPerSecond = 15988.7
 Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.59841156; EvalErr[0]PerSample = 0.46093750; TotalTime = 0.1605s; SamplesPerSecond = 15948.9
 Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.46838608; EvalErr[0]PerSample = 0.42460938; TotalTime = 0.1603s; SamplesPerSecond = 15966.0
 Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.44410706; EvalErr[0]PerSample = 0.42343750; TotalTime = 0.1601s; SamplesPerSecond = 15987.4
 Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.40599060; EvalErr[0]PerSample = 0.41406250; TotalTime = 0.1603s; SamplesPerSecond = 15969.4
 Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.31691589; EvalErr[0]PerSample = 0.39335938; TotalTime = 0.1602s; SamplesPerSecond = 15984.3
 Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.32945862; EvalErr[0]PerSample = 0.39882812; TotalTime = 0.1603s; SamplesPerSecond = 15973.4
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.27787933; EvalErr[0]PerSample = 0.38320312; TotalTime = 0.1606s; SamplesPerSecond = 15938.7
 Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.28078308; EvalErr[0]PerSample = 0.38046875; TotalTime = 0.1601s; SamplesPerSecond = 15990.1
 Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.21031036; EvalErr[0]PerSample = 0.35273437; TotalTime = 0.1607s; SamplesPerSecond = 15934.5
 Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.18942261; EvalErr[0]PerSample = 0.35742188; TotalTime = 0.1603s; SamplesPerSecond = 15972.5
 Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.21306763; EvalErr[0]PerSample = 0.36093750; TotalTime = 0.1602s; SamplesPerSecond = 15979.4
 Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.24757233; EvalErr[0]PerSample = 0.38359375; TotalTime = 0.1607s; SamplesPerSecond = 15932.6
 Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.19554443; EvalErr[0]PerSample = 0.35078125; TotalTime = 0.1600s; SamplesPerSecond = 16004.8
 Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.20384216; EvalErr[0]PerSample = 0.36367187; TotalTime = 0.1602s; SamplesPerSecond = 15981.9
 Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.16630554; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.1604s; SamplesPerSecond = 15963.7
 Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.18458252; EvalErr[0]PerSample = 0.36171875; TotalTime = 0.1601s; SamplesPerSecond = 15992.8
 Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.19365845; EvalErr[0]PerSample = 0.36484375; TotalTime = 0.1605s; SamplesPerSecond = 15950.4
 Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.21530151; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.1601s; SamplesPerSecond = 15991.3
 Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.14462280; EvalErr[0]PerSample = 0.34453125; TotalTime = 0.1602s; SamplesPerSecond = 15975.5
 Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.19126282; EvalErr[0]PerSample = 0.36093750; TotalTime = 0.1606s; SamplesPerSecond = 15943.9
 Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.17626343; EvalErr[0]PerSample = 0.35195312; TotalTime = 0.1601s; SamplesPerSecond = 15988.9
 Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.16186218; EvalErr[0]PerSample = 0.35976562; TotalTime = 0.1606s; SamplesPerSecond = 15945.1
 Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.14346313; EvalErr[0]PerSample = 0.36406250; TotalTime = 0.1603s; SamplesPerSecond = 15975.0
 Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.13142090; EvalErr[0]PerSample = 0.35507813; TotalTime = 0.1600s; SamplesPerSecond = 16000.4
 Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.13270264; EvalErr[0]PerSample = 0.34023437; TotalTime = 0.1604s; SamplesPerSecond = 15957.5
 Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.12292480; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.1601s; SamplesPerSecond = 15985.8
 Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.14829407; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.1603s; SamplesPerSecond = 15971.4
 Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.13497009; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.1606s; SamplesPerSecond = 15943.2
Finished Epoch[ 1 of 4]: [Training Set] TrainLossPerSample = 1.4154698; EvalErrPerSample = 0.40495607; AvgLearningRatePerSample = 0.003125; EpochTime=5.21637
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.21730766; EvalErr[0]PerSample = 0.36503906; TotalTime = 0.2712s; SamplesPerSecond = 18881.1
 Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.21715870; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.2664s; SamplesPerSecond = 19220.7
 Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.16129208; EvalErr[0]PerSample = 0.35839844; TotalTime = 0.2665s; SamplesPerSecond = 19212.4
 Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.15958786; EvalErr[0]PerSample = 0.35644531; TotalTime = 0.2667s; SamplesPerSecond = 19195.7
 Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.16776390; EvalErr[0]PerSample = 0.35839844; TotalTime = 0.2667s; SamplesPerSecond = 19199.4
 Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.13870163; EvalErr[0]PerSample = 0.35039063; TotalTime = 0.2665s; SamplesPerSecond = 19215.2
 Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.11073608; EvalErr[0]PerSample = 0.34042969; TotalTime = 0.2665s; SamplesPerSecond = 19213.2
 Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.11032486; EvalErr[0]PerSample = 0.34023437; TotalTime = 0.2664s; SamplesPerSecond = 19217.7
 Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.09545898; EvalErr[0]PerSample = 0.33261719; TotalTime = 0.2663s; SamplesPerSecond = 19224.6
 Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.07892990; EvalErr[0]PerSample = 0.32734375; TotalTime = 0.2662s; SamplesPerSecond = 19235.7
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.05840988; EvalErr[0]PerSample = 0.32050781; TotalTime = 0.2662s; SamplesPerSecond = 19233.8
 Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.07349625; EvalErr[0]PerSample = 0.32753906; TotalTime = 0.2662s; SamplesPerSecond = 19233.9
 Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.05989685; EvalErr[0]PerSample = 0.32304688; TotalTime = 0.2663s; SamplesPerSecond = 19226.0
 Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.10408173; EvalErr[0]PerSample = 0.34003906; TotalTime = 0.2663s; SamplesPerSecond = 19225.7
 Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.08359222; EvalErr[0]PerSample = 0.32773438; TotalTime = 0.2663s; SamplesPerSecond = 19222.9
 Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.09249420; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.2662s; SamplesPerSecond = 19231.6
Finished Epoch[ 2 of 4]: [Training Set] TrainLossPerSample = 1.1205771; EvalErrPerSample = 0.34210205; AvgLearningRatePerSample = 0.003125; EpochTime=4.27164
Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.09719744; EvalErr[0]PerSample = 0.33574219; TotalTime = 0.2667s; SamplesPerSecond = 19198.3
 Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.12543898; EvalErr[0]PerSample = 0.34804687; TotalTime = 0.2665s; SamplesPerSecond = 19210.1
 Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.09644928; EvalErr[0]PerSample = 0.33574219; TotalTime = 0.2665s; SamplesPerSecond = 19213.7
 Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.07368393; EvalErr[0]PerSample = 0.33339844; TotalTime = 0.2665s; SamplesPerSecond = 19209.8
 Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.05899544; EvalErr[0]PerSample = 0.33320312; TotalTime = 0.2664s; SamplesPerSecond = 19221.0
 Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.06317329; EvalErr[0]PerSample = 0.32792969; TotalTime = 0.2662s; SamplesPerSecond = 19230.8
 Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07847366; EvalErr[0]PerSample = 0.33242187; TotalTime = 0.2662s; SamplesPerSecond = 19234.0
 Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.00441132; EvalErr[0]PerSample = 0.32207031; TotalTime = 0.2661s; SamplesPerSecond = 19242.8
 Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.06490250; EvalErr[0]PerSample = 0.32597656; TotalTime = 0.2660s; SamplesPerSecond = 19244.8
 Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.11251144; EvalErr[0]PerSample = 0.34765625; TotalTime = 0.2664s; SamplesPerSecond = 19221.0
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07948303; EvalErr[0]PerSample = 0.33945313; TotalTime = 0.2665s; SamplesPerSecond = 19212.5
 Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.04700241; EvalErr[0]PerSample = 0.33105469; TotalTime = 0.2667s; SamplesPerSecond = 19199.3
 Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.02061920; EvalErr[0]PerSample = 0.31347656; TotalTime = 0.2665s; SamplesPerSecond = 19210.3
 Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.02502289; EvalErr[0]PerSample = 0.31269531; TotalTime = 0.2666s; SamplesPerSecond = 19207.6
 Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07673340; EvalErr[0]PerSample = 0.33222656; TotalTime = 0.2666s; SamplesPerSecond = 19203.5
 Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.08750458; EvalErr[0]PerSample = 0.33554688; TotalTime = 0.2664s; SamplesPerSecond = 19218.1
Finished Epoch[ 3 of 4]: [Training Set] TrainLossPerSample = 1.0694752; EvalErrPerSample = 0.33166504; AvgLearningRatePerSample = 0.003125; EpochTime=4.26732
Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.06360312; EvalErr[0]PerSample = 0.32929687; TotalTime = 0.2669s; SamplesPerSecond = 19186.5
 Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 4926; TrainLossPerSample =  1.03978461; EvalErr[0]PerSample = 0.32135607; TotalTime = 0.3002s; SamplesPerSecond = 16409.7
 Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.02589188; EvalErr[0]PerSample = 0.32578125; TotalTime = 0.2664s; SamplesPerSecond = 19218.8
 Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.98919983; EvalErr[0]PerSample = 0.31269531; TotalTime = 0.2664s; SamplesPerSecond = 19217.3
 Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.02877541; EvalErr[0]PerSample = 0.31640625; TotalTime = 0.2664s; SamplesPerSecond = 19216.1
 Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  0.99369431; EvalErr[0]PerSample = 0.30625000; TotalTime = 0.2664s; SamplesPerSecond = 19215.9
 Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.02620697; EvalErr[0]PerSample = 0.32109375; TotalTime = 0.2660s; SamplesPerSecond = 19246.4
 Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.98886795; EvalErr[0]PerSample = 0.31113281; TotalTime = 0.2662s; SamplesPerSecond = 19233.4
 Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  0.99234619; EvalErr[0]PerSample = 0.30878906; TotalTime = 0.2661s; SamplesPerSecond = 19240.2
 Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.01634216; EvalErr[0]PerSample = 0.31582031; TotalTime = 0.2664s; SamplesPerSecond = 19220.4
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.00039215; EvalErr[0]PerSample = 0.31542969; TotalTime = 0.2664s; SamplesPerSecond = 19216.0
 Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.01849976; EvalErr[0]PerSample = 0.31875000; TotalTime = 0.2666s; SamplesPerSecond = 19202.7
 Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  0.99030609; EvalErr[0]PerSample = 0.30468750; TotalTime = 0.2667s; SamplesPerSecond = 19198.3
 Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.01111908; EvalErr[0]PerSample = 0.31621094; TotalTime = 0.2667s; SamplesPerSecond = 19195.8
 Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.01877289; EvalErr[0]PerSample = 0.31015625; TotalTime = 0.2666s; SamplesPerSecond = 19207.1
 Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.94772644; EvalErr[0]PerSample = 0.30390625; TotalTime = 0.2663s; SamplesPerSecond = 19229.5
Finished Epoch[ 4 of 4]: [Training Set] TrainLossPerSample = 1.0093293; EvalErrPerSample = 0.31480715; AvgLearningRatePerSample = 0.003125; EpochTime=4.3168
CNTKCommandTrainEnd: speechTrain
COMPLETED