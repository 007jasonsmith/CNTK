=== Running /cygdrive/c/src/cntk/x64/release/cntk.exe configFile=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.config currentDirectory=C:\src\cntk\tests\Speech\Data RunDir=E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu DataDir=C:\src\cntk\tests\Speech\Data ConfigDir=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining DeviceId=-1
-------------------------------------------------------------------
Build info: 

		Built time: Dec 16 2015 09:55:54
		Last modified date: Tue Dec 15 10:44:51 2015
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.0
		Build Branch: alrezni/RandomUniform
		Build SHA1: 6c3b8f4ce07c581c8a97d97535507c9234bd41a3
		Built by alrezni on DIFFENG           
		Build Path: c:\src\cntk\MachineLearning\CNTK\
-------------------------------------------------------------------
running on DIFFENG at 2015/12/16 12:51:11
command line: 
C:\src\cntk\x64\release\cntk.exe configFile=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.config currentDirectory=C:\src\cntk\tests\Speech\Data RunDir=E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu DataDir=C:\src\cntk\tests\Speech\Data ConfigDir=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining DeviceId=-1 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\src\cntk\tests\Speech\Data
RunDir=E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu
DataDir=C:\src\cntk\tests\Speech\Data
ConfigDir=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = -1
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    newModel  = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    newModel  = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech.0"
    editPath  = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech"
    deviceId = -1
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\src\cntk\tests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\src\cntk\tests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\src\cntk\tests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\src\cntk\tests\Speech\Data
RunDir=E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu
DataDir=C:\src\cntk\tests\Speech\Data
ConfigDir=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.config:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    newModel  = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    newModel  = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech.0"
    editPath  = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.config:ConfigDir=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining
configparameters: cntk_dpt.config:currentDirectory=C:\src\cntk\tests\Speech\Data
configparameters: cntk_dpt.config:DataDir=C:\src\cntk\tests\Speech\Data
configparameters: cntk_dpt.config:deviceId=-1
configparameters: cntk_dpt.config:dptPre1=[
    action = "train"
    modelPath = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:dptPre2=[
    action = "train"
    modelPath = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.config:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.config:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.config:ndlMacros=C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.config:precision=float
configparameters: cntk_dpt.config:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\src\cntk\tests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\src\cntk\tests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\src\cntk\tests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.config:RunDir=E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu
configparameters: cntk_dpt.config:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.config:speechTrain=[
    action = "train"
    modelPath = "E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech"
    deviceId = -1
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\src\cntk\tests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.config:traceLevel=1
<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain 
precision = float
CNTKModelPath: E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech
CNTKCommandTrainInfo: dptPre1 : 2
CNTKModelPath: E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech
CNTKCommandTrainInfo: dptPre2 : 2
CNTKModelPath: E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech
CNTKCommandTrainInfo: speechTrain : 4
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8
CNTKCommandTrainBegin: dptPre1
NDLBuilder Using CPU
reading script file C:\src\cntk\tests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\src\cntk\tests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\src\cntk\tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

Post-processing network...

3 roots:
	err = ErrorPrediction
	cr = CrossEntropyWithSoftmax
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using CPU.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: SamplesSeen = 2560; TrainLossPerSample =  3.83132935; EvalErr[0]PerSample = 0.82226563; TotalTime = 0.5467s; SamplesPerSecond = 4682.9
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.83457413; EvalErr[0]PerSample = 0.67890625; TotalTime = 0.5774s; SamplesPerSecond = 4433.4
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.54018173; EvalErr[0]PerSample = 0.63945312; TotalTime = 1.2185s; SamplesPerSecond = 2101.0
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.23101807; EvalErr[0]PerSample = 0.58828125; TotalTime = 0.7424s; SamplesPerSecond = 3448.4
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: SamplesSeen = 2560; TrainLossPerSample =  2.03877258; EvalErr[0]PerSample = 0.55234375; TotalTime = 0.6605s; SamplesPerSecond = 3875.9
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.89007111; EvalErr[0]PerSample = 0.51796875; TotalTime = 1.2336s; SamplesPerSecond = 2075.2
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.80725555; EvalErr[0]PerSample = 0.50195313; TotalTime = 0.5224s; SamplesPerSecond = 4900.5
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.73773956; EvalErr[0]PerSample = 0.50117188; TotalTime = 1.1173s; SamplesPerSecond = 2291.2
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.62159882; EvalErr[0]PerSample = 0.48437500; TotalTime = 0.8233s; SamplesPerSecond = 3109.3
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.64018860; EvalErr[0]PerSample = 0.47890625; TotalTime = 0.3723s; SamplesPerSecond = 6875.3
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.62083740; EvalErr[0]PerSample = 0.47968750; TotalTime = 1.0252s; SamplesPerSecond = 2497.2
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.61514587; EvalErr[0]PerSample = 0.46640625; TotalTime = 0.8290s; SamplesPerSecond = 3088.2
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.49160156; EvalErr[0]PerSample = 0.44101563; TotalTime = 0.4095s; SamplesPerSecond = 6252.1
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.45087585; EvalErr[0]PerSample = 0.43828125; TotalTime = 1.1766s; SamplesPerSecond = 2175.7
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.42432251; EvalErr[0]PerSample = 0.40937500; TotalTime = 0.8162s; SamplesPerSecond = 3136.3
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.42227478; EvalErr[0]PerSample = 0.42265625; TotalTime = 0.8180s; SamplesPerSecond = 3129.7
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.39202576; EvalErr[0]PerSample = 0.39375000; TotalTime = 1.2500s; SamplesPerSecond = 2048.0
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.39772644; EvalErr[0]PerSample = 0.41367188; TotalTime = 0.4506s; SamplesPerSecond = 5681.4
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.32753601; EvalErr[0]PerSample = 0.39218750; TotalTime = 1.0455s; SamplesPerSecond = 2448.7
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.36070557; EvalErr[0]PerSample = 0.41484375; TotalTime = 1.0024s; SamplesPerSecond = 2553.7
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.39848022; EvalErr[0]PerSample = 0.42304687; TotalTime = 0.6242s; SamplesPerSecond = 4101.2
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.41367187; EvalErr[0]PerSample = 0.41718750; TotalTime = 1.1200s; SamplesPerSecond = 2285.7
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.32878113; EvalErr[0]PerSample = 0.39453125; TotalTime = 0.6112s; SamplesPerSecond = 4188.2
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.33235474; EvalErr[0]PerSample = 0.40039063; TotalTime = 0.9638s; SamplesPerSecond = 2656.1
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.34081116; EvalErr[0]PerSample = 0.40742187; TotalTime = 1.1206s; SamplesPerSecond = 2284.6
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.34816895; EvalErr[0]PerSample = 0.40234375; TotalTime = 0.4815s; SamplesPerSecond = 5316.5
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.29427795; EvalErr[0]PerSample = 0.41328125; TotalTime = 1.2482s; SamplesPerSecond = 2051.0
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.26855774; EvalErr[0]PerSample = 0.37890625; TotalTime = 0.6726s; SamplesPerSecond = 3806.0
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.29057922; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.6711s; SamplesPerSecond = 3814.9
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.26688843; EvalErr[0]PerSample = 0.37226562; TotalTime = 0.9120s; SamplesPerSecond = 2806.9
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.31537170; EvalErr[0]PerSample = 0.40390625; TotalTime = 0.7437s; SamplesPerSecond = 3442.2
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.32760010; EvalErr[0]PerSample = 0.41132812; TotalTime = 0.5974s; SamplesPerSecond = 4285.2
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.6437914; EvalErrPerSample = 0.46402588; AvgLearningRatePerSample = 0.003125; EpochTime=26.5598
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.26175909; EvalErr[0]PerSample = 0.38046875; TotalTime = 1.2700s; SamplesPerSecond = 2015.7
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.27203035; EvalErr[0]PerSample = 0.39062500; TotalTime = 0.4860s; SamplesPerSecond = 5266.9
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.25914268; EvalErr[0]PerSample = 0.37851563; TotalTime = 0.8493s; SamplesPerSecond = 3014.4
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.24175148; EvalErr[0]PerSample = 0.37695313; TotalTime = 1.1042s; SamplesPerSecond = 2318.5
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.26896439; EvalErr[0]PerSample = 0.37890625; TotalTime = 0.4829s; SamplesPerSecond = 5301.5
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.18769341; EvalErr[0]PerSample = 0.36171875; TotalTime = 1.1009s; SamplesPerSecond = 2325.3
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.19647293; EvalErr[0]PerSample = 0.35742188; TotalTime = 0.8577s; SamplesPerSecond = 2984.6
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.21377335; EvalErr[0]PerSample = 0.38007812; TotalTime = 0.7282s; SamplesPerSecond = 3515.7
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.23979187; EvalErr[0]PerSample = 0.37500000; TotalTime = 1.2759s; SamplesPerSecond = 2006.4
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.22581253; EvalErr[0]PerSample = 0.36914063; TotalTime = 0.5208s; SamplesPerSecond = 4915.5
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.20285797; EvalErr[0]PerSample = 0.36796875; TotalTime = 1.2012s; SamplesPerSecond = 2131.3
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.21250305; EvalErr[0]PerSample = 0.36328125; TotalTime = 1.0059s; SamplesPerSecond = 2545.0
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.18704224; EvalErr[0]PerSample = 0.35898438; TotalTime = 0.8414s; SamplesPerSecond = 3042.6
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.16055756; EvalErr[0]PerSample = 0.35859375; TotalTime = 1.1251s; SamplesPerSecond = 2275.4
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.16683502; EvalErr[0]PerSample = 0.34296875; TotalTime = 0.4348s; SamplesPerSecond = 5887.9
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.10802307; EvalErr[0]PerSample = 0.34023437; TotalTime = 1.0801s; SamplesPerSecond = 2370.1
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.11230621; EvalErr[0]PerSample = 0.33671875; TotalTime = 0.8916s; SamplesPerSecond = 2871.2
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.18252411; EvalErr[0]PerSample = 0.34648438; TotalTime = 0.4033s; SamplesPerSecond = 6348.3
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.14249878; EvalErr[0]PerSample = 0.35468750; TotalTime = 1.1140s; SamplesPerSecond = 2298.0
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.13199158; EvalErr[0]PerSample = 0.34140625; TotalTime = 0.8137s; SamplesPerSecond = 3146.2
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.15645599; EvalErr[0]PerSample = 0.35976562; TotalTime = 0.6033s; SamplesPerSecond = 4243.3
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.11040497; EvalErr[0]PerSample = 0.33593750; TotalTime = 1.3501s; SamplesPerSecond = 1896.1
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.14821167; EvalErr[0]PerSample = 0.36796875; TotalTime = 0.5839s; SamplesPerSecond = 4384.1
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.12954712; EvalErr[0]PerSample = 0.34609375; TotalTime = 1.0188s; SamplesPerSecond = 2512.9
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.07280884; EvalErr[0]PerSample = 0.32031250; TotalTime = 0.9063s; SamplesPerSecond = 2824.7
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.15885315; EvalErr[0]PerSample = 0.34062500; TotalTime = 0.6539s; SamplesPerSecond = 3915.2
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.16690369; EvalErr[0]PerSample = 0.35742188; TotalTime = 1.1581s; SamplesPerSecond = 2210.6
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.14694824; EvalErr[0]PerSample = 0.34570313; TotalTime = 0.6763s; SamplesPerSecond = 3785.4
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.09171143; EvalErr[0]PerSample = 0.33828125; TotalTime = 0.8276s; SamplesPerSecond = 3093.2
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.10540771; EvalErr[0]PerSample = 0.33828125; TotalTime = 1.1961s; SamplesPerSecond = 2140.3
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.15972290; EvalErr[0]PerSample = 0.34492187; TotalTime = 0.4821s; SamplesPerSecond = 5310.5
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.05378418; EvalErr[0]PerSample = 0.32500000; TotalTime = 0.9329s; SamplesPerSecond = 2744.0
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1710967; EvalErrPerSample = 0.35563967; AvgLearningRatePerSample = 0.003125; EpochTime=27.9792
CNTKCommandTrainEnd: dptPre1

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.

Post-processing network complete.
CNTKCommandTrainBegin: dptPre2
NDLBuilder Using CPU
reading script file C:\src\cntk\tests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\src\cntk\tests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\src\cntk\tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech.0.

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using CPU.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: SamplesSeen = 2560; TrainLossPerSample =  5.10127983; EvalErr[0]PerSample = 0.84179688; TotalTime = 1.0294s; SamplesPerSecond = 2486.8
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.86521187; EvalErr[0]PerSample = 0.71054688; TotalTime = 2.4895s; SamplesPerSecond = 1028.3
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.43145370; EvalErr[0]PerSample = 0.63554687; TotalTime = 1.6025s; SamplesPerSecond = 1597.5
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.05614700; EvalErr[0]PerSample = 0.57656250; TotalTime = 1.6826s; SamplesPerSecond = 1521.5
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.81891785; EvalErr[0]PerSample = 0.50507813; TotalTime = 1.1181s; SamplesPerSecond = 2289.6
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.65977020; EvalErr[0]PerSample = 0.48164062; TotalTime = 1.7583s; SamplesPerSecond = 1456.0
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.61013641; EvalErr[0]PerSample = 0.46718750; TotalTime = 1.5712s; SamplesPerSecond = 1629.4
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.55111084; EvalErr[0]PerSample = 0.44375000; TotalTime = 2.0176s; SamplesPerSecond = 1268.8
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.45390930; EvalErr[0]PerSample = 0.43750000; TotalTime = 1.4219s; SamplesPerSecond = 1800.4
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.48341980; EvalErr[0]PerSample = 0.44531250; TotalTime = 1.7712s; SamplesPerSecond = 1445.4
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.39064331; EvalErr[0]PerSample = 0.41601563; TotalTime = 1.2853s; SamplesPerSecond = 1991.8
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.38506622; EvalErr[0]PerSample = 0.39765625; TotalTime = 1.3100s; SamplesPerSecond = 1954.1
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.32042084; EvalErr[0]PerSample = 0.39257813; TotalTime = 1.7647s; SamplesPerSecond = 1450.7
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.28485413; EvalErr[0]PerSample = 0.39218750; TotalTime = 0.6371s; SamplesPerSecond = 4018.1
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.30762024; EvalErr[0]PerSample = 0.38515625; TotalTime = 1.8442s; SamplesPerSecond = 1388.1
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.31369629; EvalErr[0]PerSample = 0.39296875; TotalTime = 1.4646s; SamplesPerSecond = 1747.9
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.26266785; EvalErr[0]PerSample = 0.36875000; TotalTime = 1.4117s; SamplesPerSecond = 1813.4
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.29051819; EvalErr[0]PerSample = 0.37695313; TotalTime = 1.7767s; SamplesPerSecond = 1440.8
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.24666443; EvalErr[0]PerSample = 0.37968750; TotalTime = 1.6703s; SamplesPerSecond = 1532.7
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.27678833; EvalErr[0]PerSample = 0.39062500; TotalTime = 1.4169s; SamplesPerSecond = 1806.8
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.25270081; EvalErr[0]PerSample = 0.37695313; TotalTime = 1.5489s; SamplesPerSecond = 1652.8
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.28959961; EvalErr[0]PerSample = 0.37851563; TotalTime = 1.8292s; SamplesPerSecond = 1399.5
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.21957703; EvalErr[0]PerSample = 0.36757812; TotalTime = 1.2669s; SamplesPerSecond = 2020.7
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.25221558; EvalErr[0]PerSample = 0.37148437; TotalTime = 1.6251s; SamplesPerSecond = 1575.2
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.25035400; EvalErr[0]PerSample = 0.37656250; TotalTime = 2.1321s; SamplesPerSecond = 1200.7
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.22284546; EvalErr[0]PerSample = 0.37656250; TotalTime = 1.4878s; SamplesPerSecond = 1720.7
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.20678406; EvalErr[0]PerSample = 0.37382813; TotalTime = 1.1380s; SamplesPerSecond = 2249.6
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.19659729; EvalErr[0]PerSample = 0.36953125; TotalTime = 1.8258s; SamplesPerSecond = 1402.1
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.19133606; EvalErr[0]PerSample = 0.35976562; TotalTime = 1.4235s; SamplesPerSecond = 1798.3
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.16777954; EvalErr[0]PerSample = 0.35312500; TotalTime = 1.3260s; SamplesPerSecond = 1930.6
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.20301819; EvalErr[0]PerSample = 0.36601563; TotalTime = 1.8448s; SamplesPerSecond = 1387.7
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.17025452; EvalErr[0]PerSample = 0.36250000; TotalTime = 1.4732s; SamplesPerSecond = 1737.7
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.5541675; EvalErrPerSample = 0.43343505; AvgLearningRatePerSample = 0.003125; EpochTime=50.1142
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.18681822; EvalErr[0]PerSample = 0.36093750; TotalTime = 1.0669s; SamplesPerSecond = 2399.4
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.20552111; EvalErr[0]PerSample = 0.37148437; TotalTime = 1.8157s; SamplesPerSecond = 1409.9
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.17922058; EvalErr[0]PerSample = 0.36015625; TotalTime = 1.2338s; SamplesPerSecond = 2074.9
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.17175713; EvalErr[0]PerSample = 0.35234375; TotalTime = 1.4165s; SamplesPerSecond = 1807.2
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.19047356; EvalErr[0]PerSample = 0.34882812; TotalTime = 1.8422s; SamplesPerSecond = 1389.7
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.14220276; EvalErr[0]PerSample = 0.35273437; TotalTime = 1.7756s; SamplesPerSecond = 1441.8
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.15370560; EvalErr[0]PerSample = 0.35390625; TotalTime = 1.3511s; SamplesPerSecond = 1894.7
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.16578293; EvalErr[0]PerSample = 0.35781250; TotalTime = 1.6720s; SamplesPerSecond = 1531.1
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.18986206; EvalErr[0]PerSample = 0.36367187; TotalTime = 1.5108s; SamplesPerSecond = 1694.5
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.15478592; EvalErr[0]PerSample = 0.34101562; TotalTime = 1.1601s; SamplesPerSecond = 2206.6
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.15113602; EvalErr[0]PerSample = 0.35039063; TotalTime = 1.9834s; SamplesPerSecond = 1290.7
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.14106140; EvalErr[0]PerSample = 0.34375000; TotalTime = 1.3129s; SamplesPerSecond = 1949.9
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.14694824; EvalErr[0]PerSample = 0.33984375; TotalTime = 1.1319s; SamplesPerSecond = 2261.7
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.12372742; EvalErr[0]PerSample = 0.34960938; TotalTime = 1.7481s; SamplesPerSecond = 1464.4
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.09288025; EvalErr[0]PerSample = 0.32890625; TotalTime = 1.0376s; SamplesPerSecond = 2467.3
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.08198547; EvalErr[0]PerSample = 0.33554688; TotalTime = 1.7154s; SamplesPerSecond = 1492.4
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.09102631; EvalErr[0]PerSample = 0.33984375; TotalTime = 1.5080s; SamplesPerSecond = 1697.6
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.10075836; EvalErr[0]PerSample = 0.32617188; TotalTime = 1.2000s; SamplesPerSecond = 2133.4
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.08464813; EvalErr[0]PerSample = 0.33320312; TotalTime = 1.7662s; SamplesPerSecond = 1449.4
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.07890320; EvalErr[0]PerSample = 0.32421875; TotalTime = 1.5642s; SamplesPerSecond = 1636.6
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.07618561; EvalErr[0]PerSample = 0.33671875; TotalTime = 1.4140s; SamplesPerSecond = 1810.5
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.07085114; EvalErr[0]PerSample = 0.32187500; TotalTime = 1.8190s; SamplesPerSecond = 1407.4
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.08208771; EvalErr[0]PerSample = 0.32812500; TotalTime = 1.3749s; SamplesPerSecond = 1861.9
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.07838440; EvalErr[0]PerSample = 0.33007813; TotalTime = 1.0003s; SamplesPerSecond = 2559.3
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.03401489; EvalErr[0]PerSample = 0.31445313; TotalTime = 0.9196s; SamplesPerSecond = 2783.7
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.11870728; EvalErr[0]PerSample = 0.33046875; TotalTime = 0.9722s; SamplesPerSecond = 2633.1
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.12119141; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.2493s; SamplesPerSecond = 10267.7
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.12145691; EvalErr[0]PerSample = 0.34062500; TotalTime = 0.7494s; SamplesPerSecond = 3416.0
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.07471313; EvalErr[0]PerSample = 0.32031250; TotalTime = 1.0719s; SamplesPerSecond = 2388.2
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.06800842; EvalErr[0]PerSample = 0.32109375; TotalTime = 0.9211s; SamplesPerSecond = 2779.4
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.13219910; EvalErr[0]PerSample = 0.33085938; TotalTime = 1.0993s; SamplesPerSecond = 2328.7
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.04135742; EvalErr[0]PerSample = 0.31992188; TotalTime = 0.8818s; SamplesPerSecond = 2903.2
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1203864; EvalErrPerSample = 0.33937988; AvgLearningRatePerSample = 0.003125; EpochTime=42.2887
CNTKCommandTrainEnd: dptPre2

Post-processing network...

3 roots:
	scaledLogLikelihood = Minus
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 11 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

Post-processing network complete.
CNTKCommandTrainBegin: speechTrain
NDLBuilder Using CPU
reading script file C:\src\cntk\tests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\src\cntk\tests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\src\cntk\tests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File E:\cygwin64\tmp\cntk-test-20151216133754.160067\Speech\DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech.0.

Post-processing network...

3 roots:
	scaledLogLikelihood = Minus
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node scaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 14 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.


Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.


Validating for node err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using CPU.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: SamplesSeen = 2560; TrainLossPerSample =  4.13687515; EvalErr[0]PerSample = 0.83515625; TotalTime = 1.2062s; SamplesPerSecond = 2122.3
 Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.51949005; EvalErr[0]PerSample = 0.62304688; TotalTime = 1.4136s; SamplesPerSecond = 1811.0
 Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.11661987; EvalErr[0]PerSample = 0.56601563; TotalTime = 2.2406s; SamplesPerSecond = 1142.6
 Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.77143402; EvalErr[0]PerSample = 0.49492188; TotalTime = 1.9722s; SamplesPerSecond = 1298.0
 Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.59080048; EvalErr[0]PerSample = 0.45742187; TotalTime = 2.0952s; SamplesPerSecond = 1221.8
 Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.46648636; EvalErr[0]PerSample = 0.42500000; TotalTime = 2.5208s; SamplesPerSecond = 1015.5
 Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.44221954; EvalErr[0]PerSample = 0.42382813; TotalTime = 2.5050s; SamplesPerSecond = 1021.9
 Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.40807037; EvalErr[0]PerSample = 0.40703125; TotalTime = 3.5720s; SamplesPerSecond = 716.7
 Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.32473145; EvalErr[0]PerSample = 0.39609375; TotalTime = 2.2470s; SamplesPerSecond = 1139.3
 Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.32997284; EvalErr[0]PerSample = 0.39921875; TotalTime = 2.2524s; SamplesPerSecond = 1136.5
 Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.27397614; EvalErr[0]PerSample = 0.38046875; TotalTime = 2.2721s; SamplesPerSecond = 1126.7
 Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.28483124; EvalErr[0]PerSample = 0.38242188; TotalTime = 2.2472s; SamplesPerSecond = 1139.2
 Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.21706696; EvalErr[0]PerSample = 0.36054687; TotalTime = 2.2523s; SamplesPerSecond = 1136.6
 Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.18651428; EvalErr[0]PerSample = 0.36132813; TotalTime = 2.1491s; SamplesPerSecond = 1191.2
 Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.22145691; EvalErr[0]PerSample = 0.36523438; TotalTime = 2.1755s; SamplesPerSecond = 1176.8
 Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.24529114; EvalErr[0]PerSample = 0.37656250; TotalTime = 1.8530s; SamplesPerSecond = 1381.5
 Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.19645081; EvalErr[0]PerSample = 0.36015625; TotalTime = 2.1636s; SamplesPerSecond = 1183.2
 Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.20987244; EvalErr[0]PerSample = 0.35781250; TotalTime = 2.9905s; SamplesPerSecond = 856.1
 Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.16274414; EvalErr[0]PerSample = 0.35156250; TotalTime = 2.2066s; SamplesPerSecond = 1160.2
 Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.19518433; EvalErr[0]PerSample = 0.36210938; TotalTime = 2.1595s; SamplesPerSecond = 1185.4
 Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.19760437; EvalErr[0]PerSample = 0.36093750; TotalTime = 2.2349s; SamplesPerSecond = 1145.5
 Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.21906128; EvalErr[0]PerSample = 0.36171875; TotalTime = 2.0436s; SamplesPerSecond = 1252.7
 Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.15016174; EvalErr[0]PerSample = 0.35117188; TotalTime = 2.2597s; SamplesPerSecond = 1132.9
 Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.18217468; EvalErr[0]PerSample = 0.35273437; TotalTime = 1.9021s; SamplesPerSecond = 1345.8
 Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: SamplesSeen = 2560; TrainLossPerSample =  1.18083801; EvalErr[0]PerSample = 0.35859375; TotalTime = 2.4178s; SamplesPerSecond = 1058.8
 Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.16531067; EvalErr[0]PerSample = 0.36250000; TotalTime = 1.9678s; SamplesPerSecond = 1301.0
 Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.14478455; EvalErr[0]PerSample = 0.35859375; TotalTime = 2.0749s; SamplesPerSecond = 1233.8
 Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.12984619; EvalErr[0]PerSample = 0.35507813; TotalTime = 2.3618s; SamplesPerSecond = 1083.9
 Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: SamplesSeen = 2560; TrainLossPerSample =  1.12773743; EvalErr[0]PerSample = 0.34179688; TotalTime = 2.0020s; SamplesPerSecond = 1278.7
 Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.11937256; EvalErr[0]PerSample = 0.33906250; TotalTime = 2.1747s; SamplesPerSecond = 1177.2
 Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.14234619; EvalErr[0]PerSample = 0.35351563; TotalTime = 3.4765s; SamplesPerSecond = 736.4
 Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.13502502; EvalErr[0]PerSample = 0.34257813; TotalTime = 2.0512s; SamplesPerSecond = 1248.0
Finished Epoch[ 1 of 4]: [Training Set] TrainLossPerSample = 1.4123235; EvalErrPerSample = 0.40388185; AvgLearningRatePerSample = 0.003125; EpochTime=71.636
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.22579308; EvalErr[0]PerSample = 0.37109375; TotalTime = 2.6241s; SamplesPerSecond = 1951.2
 Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.19644642; EvalErr[0]PerSample = 0.36933594; TotalTime = 2.5524s; SamplesPerSecond = 2005.9
 Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.18332996; EvalErr[0]PerSample = 0.35878906; TotalTime = 2.8407s; SamplesPerSecond = 1802.4
 Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.15550995; EvalErr[0]PerSample = 0.35156250; TotalTime = 2.9258s; SamplesPerSecond = 1749.9
 Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.16411247; EvalErr[0]PerSample = 0.35039063; TotalTime = 2.7767s; SamplesPerSecond = 1843.9
 Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.13960152; EvalErr[0]PerSample = 0.35097656; TotalTime = 2.7687s; SamplesPerSecond = 1849.2
 Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.13937607; EvalErr[0]PerSample = 0.34511719; TotalTime = 2.5241s; SamplesPerSecond = 2028.4
 Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.10203705; EvalErr[0]PerSample = 0.33925781; TotalTime = 3.6322s; SamplesPerSecond = 1409.6
 Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.08201599; EvalErr[0]PerSample = 0.33339844; TotalTime = 2.9353s; SamplesPerSecond = 1744.3
 Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.07910156; EvalErr[0]PerSample = 0.32968750; TotalTime = 2.8807s; SamplesPerSecond = 1777.3
 Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.06678619; EvalErr[0]PerSample = 0.32792969; TotalTime = 2.8473s; SamplesPerSecond = 1798.2
 Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.06557236; EvalErr[0]PerSample = 0.32695313; TotalTime = 2.4350s; SamplesPerSecond = 2102.7
 Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.05176849; EvalErr[0]PerSample = 0.31660156; TotalTime = 2.8992s; SamplesPerSecond = 1766.0
 Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.10163116; EvalErr[0]PerSample = 0.33710937; TotalTime = 2.8585s; SamplesPerSecond = 1791.2
 Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.11587982; EvalErr[0]PerSample = 0.33847656; TotalTime = 2.7854s; SamplesPerSecond = 1838.2
 Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.10506897; EvalErr[0]PerSample = 0.33164063; TotalTime = 3.0453s; SamplesPerSecond = 1681.3
Finished Epoch[ 2 of 4]: [Training Set] TrainLossPerSample = 1.123377; EvalErrPerSample = 0.34239504; AvgLearningRatePerSample = 0.003125; EpochTime=45.3343
Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.10629292; EvalErr[0]PerSample = 0.34316406; TotalTime = 2.8818s; SamplesPerSecond = 1776.6
 Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.12694092; EvalErr[0]PerSample = 0.34433594; TotalTime = 2.5341s; SamplesPerSecond = 2020.5
 Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.09634953; EvalErr[0]PerSample = 0.33867188; TotalTime = 2.5339s; SamplesPerSecond = 2020.6
 Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.05796852; EvalErr[0]PerSample = 0.33300781; TotalTime = 2.8041s; SamplesPerSecond = 1825.9
 Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.05851707; EvalErr[0]PerSample = 0.33222656; TotalTime = 2.9326s; SamplesPerSecond = 1745.9
 Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.06587944; EvalErr[0]PerSample = 0.32929687; TotalTime = 2.9918s; SamplesPerSecond = 1711.3
 Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07822037; EvalErr[0]PerSample = 0.32695313; TotalTime = 2.6753s; SamplesPerSecond = 1913.8
 Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.00518265; EvalErr[0]PerSample = 0.31835938; TotalTime = 2.6277s; SamplesPerSecond = 1948.5
 Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.06181793; EvalErr[0]PerSample = 0.32460937; TotalTime = 2.9600s; SamplesPerSecond = 1729.8
 Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.11828995; EvalErr[0]PerSample = 0.34492187; TotalTime = 2.7723s; SamplesPerSecond = 1846.9
 Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07651520; EvalErr[0]PerSample = 0.33632812; TotalTime = 2.7823s; SamplesPerSecond = 1840.2
 Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.05281982; EvalErr[0]PerSample = 0.33515625; TotalTime = 2.6387s; SamplesPerSecond = 1940.3
 Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.03086548; EvalErr[0]PerSample = 0.31699219; TotalTime = 2.7480s; SamplesPerSecond = 1863.2
 Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.02187195; EvalErr[0]PerSample = 0.31250000; TotalTime = 3.1523s; SamplesPerSecond = 1624.2
 Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07047272; EvalErr[0]PerSample = 0.32929687; TotalTime = 3.3451s; SamplesPerSecond = 1530.6
 Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.08807526; EvalErr[0]PerSample = 0.34472656; TotalTime = 3.0613s; SamplesPerSecond = 1672.5
Finished Epoch[ 3 of 4]: [Training Set] TrainLossPerSample = 1.069755; EvalErrPerSample = 0.33190918; AvgLearningRatePerSample = 0.003125; EpochTime=45.4441
Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.06099691; EvalErr[0]PerSample = 0.33671875; TotalTime = 2.4776s; SamplesPerSecond = 2066.5
 Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 4926; TrainLossPerSample =  1.03466331; EvalErr[0]PerSample = 0.32358912; TotalTime = 2.9045s; SamplesPerSecond = 1696.0
 Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.03127460; EvalErr[0]PerSample = 0.32597656; TotalTime = 2.7087s; SamplesPerSecond = 1890.2
 Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.99422379; EvalErr[0]PerSample = 0.30957031; TotalTime = 2.7916s; SamplesPerSecond = 1834.1
 Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.02041893; EvalErr[0]PerSample = 0.31679687; TotalTime = 3.1611s; SamplesPerSecond = 1619.7
 Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  0.99272194; EvalErr[0]PerSample = 0.30449219; TotalTime = 3.1203s; SamplesPerSecond = 1640.9
 Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.02109184; EvalErr[0]PerSample = 0.32031250; TotalTime = 2.4801s; SamplesPerSecond = 2064.4
 Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.99958649; EvalErr[0]PerSample = 0.31386719; TotalTime = 2.6852s; SamplesPerSecond = 1906.8
 Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  0.99857788; EvalErr[0]PerSample = 0.31582031; TotalTime = 2.3044s; SamplesPerSecond = 2221.8
 Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.01854477; EvalErr[0]PerSample = 0.31367187; TotalTime = 2.7566s; SamplesPerSecond = 1857.4
 Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.01711502; EvalErr[0]PerSample = 0.31953125; TotalTime = 3.1474s; SamplesPerSecond = 1626.8
 Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.04888077; EvalErr[0]PerSample = 0.32324219; TotalTime = 2.6620s; SamplesPerSecond = 1923.4
 Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.01893158; EvalErr[0]PerSample = 0.31875000; TotalTime = 2.9165s; SamplesPerSecond = 1755.5
 Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.00973663; EvalErr[0]PerSample = 0.31972656; TotalTime = 2.8271s; SamplesPerSecond = 1811.0
 Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.02276459; EvalErr[0]PerSample = 0.31210938; TotalTime = 3.0211s; SamplesPerSecond = 1694.7
 Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.96413574; EvalErr[0]PerSample = 0.30605469; TotalTime = 2.7955s; SamplesPerSecond = 1831.5
Finished Epoch[ 4 of 4]: [Training Set] TrainLossPerSample = 1.0157522; EvalErrPerSample = 0.31748047; AvgLearningRatePerSample = 0.003125; EpochTime=44.8553
CNTKCommandTrainEnd: speechTrain
COMPLETED