=== Running /home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining DeviceId=-1
-------------------------------------------------------------------
Build info: 

		Built time: Dec 15 2015 16:32:52
		Last modified date: Tue Dec 15 16:31:42 2015
		Build type: release
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.0
		CUB_PATH: /usr/local/cub-1.4.1
		Build Branch: master
		Build SHA1: 5e0017ac9c55c23d53cb524c8acb7d6d9bfd0269
-------------------------------------------------------------------
running on localhost at 2015/12/15 16:48:53
command line: 
/home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining DeviceId=-1 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision = "float"
deviceId = -1
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech"
    deviceId = -1
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
DeviceId=-1

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.config:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech.0"
    editPath  = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.config:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.config:ConfigDir=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining
configparameters: cntk_dpt.config:currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk_dpt.config:DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk_dpt.config:deviceId=-1
configparameters: cntk_dpt.config:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.config:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.config:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.config:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.config:ndlMacros=/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.config:precision=float
configparameters: cntk_dpt.config:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.config:RunDir=/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu
configparameters: cntk_dpt.config:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.config:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech"
    deviceId = -1
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/alrezni/src/cntk/Tests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.config:traceLevel=1
<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain 
precision = float
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre1/cntkSpeech
CNTKCommandTrainInfo: dptPre1 : 2
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech
CNTKCommandTrainInfo: dptPre2 : 2
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech
CNTKCommandTrainInfo: speechTrain : 4
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8
CNTKCommandTrainBegin: dptPre1
NDLBuilder Using CPU
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 1]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 1], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 1]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 1], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 1]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 1]) -> [512, MBSize 1]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 1], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using CPU.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  3.83132935; EvalErr[0]PerSample = 0.82226562; TotalTime = 0.0697s; SamplesPerSecond = 36725.7
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.83457413; EvalErr[0]PerSample = 0.67890625; TotalTime = 0.0552s; SamplesPerSecond = 46415.5
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.54018173; EvalErr[0]PerSample = 0.63945312; TotalTime = 0.0552s; SamplesPerSecond = 46355.8
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.23101730; EvalErr[0]PerSample = 0.58828125; TotalTime = 0.0552s; SamplesPerSecond = 46352.5
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  2.03877335; EvalErr[0]PerSample = 0.55234375; TotalTime = 0.0552s; SamplesPerSecond = 46344.1
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.89007111; EvalErr[0]PerSample = 0.51796875; TotalTime = 0.0553s; SamplesPerSecond = 46320.6
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.80725708; EvalErr[0]PerSample = 0.50195312; TotalTime = 0.0553s; SamplesPerSecond = 46301.3
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.73773956; EvalErr[0]PerSample = 0.50117188; TotalTime = 0.0553s; SamplesPerSecond = 46316.4
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.62159882; EvalErr[0]PerSample = 0.48437500; TotalTime = 0.0553s; SamplesPerSecond = 46331.5
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.64018860; EvalErr[0]PerSample = 0.47890625; TotalTime = 0.0554s; SamplesPerSecond = 46242.8
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.62083740; EvalErr[0]PerSample = 0.47968750; TotalTime = 0.0553s; SamplesPerSecond = 46259.5
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.61514740; EvalErr[0]PerSample = 0.46640625; TotalTime = 0.0552s; SamplesPerSecond = 46381.0
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.49160156; EvalErr[0]PerSample = 0.44101563; TotalTime = 0.0553s; SamplesPerSecond = 46287.1
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.45087891; EvalErr[0]PerSample = 0.43828125; TotalTime = 0.0553s; SamplesPerSecond = 46329.8
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.42432251; EvalErr[0]PerSample = 0.40937500; TotalTime = 0.0558s; SamplesPerSecond = 45841.2
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.42227783; EvalErr[0]PerSample = 0.42265625; TotalTime = 0.0554s; SamplesPerSecond = 46216.9
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.39202576; EvalErr[0]PerSample = 0.39375000; TotalTime = 0.0555s; SamplesPerSecond = 46095.4
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.39772339; EvalErr[0]PerSample = 0.41367188; TotalTime = 0.0553s; SamplesPerSecond = 46270.4
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.32752991; EvalErr[0]PerSample = 0.39218750; TotalTime = 0.0554s; SamplesPerSecond = 46247.0
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.36070557; EvalErr[0]PerSample = 0.41484375; TotalTime = 0.0553s; SamplesPerSecond = 46285.4
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.39848328; EvalErr[0]PerSample = 0.42304687; TotalTime = 0.0554s; SamplesPerSecond = 46186.0
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.41366272; EvalErr[0]PerSample = 0.41718750; TotalTime = 0.0553s; SamplesPerSecond = 46307.2
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.32877502; EvalErr[0]PerSample = 0.39453125; TotalTime = 0.0553s; SamplesPerSecond = 46334.0
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.33236084; EvalErr[0]PerSample = 0.40039062; TotalTime = 0.0553s; SamplesPerSecond = 46288.8
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.34082031; EvalErr[0]PerSample = 0.40742187; TotalTime = 0.0552s; SamplesPerSecond = 46345.7
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.34818115; EvalErr[0]PerSample = 0.40234375; TotalTime = 0.0553s; SamplesPerSecond = 46277.0
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.29428711; EvalErr[0]PerSample = 0.41367188; TotalTime = 0.0552s; SamplesPerSecond = 46355.0
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.26860352; EvalErr[0]PerSample = 0.37851563; TotalTime = 0.0553s; SamplesPerSecond = 46289.6
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.29054565; EvalErr[0]PerSample = 0.38671875; TotalTime = 0.0552s; SamplesPerSecond = 46339.0
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.26689758; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.0553s; SamplesPerSecond = 46268.7
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.31538086; EvalErr[0]PerSample = 0.40390625; TotalTime = 0.0554s; SamplesPerSecond = 46249.5
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.32755737; EvalErr[0]PerSample = 0.41054687; TotalTime = 0.0546s; SamplesPerSecond = 46858.1
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.6437918; EvalErrPerSample = 0.46396485; AvgLearningRatePerSample = 0.003125; EpochTime=1.86856
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.26157866; EvalErr[0]PerSample = 0.38007812; TotalTime = 0.0557s; SamplesPerSecond = 45976.2
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.27182264; EvalErr[0]PerSample = 0.39140625; TotalTime = 0.0556s; SamplesPerSecond = 46006.8
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.25893726; EvalErr[0]PerSample = 0.37929687; TotalTime = 0.0555s; SamplesPerSecond = 46104.5
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.24161911; EvalErr[0]PerSample = 0.37539062; TotalTime = 0.0555s; SamplesPerSecond = 46160.2
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.26974945; EvalErr[0]PerSample = 0.38203125; TotalTime = 0.0555s; SamplesPerSecond = 46148.6
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.18642731; EvalErr[0]PerSample = 0.36210938; TotalTime = 0.0555s; SamplesPerSecond = 46138.6
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.19709320; EvalErr[0]PerSample = 0.35781250; TotalTime = 0.0556s; SamplesPerSecond = 46053.9
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.21570816; EvalErr[0]PerSample = 0.37851563; TotalTime = 0.0562s; SamplesPerSecond = 45553.2
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.24040680; EvalErr[0]PerSample = 0.37500000; TotalTime = 0.0555s; SamplesPerSecond = 46166.1
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.22780533; EvalErr[0]PerSample = 0.36953125; TotalTime = 0.0555s; SamplesPerSecond = 46128.6
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.20197067; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.0555s; SamplesPerSecond = 46144.4
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.20512390; EvalErr[0]PerSample = 0.36367187; TotalTime = 0.0555s; SamplesPerSecond = 46126.1
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.18534546; EvalErr[0]PerSample = 0.35898438; TotalTime = 0.0556s; SamplesPerSecond = 46066.4
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.16091156; EvalErr[0]PerSample = 0.36171875; TotalTime = 0.0555s; SamplesPerSecond = 46122.0
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.15581207; EvalErr[0]PerSample = 0.33906250; TotalTime = 0.0555s; SamplesPerSecond = 46100.4
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.11231842; EvalErr[0]PerSample = 0.34179688; TotalTime = 0.0555s; SamplesPerSecond = 46094.6
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.10982819; EvalErr[0]PerSample = 0.33671875; TotalTime = 0.0555s; SamplesPerSecond = 46113.7
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.17951050; EvalErr[0]PerSample = 0.34882812; TotalTime = 0.0555s; SamplesPerSecond = 46142.8
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.13334808; EvalErr[0]PerSample = 0.35234375; TotalTime = 0.0555s; SamplesPerSecond = 46099.5
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.12929535; EvalErr[0]PerSample = 0.34648438; TotalTime = 0.0556s; SamplesPerSecond = 46031.6
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.14827423; EvalErr[0]PerSample = 0.35625000; TotalTime = 0.0556s; SamplesPerSecond = 46060.6
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.11930237; EvalErr[0]PerSample = 0.33710937; TotalTime = 0.0555s; SamplesPerSecond = 46141.9
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.16259460; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.0555s; SamplesPerSecond = 46146.1
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.13496094; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.0557s; SamplesPerSecond = 45998.5
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.07667542; EvalErr[0]PerSample = 0.32578125; TotalTime = 0.0554s; SamplesPerSecond = 46169.4
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.15973511; EvalErr[0]PerSample = 0.34179688; TotalTime = 0.0555s; SamplesPerSecond = 46101.2
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.16832275; EvalErr[0]PerSample = 0.35351562; TotalTime = 0.0556s; SamplesPerSecond = 46073.8
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.14626465; EvalErr[0]PerSample = 0.34570312; TotalTime = 0.0555s; SamplesPerSecond = 46103.7
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.09096069; EvalErr[0]PerSample = 0.33437500; TotalTime = 0.0554s; SamplesPerSecond = 46173.5
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.10794678; EvalErr[0]PerSample = 0.33984375; TotalTime = 0.0555s; SamplesPerSecond = 46088.8
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.16580200; EvalErr[0]PerSample = 0.34531250; TotalTime = 0.0556s; SamplesPerSecond = 46062.2
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.05586853; EvalErr[0]PerSample = 0.32265625; TotalTime = 0.0548s; SamplesPerSecond = 46717.9
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1712912; EvalErrPerSample = 0.35571289; AvgLearningRatePerSample = 0.003125; EpochTime=1.78342
CNTKCommandTrainEnd: dptPre1

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node cr. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 7 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node err. 15 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 6 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

7 out of 15 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 16 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 7 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL1.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

8 out of 16 nodes do not share the minibatch layout with the input data.

Post-processing network complete.
CNTKCommandTrainBegin: dptPre2
NDLBuilder Using CPU
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/Pre2/cntkSpeech.0.

Post-processing network...

3 roots:
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
	scaledLogLikelihood = Minus
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using CPU.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  5.03521500; EvalErr[0]PerSample = 0.83554688; TotalTime = 0.1170s; SamplesPerSecond = 21876.0
 Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.84799118; EvalErr[0]PerSample = 0.70156250; TotalTime = 0.1126s; SamplesPerSecond = 22741.2
 Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.43217316; EvalErr[0]PerSample = 0.63007813; TotalTime = 0.1126s; SamplesPerSecond = 22741.0
 Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.05319138; EvalErr[0]PerSample = 0.57929688; TotalTime = 0.1126s; SamplesPerSecond = 22726.5
 Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.81479416; EvalErr[0]PerSample = 0.50507813; TotalTime = 0.1126s; SamplesPerSecond = 22729.5
 Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.65699310; EvalErr[0]PerSample = 0.47656250; TotalTime = 0.1127s; SamplesPerSecond = 22722.6
 Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.60449066; EvalErr[0]PerSample = 0.46406250; TotalTime = 0.1135s; SamplesPerSecond = 22555.3
 Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.54690704; EvalErr[0]PerSample = 0.44570312; TotalTime = 0.1129s; SamplesPerSecond = 22683.4
 Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.45256042; EvalErr[0]PerSample = 0.43671875; TotalTime = 0.1145s; SamplesPerSecond = 22350.9
 Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.48195496; EvalErr[0]PerSample = 0.44882813; TotalTime = 0.1131s; SamplesPerSecond = 22634.2
 Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.39197998; EvalErr[0]PerSample = 0.41718750; TotalTime = 0.1129s; SamplesPerSecond = 22668.5
 Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.38458710; EvalErr[0]PerSample = 0.39726563; TotalTime = 0.1128s; SamplesPerSecond = 22699.7
 Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.32102051; EvalErr[0]PerSample = 0.39218750; TotalTime = 0.1127s; SamplesPerSecond = 22711.5
 Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.28715210; EvalErr[0]PerSample = 0.39609375; TotalTime = 0.1127s; SamplesPerSecond = 22709.3
 Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.30798645; EvalErr[0]PerSample = 0.38554688; TotalTime = 0.1129s; SamplesPerSecond = 22679.2
 Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.31445618; EvalErr[0]PerSample = 0.38984375; TotalTime = 0.1128s; SamplesPerSecond = 22704.3
 Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.26487427; EvalErr[0]PerSample = 0.36875000; TotalTime = 0.1129s; SamplesPerSecond = 22681.2
 Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.29167786; EvalErr[0]PerSample = 0.37460938; TotalTime = 0.1130s; SamplesPerSecond = 22651.9
 Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.24710999; EvalErr[0]PerSample = 0.37656250; TotalTime = 0.1128s; SamplesPerSecond = 22692.4
 Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.27827454; EvalErr[0]PerSample = 0.39023438; TotalTime = 0.1129s; SamplesPerSecond = 22678.5
 Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.25323792; EvalErr[0]PerSample = 0.37421875; TotalTime = 0.1127s; SamplesPerSecond = 22720.8
 Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.29031677; EvalErr[0]PerSample = 0.37695312; TotalTime = 0.1130s; SamplesPerSecond = 22663.7
 Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.21824341; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.1127s; SamplesPerSecond = 22707.5
 Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.25034180; EvalErr[0]PerSample = 0.37304688; TotalTime = 0.1128s; SamplesPerSecond = 22696.2
 Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.24739075; EvalErr[0]PerSample = 0.37890625; TotalTime = 0.1142s; SamplesPerSecond = 22419.2
 Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.22057800; EvalErr[0]PerSample = 0.37929687; TotalTime = 0.1127s; SamplesPerSecond = 22708.1
 Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.20379944; EvalErr[0]PerSample = 0.37265625; TotalTime = 0.1131s; SamplesPerSecond = 22630.8
 Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.19783020; EvalErr[0]PerSample = 0.36992188; TotalTime = 0.1132s; SamplesPerSecond = 22620.6
 Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.19216309; EvalErr[0]PerSample = 0.36367187; TotalTime = 0.1132s; SamplesPerSecond = 22623.0
 Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.17020264; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.1131s; SamplesPerSecond = 22643.4
 Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.20345154; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.1132s; SamplesPerSecond = 22619.6
 Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.17263489; EvalErr[0]PerSample = 0.36718750; TotalTime = 0.1125s; SamplesPerSecond = 22765.3
Finished Epoch[ 1 of 2]: [Training Set] TrainLossPerSample = 1.5511119; EvalErrPerSample = 0.43294677; AvgLearningRatePerSample = 0.003125; EpochTime=3.70099
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.18681622; EvalErr[0]PerSample = 0.36289063; TotalTime = 0.1129s; SamplesPerSecond = 22668.3
 Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.20656300; EvalErr[0]PerSample = 0.37031250; TotalTime = 0.1130s; SamplesPerSecond = 22660.7
 Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.18077774; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.1129s; SamplesPerSecond = 22682.4
 Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.16997910; EvalErr[0]PerSample = 0.35117188; TotalTime = 0.1131s; SamplesPerSecond = 22630.0
 Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.19292183; EvalErr[0]PerSample = 0.34843750; TotalTime = 0.1129s; SamplesPerSecond = 22676.5
 Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.14131966; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.1129s; SamplesPerSecond = 22683.4
 Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.15585327; EvalErr[0]PerSample = 0.35429688; TotalTime = 0.1129s; SamplesPerSecond = 22671.1
 Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.16749725; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.1128s; SamplesPerSecond = 22687.8
 Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.19115524; EvalErr[0]PerSample = 0.36523438; TotalTime = 0.1131s; SamplesPerSecond = 22626.8
 Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.15414658; EvalErr[0]PerSample = 0.34960938; TotalTime = 0.1132s; SamplesPerSecond = 22606.7
 Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.14885178; EvalErr[0]PerSample = 0.35312500; TotalTime = 0.1131s; SamplesPerSecond = 22626.2
 Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.13697357; EvalErr[0]PerSample = 0.34414062; TotalTime = 0.1132s; SamplesPerSecond = 22622.4
 Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.14294739; EvalErr[0]PerSample = 0.33398438; TotalTime = 0.1132s; SamplesPerSecond = 22606.1
 Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.11870117; EvalErr[0]PerSample = 0.35039063; TotalTime = 0.1134s; SamplesPerSecond = 22570.8
 Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.09258270; EvalErr[0]PerSample = 0.32812500; TotalTime = 0.1131s; SamplesPerSecond = 22625.4
 Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.08270874; EvalErr[0]PerSample = 0.33710937; TotalTime = 0.1133s; SamplesPerSecond = 22604.3
 Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.09057159; EvalErr[0]PerSample = 0.33906250; TotalTime = 0.1132s; SamplesPerSecond = 22622.6
 Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.09904175; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.1134s; SamplesPerSecond = 22578.1
 Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.07942047; EvalErr[0]PerSample = 0.32734375; TotalTime = 0.1132s; SamplesPerSecond = 22615.8
 Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.07688904; EvalErr[0]PerSample = 0.32539062; TotalTime = 0.1131s; SamplesPerSecond = 22636.2
 Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.07583466; EvalErr[0]PerSample = 0.33632812; TotalTime = 0.1133s; SamplesPerSecond = 22589.9
 Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.06828156; EvalErr[0]PerSample = 0.32109375; TotalTime = 0.1132s; SamplesPerSecond = 22613.4
 Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.08527069; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.1132s; SamplesPerSecond = 22609.4
 Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.08339844; EvalErr[0]PerSample = 0.32968750; TotalTime = 0.1132s; SamplesPerSecond = 22618.0
 Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.03799438; EvalErr[0]PerSample = 0.31523438; TotalTime = 0.1132s; SamplesPerSecond = 22613.8
 Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.11759033; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.1133s; SamplesPerSecond = 22604.1
 Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.11881714; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.1163s; SamplesPerSecond = 22003.1
 Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.11436768; EvalErr[0]PerSample = 0.34257813; TotalTime = 0.1131s; SamplesPerSecond = 22639.2
 Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.07169495; EvalErr[0]PerSample = 0.31914063; TotalTime = 0.1137s; SamplesPerSecond = 22523.1
 Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.06234131; EvalErr[0]PerSample = 0.32148437; TotalTime = 0.1132s; SamplesPerSecond = 22613.0
 Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.12825012; EvalErr[0]PerSample = 0.32695313; TotalTime = 0.1133s; SamplesPerSecond = 22603.1
 Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.03787231; EvalErr[0]PerSample = 0.32343750; TotalTime = 0.1124s; SamplesPerSecond = 22778.2
Finished Epoch[ 2 of 2]: [Training Set] TrainLossPerSample = 1.1192948; EvalErrPerSample = 0.33990479; AvgLearningRatePerSample = 0.003125; EpochTime=3.63007
CNTKCommandTrainEnd: dptPre2

Post-processing network...

3 roots:
	scaledLogLikelihood = Minus
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node scaledLogLikelihood. 21 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 11 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

10 out of 21 nodes do not share the minibatch layout with the input data.


Validating for node cr. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node err. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL2.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

Post-processing network complete.
CNTKCommandTrainBegin: speechTrain
NDLBuilder Using CPU
reading script file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151215163714.581330/Speech/DNN_DiscriminativePreTraining@release_cpu/models/cntkSpeech.0.

Post-processing network...

3 roots:
	scaledLogLikelihood = Minus
	cr = CrossEntropyWithSoftmax
	err = ErrorPrediction
FormNestedNetwork: WARNING: Was called twice for scaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for cr CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for err ErrorPrediction operation


Validating for node scaledLogLikelihood. 26 nodes to process in pass 1.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood. 14 nodes to process in pass 2.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

Validating for node scaledLogLikelihood, final verification.

Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> globalPrior = LearnableParameter -> [132, 1]
Validating --> logPrior = Log(globalPrior[132, 1]) -> [132, 1]
Validating --> scaledLogLikelihood = Minus(OL.z[132, MBSize 0], logPrior[132, 1]) -> [132, MBSize 0]

12 out of 26 nodes do not share the minibatch layout with the input data.


Validating for node cr. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node cr, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> cr = CrossEntropyWithSoftmax(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.


Validating for node err. 25 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

Validating for node err, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> OL.W = LearnableParameter -> [132, 512]
Validating --> HL3.W = LearnableParameter -> [512, 512]
Validating --> HL2.W = LearnableParameter -> [512, 512]
Validating --> HL1.W = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> globalMean = LearnableParameter -> [363, 1]
Validating --> globalInvStd = LearnableParameter -> [363, 1]
Validating --> featNorm = PerDimMeanVarNormalization(features[363, MBSize 0], globalMean[363, 1], globalInvStd[363, 1]) -> [363, MBSize 0]
Validating --> HL1.t = Times(HL1.W[512, 363], featNorm[363, MBSize 0]) -> [512, MBSize 0]
Validating --> HL1.b = LearnableParameter -> [512, 1]
Validating --> HL1.z = Plus(HL1.t[512, MBSize 0], HL1.b[512, 1]) -> [512, MBSize 0]
Validating --> HL1.y = Sigmoid(HL1.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.t = Times(HL2.W[512, 512], HL1.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL2.b = LearnableParameter -> [512, 1]
Validating --> HL2.z = Plus(HL2.t[512, MBSize 0], HL2.b[512, 1]) -> [512, MBSize 0]
Validating --> HL2.y = Sigmoid(HL2.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.t = Times(HL3.W[512, 512], HL2.y[512, MBSize 0]) -> [512, MBSize 0]
Validating --> HL3.b = LearnableParameter -> [512, 1]
Validating --> HL3.z = Plus(HL3.t[512, MBSize 0], HL3.b[512, 1]) -> [512, MBSize 0]
Validating --> HL3.y = Sigmoid(HL3.z[512, MBSize 0]) -> [512, MBSize 0]
Validating --> OL.t = Times(OL.W[132, 512], HL3.y[512, MBSize 0]) -> [132, MBSize 0]
Validating --> OL.b = LearnableParameter -> [132, 1]
Validating --> OL.z = Plus(OL.t[132, MBSize 0], OL.b[132, 1]) -> [132, MBSize 0]
Validating --> err = ErrorPrediction(labels[132, MBSize 0], OL.z[132, MBSize 0]) -> [1, 1]

11 out of 25 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using CPU.

Training criterion node(s):
	cr = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 2560; TrainLossPerSample =  4.07459412; EvalErr[0]PerSample = 0.83046875; TotalTime = 0.1847s; SamplesPerSecond = 13858.9
 Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 2560; TrainLossPerSample =  2.50037994; EvalErr[0]PerSample = 0.62343750; TotalTime = 0.1686s; SamplesPerSecond = 15184.7
 Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 2560; TrainLossPerSample =  2.11056366; EvalErr[0]PerSample = 0.56679687; TotalTime = 0.1684s; SamplesPerSecond = 15197.8
 Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.76141739; EvalErr[0]PerSample = 0.49101563; TotalTime = 0.1684s; SamplesPerSecond = 15202.2
 Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.57761765; EvalErr[0]PerSample = 0.45937500; TotalTime = 0.1684s; SamplesPerSecond = 15197.9
 Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.46414337; EvalErr[0]PerSample = 0.42500000; TotalTime = 0.1684s; SamplesPerSecond = 15199.9
 Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.43758545; EvalErr[0]PerSample = 0.42382812; TotalTime = 0.1685s; SamplesPerSecond = 15194.8
 Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.40687408; EvalErr[0]PerSample = 0.40742187; TotalTime = 0.1685s; SamplesPerSecond = 15192.8
 Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.32321014; EvalErr[0]PerSample = 0.39687500; TotalTime = 0.1688s; SamplesPerSecond = 15163.3
 Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.32419281; EvalErr[0]PerSample = 0.39687500; TotalTime = 0.1686s; SamplesPerSecond = 15185.8
 Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.27560730; EvalErr[0]PerSample = 0.38281250; TotalTime = 0.1685s; SamplesPerSecond = 15193.4
 Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.28661041; EvalErr[0]PerSample = 0.38359375; TotalTime = 0.1685s; SamplesPerSecond = 15190.5
 Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.21722565; EvalErr[0]PerSample = 0.35976562; TotalTime = 0.1685s; SamplesPerSecond = 15188.9
 Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.18934326; EvalErr[0]PerSample = 0.36562500; TotalTime = 0.1686s; SamplesPerSecond = 15183.9
 Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.22049561; EvalErr[0]PerSample = 0.37109375; TotalTime = 0.1697s; SamplesPerSecond = 15089.0
 Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.24593201; EvalErr[0]PerSample = 0.37773438; TotalTime = 0.1697s; SamplesPerSecond = 15083.7
 Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.19860229; EvalErr[0]PerSample = 0.36171875; TotalTime = 0.1695s; SamplesPerSecond = 15099.4
 Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.20980530; EvalErr[0]PerSample = 0.35429688; TotalTime = 0.1703s; SamplesPerSecond = 15033.4
 Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.16387939; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.1694s; SamplesPerSecond = 15114.8
 Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.19475403; EvalErr[0]PerSample = 0.36250000; TotalTime = 0.1696s; SamplesPerSecond = 15094.4
 Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.20151978; EvalErr[0]PerSample = 0.36835937; TotalTime = 0.1696s; SamplesPerSecond = 15091.0
 Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.21634521; EvalErr[0]PerSample = 0.36289063; TotalTime = 0.1695s; SamplesPerSecond = 15100.4
 Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.14917908; EvalErr[0]PerSample = 0.35273437; TotalTime = 0.1694s; SamplesPerSecond = 15112.1
 Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.18493958; EvalErr[0]PerSample = 0.35546875; TotalTime = 0.1694s; SamplesPerSecond = 15113.4
 Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 2560; TrainLossPerSample =  1.17961426; EvalErr[0]PerSample = 0.36015625; TotalTime = 0.1694s; SamplesPerSecond = 15109.8
 Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 2560; TrainLossPerSample =  1.16185913; EvalErr[0]PerSample = 0.35820313; TotalTime = 0.1701s; SamplesPerSecond = 15049.1
 Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 2560; TrainLossPerSample =  1.13928528; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.1697s; SamplesPerSecond = 15084.4
 Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.12674561; EvalErr[0]PerSample = 0.34960938; TotalTime = 0.1695s; SamplesPerSecond = 15103.5
 Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 2560; TrainLossPerSample =  1.12367554; EvalErr[0]PerSample = 0.34101562; TotalTime = 0.1694s; SamplesPerSecond = 15111.9
 Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 2560; TrainLossPerSample =  1.11361084; EvalErr[0]PerSample = 0.33750000; TotalTime = 0.1693s; SamplesPerSecond = 15118.5
 Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 2560; TrainLossPerSample =  1.13889771; EvalErr[0]PerSample = 0.35859375; TotalTime = 0.1695s; SamplesPerSecond = 15102.4
 Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.14187012; EvalErr[0]PerSample = 0.34648438; TotalTime = 0.1690s; SamplesPerSecond = 15149.2
Finished Epoch[ 1 of 4]: [Training Set] TrainLossPerSample = 1.4081367; EvalErrPerSample = 0.40462646; AvgLearningRatePerSample = 0.003125; EpochTime=5.50636
Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.24151039; EvalErr[0]PerSample = 0.37578125; TotalTime = 0.3119s; SamplesPerSecond = 16415.7
 Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.18116150; EvalErr[0]PerSample = 0.35937500; TotalTime = 0.3101s; SamplesPerSecond = 16509.2
 Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.17582817; EvalErr[0]PerSample = 0.35683594; TotalTime = 0.3122s; SamplesPerSecond = 16397.2
 Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.16649361; EvalErr[0]PerSample = 0.35585937; TotalTime = 0.3124s; SamplesPerSecond = 16387.3
 Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.16477623; EvalErr[0]PerSample = 0.35292969; TotalTime = 0.3123s; SamplesPerSecond = 16395.7
 Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.13575516; EvalErr[0]PerSample = 0.35136719; TotalTime = 0.3127s; SamplesPerSecond = 16371.4
 Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.16274414; EvalErr[0]PerSample = 0.35468750; TotalTime = 0.3127s; SamplesPerSecond = 16375.5
 Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.12048340; EvalErr[0]PerSample = 0.34042969; TotalTime = 0.3124s; SamplesPerSecond = 16391.9
 Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.13913651; EvalErr[0]PerSample = 0.34570312; TotalTime = 0.3123s; SamplesPerSecond = 16395.0
 Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.07811508; EvalErr[0]PerSample = 0.33105469; TotalTime = 0.3123s; SamplesPerSecond = 16395.0
 Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.06469193; EvalErr[0]PerSample = 0.32812500; TotalTime = 0.3123s; SamplesPerSecond = 16397.0
 Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.07235565; EvalErr[0]PerSample = 0.33085938; TotalTime = 0.3139s; SamplesPerSecond = 16309.4
 Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.05898132; EvalErr[0]PerSample = 0.32128906; TotalTime = 0.3126s; SamplesPerSecond = 16380.1
 Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.12796021; EvalErr[0]PerSample = 0.34589844; TotalTime = 0.3121s; SamplesPerSecond = 16402.7
 Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07443848; EvalErr[0]PerSample = 0.32734375; TotalTime = 0.3122s; SamplesPerSecond = 16398.3
 Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.09533539; EvalErr[0]PerSample = 0.33125000; TotalTime = 0.3111s; SamplesPerSecond = 16455.5
Finished Epoch[ 2 of 4]: [Training Set] TrainLossPerSample = 1.1287354; EvalErrPerSample = 0.34429932; AvgLearningRatePerSample = 0.003125; EpochTime=5.00024
Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.10176105; EvalErr[0]PerSample = 0.34277344; TotalTime = 0.3095s; SamplesPerSecond = 16545.3
 Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.16201744; EvalErr[0]PerSample = 0.35625000; TotalTime = 0.3100s; SamplesPerSecond = 16514.9
 Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.10433636; EvalErr[0]PerSample = 0.34218750; TotalTime = 0.3125s; SamplesPerSecond = 16383.9
 Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.05510216; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.3124s; SamplesPerSecond = 16390.0
 Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.05669708; EvalErr[0]PerSample = 0.33574219; TotalTime = 0.3123s; SamplesPerSecond = 16394.0
 Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.06401596; EvalErr[0]PerSample = 0.33261719; TotalTime = 0.3126s; SamplesPerSecond = 16376.2
 Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.07696609; EvalErr[0]PerSample = 0.33183594; TotalTime = 0.3121s; SamplesPerSecond = 16402.7
 Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.00777740; EvalErr[0]PerSample = 0.31757812; TotalTime = 0.3130s; SamplesPerSecond = 16358.0
 Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.06549988; EvalErr[0]PerSample = 0.32460937; TotalTime = 0.3124s; SamplesPerSecond = 16391.1
 Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.10392532; EvalErr[0]PerSample = 0.33886719; TotalTime = 0.3125s; SamplesPerSecond = 16384.5
 Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.06218948; EvalErr[0]PerSample = 0.32890625; TotalTime = 0.3122s; SamplesPerSecond = 16399.4
 Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.05725784; EvalErr[0]PerSample = 0.33476563; TotalTime = 0.3127s; SamplesPerSecond = 16375.2
 Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.03517914; EvalErr[0]PerSample = 0.32050781; TotalTime = 0.3124s; SamplesPerSecond = 16391.0
 Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.02237244; EvalErr[0]PerSample = 0.31464844; TotalTime = 0.3122s; SamplesPerSecond = 16399.0
 Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.06224976; EvalErr[0]PerSample = 0.32910156; TotalTime = 0.3122s; SamplesPerSecond = 16399.3
 Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.06768646; EvalErr[0]PerSample = 0.33652344; TotalTime = 0.3112s; SamplesPerSecond = 16451.7
Finished Epoch[ 3 of 4]: [Training Set] TrainLossPerSample = 1.0690646; EvalErrPerSample = 0.33223876; AvgLearningRatePerSample = 0.003125; EpochTime=4.99667
Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.05605412; EvalErr[0]PerSample = 0.33027344; TotalTime = 0.3100s; SamplesPerSecond = 16515.6
 Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: SamplesSeen = 4926; TrainLossPerSample =  1.03406768; EvalErr[0]PerSample = 0.32034105; TotalTime = 0.3406s; SamplesPerSecond = 14460.7
 Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.02853432; EvalErr[0]PerSample = 0.32636719; TotalTime = 0.3100s; SamplesPerSecond = 16517.1
 Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.98784370; EvalErr[0]PerSample = 0.30488281; TotalTime = 0.3121s; SamplesPerSecond = 16403.3
 Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: SamplesSeen = 5120; TrainLossPerSample =  1.02745323; EvalErr[0]PerSample = 0.31601563; TotalTime = 0.3123s; SamplesPerSecond = 16394.4
 Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: SamplesSeen = 5120; TrainLossPerSample =  0.99942627; EvalErr[0]PerSample = 0.30996094; TotalTime = 0.3121s; SamplesPerSecond = 16406.8
 Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.02917633; EvalErr[0]PerSample = 0.32460937; TotalTime = 0.3121s; SamplesPerSecond = 16405.1
 Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.99381027; EvalErr[0]PerSample = 0.31308594; TotalTime = 0.3120s; SamplesPerSecond = 16409.9
 Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: SamplesSeen = 5120; TrainLossPerSample =  0.98878403; EvalErr[0]PerSample = 0.31035156; TotalTime = 0.3121s; SamplesPerSecond = 16405.5
 Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.01263962; EvalErr[0]PerSample = 0.31347656; TotalTime = 0.3123s; SamplesPerSecond = 16396.0
 Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.00671692; EvalErr[0]PerSample = 0.30976562; TotalTime = 0.3124s; SamplesPerSecond = 16389.9
 Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: SamplesSeen = 5120; TrainLossPerSample =  1.02811584; EvalErr[0]PerSample = 0.31972656; TotalTime = 0.3131s; SamplesPerSecond = 16351.1
 Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: SamplesSeen = 5120; TrainLossPerSample =  0.99318237; EvalErr[0]PerSample = 0.30898437; TotalTime = 0.3118s; SamplesPerSecond = 16421.0
 Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: SamplesSeen = 5120; TrainLossPerSample =  1.00085297; EvalErr[0]PerSample = 0.31757812; TotalTime = 0.3120s; SamplesPerSecond = 16409.8
 Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: SamplesSeen = 5120; TrainLossPerSample =  1.01911316; EvalErr[0]PerSample = 0.31621094; TotalTime = 0.3119s; SamplesPerSecond = 16413.4
 Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: SamplesSeen = 5120; TrainLossPerSample =  0.95961304; EvalErr[0]PerSample = 0.30605469; TotalTime = 0.3116s; SamplesPerSecond = 16433.5
Finished Epoch[ 4 of 4]: [Training Set] TrainLossPerSample = 1.0102775; EvalErrPerSample = 0.31549072; AvgLearningRatePerSample = 0.003125; EpochTime=5.03568
CNTKCommandTrainEnd: speechTrain
COMPLETED