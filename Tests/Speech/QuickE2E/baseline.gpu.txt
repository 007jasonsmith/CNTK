=== Running /home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/QuickE2E/cntk.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E DeviceId=0
-------------------------------------------------------------------
Build info: 

		Built time: Dec 15 2015 16:32:52
		Last modified date: Tue Dec 15 16:31:42 2015
		Build type: release
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.0
		CUB_PATH: /usr/local/cub-1.4.1
		Build Branch: master
		Build SHA1: 5e0017ac9c55c23d53cb524c8acb7d6d9bfd0269
-------------------------------------------------------------------
running on localhost at 2015/12/15 16:38:10
command line: 
/home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/QuickE2E/cntk.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E DeviceId=0 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision = "float"
command = speechTrain
deviceId = $DeviceId$
parallelTrain = false
makeMode = false
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.dnn"
    deviceId = $DeviceId$
    traceLevel = 1
    SimpleNetworkBuilder = [
        layerSizes = 363:512:512:132
        trainingCriterion = "CrossEntropyWithSoftmax"
        evalCriterion = "ErrorPrediction"
        layerTypes = "Sigmoid"
        applyMeanVarNorm = true
        initValueScale = 1.0
        uniformInit = true
        needPrior = true
    ]
    ExperimentalNetworkBuilder = [    // the same as above but with BS. Currently not used. Enable by removing the SimpleNetworkBuilder above.
        // note: this does not produce identical results because of different initialization order of random-initialized LearnableParameters
        layerSizes=363:512:512:132  // [0..]
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes[i:1..Length(layerSizes)-2]=Sigmoid
        applyMeanVarNorm=true
        initValueScale=1.0
        uniformInit=true
        BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols, init = if uniformInit then 'uniform' else 'gaussian'/*, initValueScale from outer scope*/) ; z = W*in+B ]
        GBFF(f, in, rows, cols) = [ Eh = rows,f(BFF(in, rows, cols).z) ]
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then GBFF(layerTypes[layer], layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else GBFF(layerTypes[layer], featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = trainingCriterion(labels, outZ, tag='criterion')
        Err = evalCriterion(labels, outZ, tag='eval')
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD = [
        epochSize = 20480
        minibatchSize = 64:256:1024:
        learningRatesPerMB = 1.0:0.5:0.1
        numMBsToShowResult = 10
        momentumPerMB = 0.9:0.656119
        dropoutRate = 0.0
        maxEpochs = 3
        keepCheckPointFiles = true
        AutoAdjust = [
            reduceLearnRateIfImproveLessThan = 0
            loadBestModel = true
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            autoAdjustLR = "adjustAfterEpoch"
        ]
        clippingThresholdPerSample = 1#INF
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision = "float"
command = speechTrain
deviceId = 0
parallelTrain = false
makeMode = false
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu/models/cntkSpeech.dnn"
    deviceId = 0
    traceLevel = 1
    SimpleNetworkBuilder = [
        layerSizes = 363:512:512:132
        trainingCriterion = "CrossEntropyWithSoftmax"
        evalCriterion = "ErrorPrediction"
        layerTypes = "Sigmoid"
        applyMeanVarNorm = true
        initValueScale = 1.0
        uniformInit = true
        needPrior = true
    ]
    ExperimentalNetworkBuilder = [    // the same as above but with BS. Currently not used. Enable by removing the SimpleNetworkBuilder above.
        // note: this does not produce identical results because of different initialization order of random-initialized LearnableParameters
        layerSizes=363:512:512:132  // [0..]
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes[i:1..Length(layerSizes)-2]=Sigmoid
        applyMeanVarNorm=true
        initValueScale=1.0
        uniformInit=true
        BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols, init = if uniformInit then 'uniform' else 'gaussian'/*, initValueScale from outer scope*/) ; z = W*in+B ]
        GBFF(f, in, rows, cols) = [ Eh = rows,f(BFF(in, rows, cols).z) ]
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then GBFF(layerTypes[layer], layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else GBFF(layerTypes[layer], featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = trainingCriterion(labels, outZ, tag='criterion')
        Err = evalCriterion(labels, outZ, tag='eval')
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD = [
        epochSize = 20480
        minibatchSize = 64:256:1024:
        learningRatesPerMB = 1.0:0.5:0.1
        numMBsToShowResult = 10
        momentumPerMB = 0.9:0.656119
        dropoutRate = 0.0
        maxEpochs = 3
        keepCheckPointFiles = true
        AutoAdjust = [
            reduceLearnRateIfImproveLessThan = 0
            loadBestModel = true
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            autoAdjustLR = "adjustAfterEpoch"
        ]
        clippingThresholdPerSample = 1#INF
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "glob_0000.scp"
        ]
        labels = [
            mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
            labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
            labelDim = 132
            labelType = "category"
        ]
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E
DeviceId=0

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.config:command=speechTrain
configparameters: cntk.config:ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E
configparameters: cntk.config:currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk.config:DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk.config:deviceId=0
configparameters: cntk.config:makeMode=false
configparameters: cntk.config:parallelTrain=false
configparameters: cntk.config:precision=float
configparameters: cntk.config:RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu
configparameters: cntk.config:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu/models/cntkSpeech.dnn"
    deviceId = 0
    traceLevel = 1
    SimpleNetworkBuilder = [
        layerSizes = 363:512:512:132
        trainingCriterion = "CrossEntropyWithSoftmax"
        evalCriterion = "ErrorPrediction"
        layerTypes = "Sigmoid"
        applyMeanVarNorm = true
        initValueScale = 1.0
        uniformInit = true
        needPrior = true
    ]
    ExperimentalNetworkBuilder = [    // the same as above but with BS. Currently not used. Enable by removing the SimpleNetworkBuilder above.
        // note: this does not produce identical results because of different initialization order of random-initialized LearnableParameters
        layerSizes=363:512:512:132  // [0..]
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes[i:1..Length(layerSizes)-2]=Sigmoid
        applyMeanVarNorm=true
        initValueScale=1.0
        uniformInit=true
        BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols, init = if uniformInit then 'uniform' else 'gaussian'/*, initValueScale from outer scope*/) ; z = W*in+B ]
        GBFF(f, in, rows, cols) = [ Eh = rows,f(BFF(in, rows, cols).z) ]
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then GBFF(layerTypes[layer], layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else GBFF(layerTypes[layer], featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = trainingCriterion(labels, outZ, tag='criterion')
        Err = evalCriterion(labels, outZ, tag='eval')
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD = [
        epochSize = 20480
        minibatchSize = 64:256:1024:
        learningRatesPerMB = 1.0:0.5:0.1
        numMBsToShowResult = 10
        momentumPerMB = 0.9:0.656119
        dropoutRate = 0.0
        maxEpochs = 3
        keepCheckPointFiles = true
        AutoAdjust = [
            reduceLearnRateIfImproveLessThan = 0
            loadBestModel = true
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            autoAdjustLR = "adjustAfterEpoch"
        ]
        clippingThresholdPerSample = 1#INF
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "glob_0000.scp"
        ]
        labels = [
            mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
            labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
            labelDim = 132
            labelType = "category"
        ]
    ]
]

<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: speechTrain 
precision = float
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu/models/cntkSpeech.dnn
CNTKCommandTrainInfo: speechTrain : 3
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
CNTKCommandTrainBegin: speechTrain
SimpleNetworkBuilder Using GPU 0
reading script file glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

7 roots:
	InvStdOfFeatures = InvStdDev
	MeanOfFeatures = Mean
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax
	EvalErrorPrediction = ErrorPrediction
	Prior = Mean
	ScaledLogLikelihood = Minus
	PosteriorProb = Softmax
FormNestedNetwork: WARNING: Was called twice for InvStdOfFeatures InvStdDev operation
FormNestedNetwork: WARNING: Was called twice for MeanOfFeatures Mean operation
FormNestedNetwork: WARNING: Was called twice for CrossEntropyWithSoftmax CrossEntropyWithSoftmax operation
FormNestedNetwork: WARNING: Was called twice for EvalErrorPrediction ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for Prior Mean operation
FormNestedNetwork: WARNING: Was called twice for ScaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for PosteriorProb Softmax operation


Validating for node InvStdOfFeatures. 2 nodes to process in pass 1.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]

Validating for node InvStdOfFeatures. 1 nodes to process in pass 2.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]

Validating for node InvStdOfFeatures, final verification.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.


Validating for node MeanOfFeatures. 2 nodes to process in pass 1.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]

Validating for node MeanOfFeatures. 1 nodes to process in pass 2.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]

Validating for node MeanOfFeatures, final verification.

Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.


Validating for node CrossEntropyWithSoftmax. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 3], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax. 10 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 3], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 3], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node EvalErrorPrediction. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 3], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 3], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction, final verification.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 3], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node Prior. 2 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]

Validating for node Prior. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]

Validating for node Prior, final verification.

Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.


Validating for node ScaledLogLikelihood. 22 nodes to process in pass 1.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 3]
Validating --> Prior = Mean(labels[132, MBSize 3]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

10 out of 22 nodes do not share the minibatch layout with the input data.


Validating for node PosteriorProb. 19 nodes to process in pass 1.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> PosteriorProb = Softmax(HLast[132, MBSize 0]) -> [132, MBSize 0]

Validating for node PosteriorProb. 9 nodes to process in pass 2.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> PosteriorProb = Softmax(HLast[132, MBSize 0]) -> [132, MBSize 0]

Validating for node PosteriorProb, final verification.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 3]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 3]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 3]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 3], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 3]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 3]) -> [512, MBSize 3]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 3], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> PosteriorProb = Softmax(HLast[132, MBSize 0]) -> [132, MBSize 0]

8 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	EvalErrorPrediction = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Precomputing --> 3 PreCompute nodes found.

	NodeName: InvStdOfFeatures
	NodeName: MeanOfFeatures
	NodeName: Prior
minibatchiterator: epoch 0: frames [0..252734] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Precomputing --> Completed.

Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 1: learning rate per sample = 0.015625  effective momentum = 0.900000  momentum as time constant = 607.4 samples
minibatchiterator: epoch 0: frames [0..20480] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 1 of 3]-Minibatch[   1-  10, 3.12%]: SamplesSeen = 640; TrainLossPerSample =  4.57883224; EvalErr[0]PerSample = 0.91406250; TotalTime = 0.0872s; SamplesPerSecond = 7340.0
 Epoch[ 1 of 3]-Minibatch[  11-  20, 6.25%]: SamplesSeen = 640; TrainLossPerSample =  4.24617233; EvalErr[0]PerSample = 0.89687500; TotalTime = 0.0850s; SamplesPerSecond = 7529.4
 Epoch[ 1 of 3]-Minibatch[  21-  30, 9.38%]: SamplesSeen = 640; TrainLossPerSample =  4.05467224; EvalErr[0]PerSample = 0.89843750; TotalTime = 0.0851s; SamplesPerSecond = 7518.4
 Epoch[ 1 of 3]-Minibatch[  31-  40, 12.50%]: SamplesSeen = 640; TrainLossPerSample =  3.83654022; EvalErr[0]PerSample = 0.87031250; TotalTime = 0.0851s; SamplesPerSecond = 7519.9
 Epoch[ 1 of 3]-Minibatch[  41-  50, 15.62%]: SamplesSeen = 640; TrainLossPerSample =  3.71162415; EvalErr[0]PerSample = 0.87500000; TotalTime = 0.0855s; SamplesPerSecond = 7486.5
 Epoch[ 1 of 3]-Minibatch[  51-  60, 18.75%]: SamplesSeen = 640; TrainLossPerSample =  3.50715485; EvalErr[0]PerSample = 0.83906250; TotalTime = 0.0852s; SamplesPerSecond = 7515.4
 Epoch[ 1 of 3]-Minibatch[  61-  70, 21.88%]: SamplesSeen = 640; TrainLossPerSample =  3.60511627; EvalErr[0]PerSample = 0.82187500; TotalTime = 0.0851s; SamplesPerSecond = 7521.2
 Epoch[ 1 of 3]-Minibatch[  71-  80, 25.00%]: SamplesSeen = 640; TrainLossPerSample =  3.34199219; EvalErr[0]PerSample = 0.77500000; TotalTime = 0.0851s; SamplesPerSecond = 7519.2
 Epoch[ 1 of 3]-Minibatch[  81-  90, 28.12%]: SamplesSeen = 640; TrainLossPerSample =  3.50149536; EvalErr[0]PerSample = 0.82656250; TotalTime = 0.0855s; SamplesPerSecond = 7486.2
 Epoch[ 1 of 3]-Minibatch[  91- 100, 31.25%]: SamplesSeen = 640; TrainLossPerSample =  3.45998840; EvalErr[0]PerSample = 0.82343750; TotalTime = 0.0850s; SamplesPerSecond = 7526.9
WARNING: The same matrix with dim [1, 1] has been transferred between different devices for 20 times.
 Epoch[ 1 of 3]-Minibatch[ 101- 110, 34.38%]: SamplesSeen = 640; TrainLossPerSample =  3.39170532; EvalErr[0]PerSample = 0.82187500; TotalTime = 0.0851s; SamplesPerSecond = 7519.2
 Epoch[ 1 of 3]-Minibatch[ 111- 120, 37.50%]: SamplesSeen = 640; TrainLossPerSample =  3.23712463; EvalErr[0]PerSample = 0.80937500; TotalTime = 0.0851s; SamplesPerSecond = 7517.5
 Epoch[ 1 of 3]-Minibatch[ 121- 130, 40.62%]: SamplesSeen = 640; TrainLossPerSample =  3.21520996; EvalErr[0]PerSample = 0.80156250; TotalTime = 0.0852s; SamplesPerSecond = 7514.8
 Epoch[ 1 of 3]-Minibatch[ 131- 140, 43.75%]: SamplesSeen = 640; TrainLossPerSample =  3.10167236; EvalErr[0]PerSample = 0.77187500; TotalTime = 0.0853s; SamplesPerSecond = 7501.3
 Epoch[ 1 of 3]-Minibatch[ 141- 150, 46.88%]: SamplesSeen = 640; TrainLossPerSample =  3.02088013; EvalErr[0]PerSample = 0.72968750; TotalTime = 0.0851s; SamplesPerSecond = 7524.7
 Epoch[ 1 of 3]-Minibatch[ 151- 160, 50.00%]: SamplesSeen = 640; TrainLossPerSample =  3.00322876; EvalErr[0]PerSample = 0.72343750; TotalTime = 0.0851s; SamplesPerSecond = 7519.1
 Epoch[ 1 of 3]-Minibatch[ 161- 170, 53.12%]: SamplesSeen = 640; TrainLossPerSample =  2.85173340; EvalErr[0]PerSample = 0.69218750; TotalTime = 0.0851s; SamplesPerSecond = 7524.5
 Epoch[ 1 of 3]-Minibatch[ 171- 180, 56.25%]: SamplesSeen = 640; TrainLossPerSample =  2.81376343; EvalErr[0]PerSample = 0.68750000; TotalTime = 0.0854s; SamplesPerSecond = 7490.8
 Epoch[ 1 of 3]-Minibatch[ 181- 190, 59.38%]: SamplesSeen = 640; TrainLossPerSample =  2.79284058; EvalErr[0]PerSample = 0.70468750; TotalTime = 0.0851s; SamplesPerSecond = 7518.6
 Epoch[ 1 of 3]-Minibatch[ 191- 200, 62.50%]: SamplesSeen = 640; TrainLossPerSample =  2.69384155; EvalErr[0]PerSample = 0.66718750; TotalTime = 0.0850s; SamplesPerSecond = 7530.2
 Epoch[ 1 of 3]-Minibatch[ 201- 210, 65.62%]: SamplesSeen = 640; TrainLossPerSample =  2.68026733; EvalErr[0]PerSample = 0.66562500; TotalTime = 0.0851s; SamplesPerSecond = 7519.3
 Epoch[ 1 of 3]-Minibatch[ 211- 220, 68.75%]: SamplesSeen = 640; TrainLossPerSample =  2.51867065; EvalErr[0]PerSample = 0.62812500; TotalTime = 0.0852s; SamplesPerSecond = 7510.6
 Epoch[ 1 of 3]-Minibatch[ 221- 230, 71.88%]: SamplesSeen = 640; TrainLossPerSample =  2.46585693; EvalErr[0]PerSample = 0.67031250; TotalTime = 0.0852s; SamplesPerSecond = 7509.6
 Epoch[ 1 of 3]-Minibatch[ 231- 240, 75.00%]: SamplesSeen = 640; TrainLossPerSample =  2.32813110; EvalErr[0]PerSample = 0.60781250; TotalTime = 0.0851s; SamplesPerSecond = 7517.6
 Epoch[ 1 of 3]-Minibatch[ 241- 250, 78.12%]: SamplesSeen = 640; TrainLossPerSample =  2.31088257; EvalErr[0]PerSample = 0.61093750; TotalTime = 0.0851s; SamplesPerSecond = 7523.9
 Epoch[ 1 of 3]-Minibatch[ 251- 260, 81.25%]: SamplesSeen = 640; TrainLossPerSample =  2.41295776; EvalErr[0]PerSample = 0.63750000; TotalTime = 0.0850s; SamplesPerSecond = 7530.4
 Epoch[ 1 of 3]-Minibatch[ 261- 270, 84.38%]: SamplesSeen = 640; TrainLossPerSample =  2.54532471; EvalErr[0]PerSample = 0.66562500; TotalTime = 0.0855s; SamplesPerSecond = 7484.3
 Epoch[ 1 of 3]-Minibatch[ 271- 280, 87.50%]: SamplesSeen = 640; TrainLossPerSample =  2.35637207; EvalErr[0]PerSample = 0.64218750; TotalTime = 0.0851s; SamplesPerSecond = 7524.9
 Epoch[ 1 of 3]-Minibatch[ 281- 290, 90.62%]: SamplesSeen = 640; TrainLossPerSample =  2.19710693; EvalErr[0]PerSample = 0.56250000; TotalTime = 0.0850s; SamplesPerSecond = 7525.6
 Epoch[ 1 of 3]-Minibatch[ 291- 300, 93.75%]: SamplesSeen = 640; TrainLossPerSample =  2.27369385; EvalErr[0]PerSample = 0.59687500; TotalTime = 0.0851s; SamplesPerSecond = 7521.4
 Epoch[ 1 of 3]-Minibatch[ 301- 310, 96.88%]: SamplesSeen = 640; TrainLossPerSample =  2.21437378; EvalErr[0]PerSample = 0.61406250; TotalTime = 0.0853s; SamplesPerSecond = 7499.8
 Epoch[ 1 of 3]-Minibatch[ 311- 320, 100.00%]: SamplesSeen = 640; TrainLossPerSample =  2.19765625; EvalErr[0]PerSample = 0.58437500; TotalTime = 0.0852s; SamplesPerSecond = 7513.7
Finished Epoch[ 1 of 3]: [Training Set] TrainLossPerSample = 3.01459; EvalErrPerSample = 0.73237306; AvgLearningRatePerSample = 0.015625; EpochTime=2.73207
Starting Epoch 2: learning rate per sample = 0.001953  effective momentum = 0.656119  momentum as time constant = 607.5 samples
minibatchiterator: epoch 1: frames [20480..40960] (first utterance at frame 20480), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 2 of 3]-Minibatch[   1-  10, 12.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.06978111; EvalErr[0]PerSample = 0.57382813; TotalTime = 0.1180s; SamplesPerSecond = 21694.4
 Epoch[ 2 of 3]-Minibatch[  11-  20, 25.00%]: SamplesSeen = 2560; TrainLossPerSample =  2.06690979; EvalErr[0]PerSample = 0.56445312; TotalTime = 0.1154s; SamplesPerSecond = 22191.6
 Epoch[ 2 of 3]-Minibatch[  21-  30, 37.50%]: SamplesSeen = 2560; TrainLossPerSample =  2.00655518; EvalErr[0]PerSample = 0.54609375; TotalTime = 0.1156s; SamplesPerSecond = 22148.4
 Epoch[ 2 of 3]-Minibatch[  31-  40, 50.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.98024254; EvalErr[0]PerSample = 0.53242188; TotalTime = 0.1156s; SamplesPerSecond = 22154.9
 Epoch[ 2 of 3]-Minibatch[  41-  50, 62.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.94628906; EvalErr[0]PerSample = 0.53828125; TotalTime = 0.1155s; SamplesPerSecond = 22158.9
 Epoch[ 2 of 3]-Minibatch[  51-  60, 75.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.92739944; EvalErr[0]PerSample = 0.53945312; TotalTime = 0.1154s; SamplesPerSecond = 22184.9
 Epoch[ 2 of 3]-Minibatch[  61-  70, 87.50%]: SamplesSeen = 2560; TrainLossPerSample =  1.91814880; EvalErr[0]PerSample = 0.53515625; TotalTime = 0.1154s; SamplesPerSecond = 22180.6
 Epoch[ 2 of 3]-Minibatch[  71-  80, 100.00%]: SamplesSeen = 2560; TrainLossPerSample =  1.92855072; EvalErr[0]PerSample = 0.54140625; TotalTime = 0.1154s; SamplesPerSecond = 22185.1
Finished Epoch[ 2 of 3]: [Training Set] TrainLossPerSample = 1.9804846; EvalErrPerSample = 0.54638672; AvgLearningRatePerSample = 0.001953125; EpochTime=0.928856
Starting Epoch 3: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
minibatchiterator: epoch 2: frames [40960..61440] (first utterance at frame 40960), data subset 0 of 1, with 1 datapasses

Starting minibatch loop.
 Epoch[ 3 of 3]-Minibatch[   1-  10, 50.00%]: SamplesSeen = 10240; TrainLossPerSample =  1.91202354; EvalErr[0]PerSample = 0.52734375; TotalTime = 0.3490s; SamplesPerSecond = 29344.0
 Epoch[ 3 of 3]-Minibatch[  11-  20, 100.00%]: SamplesSeen = 10240; TrainLossPerSample =  1.91116600; EvalErr[0]PerSample = 0.52255859; TotalTime = 0.3407s; SamplesPerSecond = 30058.7
Finished Epoch[ 3 of 3]: [Training Set] TrainLossPerSample = 1.9115947; EvalErrPerSample = 0.52495116; AvgLearningRatePerSample = 9.7656251e-05; EpochTime=0.693648
CNTKCommandTrainEnd: speechTrain
COMPLETED
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running /home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/QuickE2E/cntk.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E DeviceId=0 makeMode=true
-------------------------------------------------------------------
Build info: 

		Built time: Dec 15 2015 16:32:52
		Last modified date: Tue Dec 15 16:31:42 2015
		Build type: release
		Math lib: acml
		CUDA_PATH: /usr/local/cuda-7.0
		CUB_PATH: /usr/local/cub-1.4.1
		Build Branch: master
		Build SHA1: 5e0017ac9c55c23d53cb524c8acb7d6d9bfd0269
-------------------------------------------------------------------
running on localhost at 2015/12/15 16:38:17
command line: 
/home/alrezni/src/cntk/build/release/bin/cntk configFile=/home/alrezni/src/cntk/Tests/Speech/QuickE2E/cntk.config currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu DataDir=/home/alrezni/src/cntk/Tests/Speech/Data ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E DeviceId=0 makeMode=true 

>>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
precision = "float"
command = speechTrain
deviceId = $DeviceId$
parallelTrain = false
makeMode = false
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.dnn"
    deviceId = $DeviceId$
    traceLevel = 1
    SimpleNetworkBuilder = [
        layerSizes = 363:512:512:132
        trainingCriterion = "CrossEntropyWithSoftmax"
        evalCriterion = "ErrorPrediction"
        layerTypes = "Sigmoid"
        applyMeanVarNorm = true
        initValueScale = 1.0
        uniformInit = true
        needPrior = true
    ]
    ExperimentalNetworkBuilder = [    // the same as above but with BS. Currently not used. Enable by removing the SimpleNetworkBuilder above.
        // note: this does not produce identical results because of different initialization order of random-initialized LearnableParameters
        layerSizes=363:512:512:132  // [0..]
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes[i:1..Length(layerSizes)-2]=Sigmoid
        applyMeanVarNorm=true
        initValueScale=1.0
        uniformInit=true
        BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols, init = if uniformInit then 'uniform' else 'gaussian'/*, initValueScale from outer scope*/) ; z = W*in+B ]
        GBFF(f, in, rows, cols) = [ Eh = rows,f(BFF(in, rows, cols).z) ]
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then GBFF(layerTypes[layer], layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else GBFF(layerTypes[layer], featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = trainingCriterion(labels, outZ, tag='criterion')
        Err = evalCriterion(labels, outZ, tag='eval')
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD = [
        epochSize = 20480
        minibatchSize = 64:256:1024:
        learningRatesPerMB = 1.0:0.5:0.1
        numMBsToShowResult = 10
        momentumPerMB = 0.9:0.656119
        dropoutRate = 0.0
        maxEpochs = 3
        keepCheckPointFiles = true
        AutoAdjust = [
            reduceLearnRateIfImproveLessThan = 0
            loadBestModel = true
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            autoAdjustLR = "adjustAfterEpoch"
        ]
        clippingThresholdPerSample = 1#INF
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E
DeviceId=0
makeMode=true

<<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
precision = "float"
command = speechTrain
deviceId = 0
parallelTrain = false
makeMode = false
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu/models/cntkSpeech.dnn"
    deviceId = 0
    traceLevel = 1
    SimpleNetworkBuilder = [
        layerSizes = 363:512:512:132
        trainingCriterion = "CrossEntropyWithSoftmax"
        evalCriterion = "ErrorPrediction"
        layerTypes = "Sigmoid"
        applyMeanVarNorm = true
        initValueScale = 1.0
        uniformInit = true
        needPrior = true
    ]
    ExperimentalNetworkBuilder = [    // the same as above but with BS. Currently not used. Enable by removing the SimpleNetworkBuilder above.
        // note: this does not produce identical results because of different initialization order of random-initialized LearnableParameters
        layerSizes=363:512:512:132  // [0..]
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes[i:1..Length(layerSizes)-2]=Sigmoid
        applyMeanVarNorm=true
        initValueScale=1.0
        uniformInit=true
        BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols, init = if uniformInit then 'uniform' else 'gaussian'/*, initValueScale from outer scope*/) ; z = W*in+B ]
        GBFF(f, in, rows, cols) = [ Eh = rows,f(BFF(in, rows, cols).z) ]
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then GBFF(layerTypes[layer], layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else GBFF(layerTypes[layer], featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = trainingCriterion(labels, outZ, tag='criterion')
        Err = evalCriterion(labels, outZ, tag='eval')
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD = [
        epochSize = 20480
        minibatchSize = 64:256:1024:
        learningRatesPerMB = 1.0:0.5:0.1
        numMBsToShowResult = 10
        momentumPerMB = 0.9:0.656119
        dropoutRate = 0.0
        maxEpochs = 3
        keepCheckPointFiles = true
        AutoAdjust = [
            reduceLearnRateIfImproveLessThan = 0
            loadBestModel = true
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            autoAdjustLR = "adjustAfterEpoch"
        ]
        clippingThresholdPerSample = 1#INF
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "glob_0000.scp"
        ]
        labels = [
            mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
            labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
            labelDim = 132
            labelType = "category"
        ]
    ]
]
currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu
DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E
DeviceId=0
makeMode=true

<<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk.config:command=speechTrain
configparameters: cntk.config:ConfigDir=/home/alrezni/src/cntk/Tests/Speech/QuickE2E
configparameters: cntk.config:currentDirectory=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk.config:DataDir=/home/alrezni/src/cntk/Tests/Speech/Data
configparameters: cntk.config:deviceId=0
configparameters: cntk.config:makeMode=true
configparameters: cntk.config:parallelTrain=false
configparameters: cntk.config:precision=float
configparameters: cntk.config:RunDir=/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu
configparameters: cntk.config:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu/models/cntkSpeech.dnn"
    deviceId = 0
    traceLevel = 1
    SimpleNetworkBuilder = [
        layerSizes = 363:512:512:132
        trainingCriterion = "CrossEntropyWithSoftmax"
        evalCriterion = "ErrorPrediction"
        layerTypes = "Sigmoid"
        applyMeanVarNorm = true
        initValueScale = 1.0
        uniformInit = true
        needPrior = true
    ]
    ExperimentalNetworkBuilder = [    // the same as above but with BS. Currently not used. Enable by removing the SimpleNetworkBuilder above.
        // note: this does not produce identical results because of different initialization order of random-initialized LearnableParameters
        layerSizes=363:512:512:132  // [0..]
        trainingCriterion=CrossEntropyWithSoftmax
        evalCriterion=ErrorPrediction
        layerTypes[i:1..Length(layerSizes)-2]=Sigmoid
        applyMeanVarNorm=true
        initValueScale=1.0
        uniformInit=true
        BFF(in, rows, cols) = [ B = Parameter(rows, 1, init = 'fixedValue', value = 0) ; W = Parameter(rows, cols, init = if uniformInit then 'uniform' else 'gaussian'/*, initValueScale from outer scope*/) ; z = W*in+B ]
        GBFF(f, in, rows, cols) = [ Eh = rows,f(BFF(in, rows, cols).z) ]
        L = Length(layerSizes)-1    // number of model layers
        features = Input(layerSizes[0], 1, tag='feature') ; labels = Input(layerSizes[Length(layerSizes)-1], 1, tag='label')
        featNorm = if applyMeanVarNorm
                   then MeanVarNorm(features)
                   else features
        layers[layer:1..L-1] = if layer > 1
                               then GBFF(layerTypes[layer], layers[layer-1].Eh, layerSizes[layer], layerSizes[layer-1])
                               else GBFF(layerTypes[layer], featNorm, layerSizes[layer], layerSizes[layer-1])
        outLayer = BFF(layers[L-1].Eh, layerSizes[L], layerSizes[L-1])
        outZ = outLayer.z        // + PastValue(layerSizes[L], 1, outLayer.z)
        CE = trainingCriterion(labels, outZ, tag='criterion')
        Err = evalCriterion(labels, outZ, tag='eval')
        logPrior = LogPrior(labels)
        // TODO: how to add a tag to an infix operation?
        ScaledLogLikelihood = Minus (outZ, logPrior, tag='output')
    ]
    SGD = [
        epochSize = 20480
        minibatchSize = 64:256:1024:
        learningRatesPerMB = 1.0:0.5:0.1
        numMBsToShowResult = 10
        momentumPerMB = 0.9:0.656119
        dropoutRate = 0.0
        maxEpochs = 3
        keepCheckPointFiles = true
        AutoAdjust = [
            reduceLearnRateIfImproveLessThan = 0
            loadBestModel = true
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            autoAdjustLR = "adjustAfterEpoch"
        ]
        clippingThresholdPerSample = 1#INF
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "glob_0000.scp"
        ]
        labels = [
            mlfFile = "/home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf"
            labelMappingFile = "/home/alrezni/src/cntk/Tests/Speech/Data/state.list"
            labelDim = 132
            labelType = "category"
        ]
    ]
]

<<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
command: speechTrain 
precision = float
CNTKModelPath: /tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu/models/cntkSpeech.dnn
CNTKCommandTrainInfo: speechTrain : 3
CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
CNTKCommandTrainBegin: speechTrain
SimpleNetworkBuilder Using GPU 0
reading script file glob_0000.scp ... 948 entries
total 132 state names in state list /home/alrezni/src/cntk/Tests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/alrezni/src/cntk/Tests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
Starting from checkpoint. Load Network From File /tmp/cntk-test-20151215163714.581330/Speech_QuickE2E@release_gpu/models/cntkSpeech.dnn.2.

Post-processing network...

7 roots:
	EvalErrorPrediction = ErrorPrediction
	InvStdOfFeatures = InvStdDev
	MeanOfFeatures = Mean
	PosteriorProb = Softmax
	Prior = Mean
	ScaledLogLikelihood = Minus
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax
FormNestedNetwork: WARNING: Was called twice for EvalErrorPrediction ErrorPrediction operation
FormNestedNetwork: WARNING: Was called twice for InvStdOfFeatures InvStdDev operation
FormNestedNetwork: WARNING: Was called twice for MeanOfFeatures Mean operation
FormNestedNetwork: WARNING: Was called twice for PosteriorProb Softmax operation
FormNestedNetwork: WARNING: Was called twice for Prior Mean operation
FormNestedNetwork: WARNING: Was called twice for ScaledLogLikelihood Minus operation
FormNestedNetwork: WARNING: Was called twice for CrossEntropyWithSoftmax CrossEntropyWithSoftmax operation


Validating for node EvalErrorPrediction. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction. 12 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node EvalErrorPrediction, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> EvalErrorPrediction = ErrorPrediction(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.


Validating for node InvStdOfFeatures. 2 nodes to process in pass 1.

Validating --> features = InputValue -> [363, MBSize 0]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]

Validating for node InvStdOfFeatures, final verification.

Validating --> features = InputValue -> [363, MBSize 0]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.


Validating for node MeanOfFeatures. 2 nodes to process in pass 1.

Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]

Validating for node MeanOfFeatures, final verification.

Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.


Validating for node PosteriorProb. 19 nodes to process in pass 1.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> PosteriorProb = Softmax(HLast[132, MBSize 0]) -> [132, MBSize 0]

Validating for node PosteriorProb. 9 nodes to process in pass 2.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> PosteriorProb = Softmax(HLast[132, MBSize 0]) -> [132, MBSize 0]

Validating for node PosteriorProb, final verification.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> PosteriorProb = Softmax(HLast[132, MBSize 0]) -> [132, MBSize 0]

8 out of 19 nodes do not share the minibatch layout with the input data.


Validating for node Prior. 2 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]

Validating for node Prior. 1 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]

Validating for node Prior, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]

1 out of 2 nodes do not share the minibatch layout with the input data.


Validating for node ScaledLogLikelihood. 22 nodes to process in pass 1.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood. 10 nodes to process in pass 2.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

Validating for node ScaledLogLikelihood, final verification.

Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> Prior = Mean(labels[132, MBSize 0]) -> [132, 1]
Validating --> LogOfPrior = Log(Prior[132, 1]) -> [132, 1]
Validating --> ScaledLogLikelihood = Minus(HLast[132, MBSize 0], LogOfPrior[132, 1]) -> [132, MBSize 0]

10 out of 22 nodes do not share the minibatch layout with the input data.


Validating for node CrossEntropyWithSoftmax. 20 nodes to process in pass 1.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax. 9 nodes to process in pass 2.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

Validating for node CrossEntropyWithSoftmax, final verification.

Validating --> labels = InputValue -> [132, MBSize 0]
Validating --> W2 = LearnableParameter -> [132, 512]
Validating --> W1 = LearnableParameter -> [512, 512]
Validating --> W0 = LearnableParameter -> [512, 363]
Validating --> features = InputValue -> [363, MBSize 0]
Validating --> MeanOfFeatures = Mean(features[363, MBSize 0]) -> [363, 1]
Validating --> InvStdOfFeatures = InvStdDev(features[363, MBSize 0]) -> [363, 1]
Validating --> MVNormalizedFeatures = PerDimMeanVarNormalization(features[363, MBSize 0], MeanOfFeatures[363, 1], InvStdOfFeatures[363, 1]) -> [363, MBSize 0]
Validating --> W0*features = Times(W0[512, 363], MVNormalizedFeatures[363, MBSize 0]) -> [512, MBSize 0]
Validating --> B0 = LearnableParameter -> [512, 1]
Validating --> W0*features+B0 = Plus(W0*features[512, MBSize 0], B0[512, 1]) -> [512, MBSize 0]
Validating --> H1 = Sigmoid(W0*features+B0[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W1*H1 = Times(W1[512, 512], H1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> B1 = LearnableParameter -> [512, 1]
Validating --> W1*H1+B1 = Plus(W1*H1[512, MBSize 0], B1[512, 1]) -> [512, MBSize 0]
Validating --> H2 = Sigmoid(W1*H1+B1[512, MBSize 0]) -> [512, MBSize 0]
Validating --> W2*H1 = Times(W2[132, 512], H2[512, MBSize 0]) -> [132, MBSize 0]
Validating --> B2 = LearnableParameter -> [132, 1]
Validating --> HLast = Plus(W2*H1[132, MBSize 0], B2[132, 1]) -> [132, MBSize 0]
Validating --> CrossEntropyWithSoftmax = CrossEntropyWithSoftmax(labels[132, MBSize 0], HLast[132, MBSize 0]) -> [1, 1]

9 out of 20 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

SGD using GPU 0.

Training criterion node(s):
	CrossEntropyWithSoftmax = CrossEntropyWithSoftmax

Evaluation criterion node(s):
	EvalErrorPrediction = ErrorPrediction


Allocating matrices for forward and/or backward propagation.
No PreCompute nodes found, skipping PreCompute step
Set Max Temp Mem Size For Convolution Nodes to 0 samples.
Starting Epoch 3: learning rate per sample = 0.000098  effective momentum = 0.656119  momentum as time constant = 2429.9 samples
minibatchiterator: epoch 2: frames [40960..61440] (first utterance at frame 40960), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

Starting minibatch loop.
 Epoch[ 3 of 3]-Minibatch[   1-  10, 50.00%]: SamplesSeen = 10240; TrainLossPerSample =  1.91202354; EvalErr[0]PerSample = 0.52734375; TotalTime = 0.4783s; SamplesPerSecond = 21408.0
 Epoch[ 3 of 3]-Minibatch[  11-  20, 100.00%]: SamplesSeen = 10240; TrainLossPerSample =  1.91116600; EvalErr[0]PerSample = 0.52255859; TotalTime = 0.3471s; SamplesPerSecond = 29502.3
Finished Epoch[ 3 of 3]: [Training Set] TrainLossPerSample = 1.9115947; EvalErrPerSample = 0.52495116; AvgLearningRatePerSample = 9.7656251e-05; EpochTime=0.90386
CNTKCommandTrainEnd: speechTrain
COMPLETED