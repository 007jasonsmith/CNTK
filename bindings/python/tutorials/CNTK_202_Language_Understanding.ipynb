{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Lab: Language Understanding with Recurrent Networks\n",
    "\n",
    "This hands-on lab shows how to implement a recurrent network to process text,\n",
    "for the Air Travel Information Services (ATIS) tasks of slot tagging and intent classification.\n",
    "We will start with a straight-forward embedding followed by a recurrent LSTM.\n",
    "We will then extend it to include neighbor words and run bidirectionally.\n",
    "Lastly, we will turn this system into an intent classifier.  \n",
    "\n",
    "The techniques you will practice include:\n",
    "\n",
    "* model description by composing layer blocks instead of writing formulas\n",
    "* creating your own layer block\n",
    "* variables with different sequence lengths in the same network\n",
    "* parallel training\n",
    "\n",
    "We assume that you are familiar with basics of deep learning, and these specific concepts:\n",
    "\n",
    "* recurrent networks ([Wikipedia page](https://en.wikipedia.org/wiki/Recurrent_neural_network))\n",
    "* text embedding ([Wikipedia page](https://en.wikipedia.org/wiki/Word_embedding))\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "We assume that you have already [installed CNTK](https://www.cntk.ai/pythondocs/setup.html).\n",
    "This tutorial requires CNTK V2. We strongly recommend to run this tutorial on a machine with \n",
    "a capable CUDA-compatible GPU. Deep learning without GPUs is not fun.\n",
    "\n",
    "Finally you need to download the training and test set. The following piece of code does that for you. If you get an error, please follow the manual instructions below it.\n",
    "\n",
    "We also list the imports we will need for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from cntk.blocks import *  # non-layer like building blocks such as LSTM()\n",
    "from cntk.layers import *  # layer-like stuff such as Linear()\n",
    "from cntk.models import *  # higher abstraction level, e.g. entire standard models and also operators like Sequential()\n",
    "from cntk.utils import *\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk import Trainer\n",
    "from cntk.ops import cross_entropy_with_softmax, classification_error, splice\n",
    "from cntk.learner import adam_sgd, learning_rate_schedule, momentum_schedule\n",
    "from cntk.persist import load_model, save_model\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except:\n",
    "    tqdm = lambda x: x\n",
    "import requests\n",
    "\n",
    "def download(data):\n",
    "    url = \"https://github.com/Microsoft/CNTK/blob/master/Examples/Tutorials/SLUHandsOn/atis.%s.ctf?raw=true\"\n",
    "    response = requests.get(url%data, stream=True)\n",
    "\n",
    "    with open(\"atis.%s.ctf\"%data, \"wb\") as handle:\n",
    "        for data in tqdm(response.iter_content()):\n",
    "            handle.write(data)\n",
    "\n",
    "for t in \"train\",\"test\":\n",
    "    try:\n",
    "        f=open(\"atis.%s.ctf\"%t)\n",
    "        f.close()\n",
    "    except:\n",
    "        download(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallback manual instructions\n",
    "Please download the ATIS [training](https://github.com/Microsoft/CNTK/blob/master/Tutorials/SLUHandsOn/atis.train.ctf) \n",
    "and [test](https://github.com/Microsoft/CNTK/blob/master/Tutorials/SLUHandsOn/atis.test.ctf) \n",
    "files and put them at the same folder as this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and Model Structure\n",
    "\n",
    "The task we want to approach in this tutorial is slot tagging.\n",
    "We use the [ATIS corpus](https://catalog.ldc.upenn.edu/LDC95S26).\n",
    "ATIS contains human-computer queries from the domain of Air Travel Information Services,\n",
    "and our task will be to annotate (tag) each word of a query whether it belongs to a\n",
    "specific item of information (slot), and which one.\n",
    "\n",
    "The data in your working folder has already been converted into the \"CNTK Text Format.\"\n",
    "Let's look at an example from the test-set file `atis.test.ctf`:\n",
    "\n",
    "    19  |S0 178:1 |# BOS      |S1 14:1 |# flight  |S2 128:1 |# O\n",
    "    19  |S0 770:1 |# show                         |S2 128:1 |# O\n",
    "    19  |S0 429:1 |# flights                      |S2 128:1 |# O\n",
    "    19  |S0 444:1 |# from                         |S2 128:1 |# O\n",
    "    19  |S0 272:1 |# burbank                      |S2 48:1  |# B-fromloc.city_name\n",
    "    19  |S0 851:1 |# to                           |S2 128:1 |# O\n",
    "    19  |S0 789:1 |# st.                          |S2 78:1  |# B-toloc.city_name\n",
    "    19  |S0 564:1 |# louis                        |S2 125:1 |# I-toloc.city_name\n",
    "    19  |S0 654:1 |# on                           |S2 128:1 |# O\n",
    "    19  |S0 601:1 |# monday                       |S2 26:1  |# B-depart_date.day_name\n",
    "    19  |S0 179:1 |# EOS                          |S2 128:1 |# O\n",
    "\n",
    "This file has 7 columns:\n",
    "\n",
    "* a sequence id (19). There are 11 entries with this sequence id. This means that sequence 19 consists\n",
    "of 11 tokens;\n",
    "* column `S0`, which contains numeric word indices;\n",
    "* a comment column denoted by `#`, to allow a human reader to know what the numeric word index stands for;\n",
    "Comment columns are ignored by the system. `BOS` and `EOS` are special words\n",
    "to denote beginning and end of sentence, respectively;\n",
    "* column `S1` is an intent label, which we will only use in the last part of the tutorial;\n",
    "* another comment column that shows the human-readable label of the numeric intent index;\n",
    "* column `S2` is the slot label, represented as a numeric index; and\n",
    "* another comment column that shows the human-readable label of the numeric label index.\n",
    "\n",
    "The task of the neural network is to look at the query (column `S0`) and predict the\n",
    "slot label (column `S2`).\n",
    "As you can see, each word in the input gets assigned either an empty label `O`\n",
    "or a slot label that begins with `B-` for the first word, and with `I-` for any\n",
    "additional consecutive word that belongs to the same slot.\n",
    "\n",
    "The model we will use is a recurrent model consisting of an embedding layer,\n",
    "a recurrent LSTM cell, and a dense layer to compute the posterior probabilities:\n",
    "\n",
    "\n",
    "    slot label   \"O\"        \"O\"        \"O\"        \"O\"  \"B-fromloc.city_name\"\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "              | Dense |  | Dense |  | Dense |  | Dense |  | Dense |  ...\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +------+   +------+   +------+   +------+   +------+   \n",
    "         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->...\n",
    "              +------+   +------+   +------+   +------+   +------+   \n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "              | Embed |  | Embed |  | Embed |  | Embed |  | Embed |  ...\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "    w      ------>+--------->+--------->+--------->+--------->+------... \n",
    "                 BOS      \"show\"    \"flights\"    \"from\"   \"burbank\"\n",
    "\n",
    "Or, as a CNTK network description. Please have a quick look and match it with the description above:\n",
    "(descriptions of these functions can be found at: [the layers reference](http://cntk.ai/pythondocs/layerref.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = 943 ; num_labels = 129 ; num_intents = 26    # number of words in vocab, slot labels, and intent labels\n",
    "\n",
    "model_dir = \"./Models\"\n",
    "data_dir  = \".\"\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  # inject an option to mimic the BrainScript version identically; remove some day\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNTK Configuration\n",
    "\n",
    "To train and test a model in CNTK, we need to create a model and specify how to read data and perform training and testing. \n",
    "\n",
    "In order to train we need to specify:\n",
    "\n",
    "* how to read the data \n",
    "* the model function and its inputs and outputs\n",
    "* hyper-parameters for the learner\n",
    "\n",
    "[comment]: <> (For testing ...)\n",
    "\n",
    "### A Brief Look at Data and Data Reading\n",
    "\n",
    "We already looked at the data.\n",
    "But how do you generate this format?\n",
    "For reading text, this tutorial uses the `CNTKTextFormatReader`. It expects the input data to be\n",
    "of a specific format, which is described [here](https://github.com/Microsoft/CNTK/wiki/CNTKTextFormat-Reader).\n",
    "\n",
    "For this tutorial, we created the corpora by two steps:\n",
    "* convert the raw data into a plain text file that contains of TAB-separated columns of space-separated text. For example:\n",
    "\n",
    "  ```\n",
    "  BOS show flights from burbank to st. louis on monday EOS (TAB) flight (TAB) O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O B-depart_date.day_name O\n",
    "  ```\n",
    "\n",
    "  This is meant to be compatible with the output of the `paste` command.\n",
    "* convert it to CNTK Text Format (CTF) with the following command:\n",
    "\n",
    "  ```\n",
    "  python Scripts/txt2ctf.py --map query.wl intent.wl slots.wl --annotated True --input atis.test.txt --output atis.test.ctf\n",
    "  ```\n",
    "\n",
    "  where the three `.wl` files give the vocabulary as plain text files, one line per word.\n",
    "\n",
    "In these CTF files, our columns are labeled `S0`, `S1`, and `S2`.\n",
    "These are connected to the actual network inputs by the corresponding lines in the reader definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "         query         = StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent_unused = StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it\n",
    "\n",
    "You can find the complete recipe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(reader, model, max_epochs):\n",
    "    # Input variables denoting the features and label data\n",
    "    query       = Input(input_dim,  is_sparse=False)\n",
    "    slot_labels = Input(num_labels, is_sparse=True)  # TODO: make sparse once it works\n",
    "\n",
    "    # apply model to input\n",
    "    z = model(query)\n",
    "\n",
    "    # loss and metric\n",
    "    ce = cross_entropy_with_softmax(z, slot_labels)\n",
    "    pe = classification_error      (z, slot_labels)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 36000\n",
    "    minibatch_size = 70\n",
    "    num_mbs_to_show_result = 100\n",
    "    momentum_as_time_constant = minibatch_size / -math.log(0.9)  # TODO: Change to round number. This is 664.39. 700?\n",
    "\n",
    "    lr_per_sample = [0.003]*2+[0.0015]*12+[0.0003] # LR schedule over epochs (we don't run that mayn epochs, but if we did, these are good values)\n",
    "\n",
    "    # trainer object\n",
    "    lr_schedule = learning_rate_schedule(lr_per_sample, units=epoch_size)\n",
    "    learner = adam_sgd(z.parameters,\n",
    "                       lr_per_sample=lr_schedule, momentum_time_constant=momentum_as_time_constant,\n",
    "                       low_memory=True,\n",
    "                       gradient_clipping_threshold_per_sample=15, gradient_clipping_with_truncation=True)\n",
    "\n",
    "    trainer = Trainer(z, ce, pe, [learner])\n",
    "\n",
    "    # define mapping from reader streams to network inputs\n",
    "    input_map = {\n",
    "        query       : reader.streams.query,\n",
    "        slot_labels : reader.streams.slot_labels\n",
    "    }\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    log_number_of_parameters(z) ; print()\n",
    "    progress_printer = ProgressPrinter(freq=100, first=10, tag='Training') # more detailed logging\n",
    "    #progress_printer = ProgressPrinter(tag='Training')\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:               # loop over minibatches on the epoch\n",
    "            # BUGBUG? The change of minibatch_size parameter vv has no effect.\n",
    "            data = reader.next_minibatch(min(minibatch_size, epoch_end-t), input_map=input_map) # fetch minibatch\n",
    "            trainer.train_minibatch(data)                                   # update model with it\n",
    "            t += data[slot_labels].num_samples                              # count samples processed so far\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True) # log progress\n",
    "            #def trace_node(name):\n",
    "            #    nl = [n for n in z.parameters if n.name() == name]\n",
    "            #    if len(nl) > 0:\n",
    "            #        print (name, np.asarray(nl[0].value))\n",
    "            #trace_node('W')\n",
    "            #trace_node('stabilizer_param')\n",
    "        loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "\n",
    "    return loss, metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from _cntk_py import set_computation_network_trace_level, set_fixed_random_seed, force_deterministic_algorithms\n",
    "#set_computation_network_trace_level(1)  # TODO: remove debugging facilities once this all works\n",
    "set_fixed_random_seed(1)  # BUGBUG: has no effect at present  # TODO: remove debugging facilities once this all works\n",
    "force_deterministic_algorithms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 721479 parameters in 6 parameter tensors.\n",
      "\n",
      " Minibatch[   1-   1]: loss = 4.865357 * 67, metric = 100.0% * 67\n",
      " Minibatch[   2-   2]: loss = 4.845598 * 63, metric = 73.0% * 63\n",
      " Minibatch[   3-   3]: loss = 4.807514 * 68, metric = 38.2% * 68\n",
      " Minibatch[   4-   4]: loss = 4.759234 * 70, metric = 35.7% * 70\n",
      " Minibatch[   5-   5]: loss = 4.681714 * 65, metric = 30.8% * 65\n",
      " Minibatch[   6-   6]: loss = 4.566493 * 62, metric = 27.4% * 62\n",
      " Minibatch[   7-   7]: loss = 4.454796 * 58, metric = 31.0% * 58\n",
      " Minibatch[   8-   8]: loss = 4.259974 * 70, metric = 28.6% * 70\n",
      " Minibatch[   9-   9]: loss = 4.057600 * 59, metric = 33.9% * 59\n",
      " Minibatch[  10-  10]: loss = 3.580238 * 64, metric = 28.1% * 64\n",
      " Minibatch[  11- 100]: loss = 1.443393 * 5654, metric = 27.4% * 5654\n",
      " Minibatch[ 101- 200]: loss = 0.825825 * 6329, metric = 17.9% * 6329\n",
      " Minibatch[ 201- 300]: loss = 0.653802 * 6259, metric = 14.4% * 6259\n",
      " Minibatch[ 301- 400]: loss = 0.521054 * 6229, metric = 11.4% * 6229\n",
      " Minibatch[ 401- 500]: loss = 0.462903 * 6289, metric = 10.2% * 6289\n",
      "Finished Epoch [1]: [Training] loss = 0.783951 * 36061, metric = 15.5% * 36061\n",
      " Minibatch[   1-   1]: loss = 0.374824 * 4665, metric = 8.3% * 4665\n",
      " Minibatch[   2-   2]: loss = 0.342561 * 60, metric = 6.7% * 60\n",
      " Minibatch[   3-   3]: loss = 0.251658 * 59, metric = 8.5% * 59\n",
      " Minibatch[   4-   4]: loss = 0.535140 * 70, metric = 12.9% * 70\n",
      " Minibatch[   5-   5]: loss = 0.388006 * 48, metric = 8.3% * 48\n",
      " Minibatch[   6-   6]: loss = 0.501494 * 53, metric = 11.3% * 53\n",
      " Minibatch[   7-   7]: loss = 0.217948 * 66, metric = 4.5% * 66\n",
      " Minibatch[   8-   8]: loss = 0.182791 * 52, metric = 7.7% * 52\n",
      " Minibatch[   9-   9]: loss = 0.864139 * 59, metric = 13.6% * 59\n",
      " Minibatch[  10-  10]: loss = 0.270139 * 59, metric = 6.8% * 59\n",
      " Minibatch[  11- 100]: loss = 0.340713 * 5616, metric = 7.2% * 5616\n",
      " Minibatch[ 101- 200]: loss = 0.304612 * 6370, metric = 6.4% * 6370\n",
      " Minibatch[ 201- 300]: loss = 0.281929 * 6374, metric = 6.2% * 6374\n",
      " Minibatch[ 301- 400]: loss = 0.237689 * 6280, metric = 5.3% * 6280\n",
      " Minibatch[ 401- 500]: loss = 0.240671 * 6240, metric = 5.1% * 6240\n",
      "Finished Epoch [2]: [Training] loss = 0.270411 * 36000, metric = 5.9% * 36000\n",
      " Minibatch[   1-   1]: loss = 0.187526 * 4595, metric = 4.7% * 4595\n",
      " Minibatch[   2-   2]: loss = 0.610558 * 60, metric = 10.0% * 60\n",
      " Minibatch[   3-   3]: loss = 0.128487 * 63, metric = 1.6% * 63\n",
      " Minibatch[   4-   4]: loss = 0.218914 * 64, metric = 6.2% * 64\n",
      " Minibatch[   5-   5]: loss = 0.131611 * 51, metric = 5.9% * 51\n",
      " Minibatch[   6-   6]: loss = 0.355228 * 62, metric = 8.1% * 62\n",
      " Minibatch[   7-   7]: loss = 0.101758 * 65, metric = 1.5% * 65\n",
      " Minibatch[   8-   8]: loss = 0.116396 * 68, metric = 2.9% * 68\n",
      " Minibatch[   9-   9]: loss = 0.060718 * 69, metric = 2.9% * 69\n",
      " Minibatch[  10-  10]: loss = 0.064905 * 58, metric = 0.0% * 58\n",
      " Minibatch[  11- 100]: loss = 0.196392 * 5684, metric = 4.3% * 5684\n",
      " Minibatch[ 101- 200]: loss = 0.158508 * 6285, metric = 3.5% * 6285\n",
      " Minibatch[ 201- 300]: loss = 0.156975 * 6305, metric = 3.7% * 6305\n",
      " Minibatch[ 301- 400]: loss = 0.173221 * 6386, metric = 3.9% * 6386\n",
      " Minibatch[ 401- 500]: loss = 0.132959 * 6332, metric = 2.6% * 6332\n",
      "Finished Epoch [3]: [Training] loss = 0.164235 * 35978, metric = 3.6% * 35978\n",
      " Minibatch[   1-   1]: loss = 0.169665 * 4441, metric = 3.7% * 4441\n",
      " Minibatch[   2-   2]: loss = 0.046308 * 64, metric = 1.6% * 64\n",
      " Minibatch[   3-   3]: loss = 0.091258 * 62, metric = 1.6% * 62\n",
      " Minibatch[   4-   4]: loss = 0.031103 * 66, metric = 0.0% * 66\n",
      " Minibatch[   5-   5]: loss = 0.039266 * 60, metric = 1.7% * 60\n",
      " Minibatch[   6-   6]: loss = 0.120681 * 65, metric = 3.1% * 65\n",
      " Minibatch[   7-   7]: loss = 0.058614 * 61, metric = 1.6% * 61\n",
      " Minibatch[   8-   8]: loss = 0.076050 * 65, metric = 1.5% * 65\n",
      " Minibatch[   9-   9]: loss = 0.178154 * 70, metric = 5.7% * 70\n",
      " Minibatch[  10-  10]: loss = 0.214431 * 61, metric = 4.9% * 61\n",
      " Minibatch[  11- 100]: loss = 0.143353 * 5720, metric = 2.9% * 5720\n",
      " Minibatch[ 101- 200]: loss = 0.115845 * 6275, metric = 2.5% * 6275\n",
      " Minibatch[ 201- 300]: loss = 0.134991 * 6250, metric = 2.8% * 6250\n",
      " Minibatch[ 301- 400]: loss = 0.126516 * 6344, metric = 2.9% * 6344\n",
      " Minibatch[ 401- 500]: loss = 0.115771 * 6191, metric = 2.5% * 6191\n",
      "Finished Epoch [4]: [Training] loss = 0.130330 * 36005, metric = 2.7% * 36005\n",
      " Minibatch[   1-   1]: loss = 0.155825 * 4642, metric = 2.8% * 4642\n",
      " Minibatch[   2-   2]: loss = 0.152449 * 59, metric = 3.4% * 59\n",
      " Minibatch[   3-   3]: loss = 0.180339 * 65, metric = 6.2% * 65\n",
      " Minibatch[   4-   4]: loss = 0.160045 * 70, metric = 7.1% * 70\n",
      " Minibatch[   5-   5]: loss = 0.065918 * 59, metric = 1.7% * 59\n",
      " Minibatch[   6-   6]: loss = 0.063076 * 64, metric = 3.1% * 64\n",
      " Minibatch[   7-   7]: loss = 0.098584 * 67, metric = 3.0% * 67\n",
      " Minibatch[   8-   8]: loss = 0.097580 * 70, metric = 1.4% * 70\n",
      " Minibatch[   9-   9]: loss = 0.004126 * 65, metric = 0.0% * 65\n",
      " Minibatch[  10-  10]: loss = 0.055877 * 66, metric = 0.0% * 66\n",
      " Minibatch[  11- 100]: loss = 0.085462 * 5653, metric = 1.8% * 5653\n",
      " Minibatch[ 101- 200]: loss = 0.097849 * 6271, metric = 2.2% * 6271\n",
      " Minibatch[ 201- 300]: loss = 0.094764 * 6274, metric = 2.2% * 6274\n",
      " Minibatch[ 301- 400]: loss = 0.096376 * 6305, metric = 2.1% * 6305\n",
      " Minibatch[ 401- 500]: loss = 0.086629 * 6334, metric = 1.9% * 6334\n",
      "Finished Epoch [5]: [Training] loss = 0.093036 * 35995, metric = 2.1% * 35995\n",
      " Minibatch[   1-   1]: loss = 0.096901 * 4576, metric = 2.1% * 4576\n",
      " Minibatch[   2-   2]: loss = 0.007716 * 69, metric = 0.0% * 69\n",
      " Minibatch[   3-   3]: loss = 0.035637 * 65, metric = 1.5% * 65\n",
      " Minibatch[   4-   4]: loss = 0.083248 * 70, metric = 1.4% * 70\n",
      " Minibatch[   5-   5]: loss = 0.009262 * 60, metric = 0.0% * 60\n",
      " Minibatch[   6-   6]: loss = 0.054556 * 59, metric = 0.0% * 59\n",
      " Minibatch[   7-   7]: loss = 0.272605 * 66, metric = 7.6% * 66\n",
      " Minibatch[   8-   8]: loss = 0.221326 * 56, metric = 7.1% * 56\n",
      " Minibatch[   9-   9]: loss = 0.036428 * 61, metric = 0.0% * 61\n",
      " Minibatch[  10-  10]: loss = 0.039098 * 69, metric = 1.4% * 69\n",
      " Minibatch[  11- 100]: loss = 0.087705 * 5692, metric = 1.9% * 5692\n",
      " Minibatch[ 101- 200]: loss = 0.096981 * 6341, metric = 2.1% * 6341\n",
      " Minibatch[ 201- 300]: loss = 0.106464 * 6323, metric = 2.5% * 6323\n",
      " Minibatch[ 301- 400]: loss = 0.067923 * 6250, metric = 1.3% * 6250\n",
      " Minibatch[ 401- 500]: loss = 0.069463 * 6276, metric = 1.7% * 6276\n",
      "Finished Epoch [6]: [Training] loss = 0.084151 * 36007, metric = 1.8% * 36007\n",
      " Minibatch[   1-   1]: loss = 0.073761 * 4547, metric = 1.5% * 4547\n",
      " Minibatch[   2-   2]: loss = 0.004342 * 63, metric = 0.0% * 63\n",
      " Minibatch[   3-   3]: loss = 0.053394 * 61, metric = 0.0% * 61\n",
      " Minibatch[   4-   4]: loss = 0.031223 * 68, metric = 1.5% * 68\n",
      " Minibatch[   5-   5]: loss = 0.016739 * 62, metric = 0.0% * 62\n",
      " Minibatch[   6-   6]: loss = 0.073060 * 62, metric = 1.6% * 62\n",
      " Minibatch[   7-   7]: loss = 0.038452 * 58, metric = 0.0% * 58\n",
      " Minibatch[   8-   8]: loss = 0.084096 * 66, metric = 1.5% * 66\n",
      " Minibatch[   9-   9]: loss = 0.010647 * 63, metric = 0.0% * 63\n",
      " Minibatch[  10-  10]: loss = 0.051993 * 66, metric = 1.5% * 66\n",
      " Minibatch[  11- 100]: loss = 0.079250 * 5753, metric = 1.6% * 5753\n",
      " Minibatch[ 101- 200]: loss = 0.069914 * 6207, metric = 1.4% * 6207\n",
      " Minibatch[ 201- 300]: loss = 0.066953 * 6388, metric = 1.5% * 6388\n",
      " Minibatch[ 301- 400]: loss = 0.076477 * 6355, metric = 1.7% * 6355\n",
      " Minibatch[ 401- 500]: loss = 0.078168 * 6378, metric = 1.7% * 6378\n",
      "Finished Epoch [7]: [Training] loss = 0.073427 * 35960, metric = 1.6% * 35960\n",
      " Minibatch[   1-   1]: loss = 0.072960 * 4336, metric = 1.7% * 4336\n",
      " Minibatch[   2-   2]: loss = 0.024331 * 61, metric = 1.6% * 61\n",
      " Minibatch[   3-   3]: loss = 0.048853 * 55, metric = 0.0% * 55\n",
      " Minibatch[   4-   4]: loss = 0.017478 * 69, metric = 0.0% * 69\n",
      " Minibatch[   5-   5]: loss = 0.010466 * 69, metric = 0.0% * 69\n",
      " Minibatch[   6-   6]: loss = 0.018562 * 69, metric = 0.0% * 69\n",
      " Minibatch[   7-   7]: loss = 0.132872 * 64, metric = 4.7% * 64\n",
      " Minibatch[   8-   8]: loss = 0.022948 * 66, metric = 0.0% * 66\n",
      " Minibatch[   9-   9]: loss = 0.008601 * 67, metric = 0.0% * 67\n",
      " Minibatch[  10-  10]: loss = 0.053077 * 70, metric = 1.4% * 70\n",
      " Minibatch[  11- 100]: loss = 0.068197 * 5595, metric = 1.4% * 5595\n",
      " Minibatch[ 101- 200]: loss = 0.066083 * 6299, metric = 1.5% * 6299\n",
      " Minibatch[ 201- 300]: loss = 0.066587 * 6295, metric = 1.6% * 6295\n",
      " Minibatch[ 301- 400]: loss = 0.057647 * 6337, metric = 1.4% * 6337\n",
      " Minibatch[ 401- 500]: loss = 0.057751 * 6253, metric = 1.3% * 6253\n",
      "Finished Epoch [8]: [Training] loss = 0.061930 * 36044, metric = 1.4% * 36044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06193035719939996, 0.014038397514149373)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = create_reader(data_dir + \"/atis.train.ctf\", is_training=True)\n",
    "model = create_model()\n",
    "train(reader, model, max_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run it you will soon see out this:\n",
    "\n",
    "```\n",
    "Training 721479 parameters in 6 parameter tensors.\n",
    "\n",
    " Minibatch[   1-   1]: loss = 4.865357 * 67, metric = 100.0% * 67\n",
    " Minibatch[   2-   2]: loss = 4.845598 * 63, metric = 73.0% * 63\n",
    " Minibatch[   3-   3]: loss = 4.807514 * 68, metric = 38.2% * 68\n",
    " Minibatch[   4-   4]: loss = 4.759234 * 70, metric = 35.7% * 70\n",
    " Minibatch[   5-   5]: loss = 4.681714 * 65, metric = 30.8% * 65\n",
    " Minibatch[   6-   6]: loss = 4.566493 * 62, metric = 27.4% * 62\n",
    " Minibatch[   7-   7]: loss = 4.454796 * 58, metric = 31.0% * 58\n",
    " Minibatch[   8-   8]: loss = 4.259974 * 70, metric = 28.6% * 70\n",
    " Minibatch[   9-   9]: loss = 4.057600 * 59, metric = 33.9% * 59\n",
    " Minibatch[  10-  10]: loss = 3.580238 * 64, metric = 28.1% * 64\n",
    " Minibatch[  11- 100]: loss = 1.443393 * 5654, metric = 27.4% * 5654\n",
    " Minibatch[ 101- 200]: loss = 0.825825 * 6329, metric = 17.9% * 6329\n",
    " Minibatch[ 201- 300]: loss = 0.653802 * 6259, metric = 14.4% * 6259\n",
    " Minibatch[ 301- 400]: loss = 0.521054 * 6229, metric = 11.4% * 6229\n",
    " Minibatch[ 401- 500]: loss = 0.462903 * 6289, metric = 10.2% * 6289\n",
    "Finished Epoch [1]: [Training] loss = 0.783951 * 36061, metric = 15.5% * 36061\n",
    "```\n",
    "\n",
    "This shows how learning proceeds over epochs (passes through the data).\n",
    "For example, after two epochs, the cross-entropy criterion, which is the `ce` variable in\n",
    "the `train` function, has reached 0.27 as measured on the 36000 samples of this epoch,\n",
    "and that the error rate is 5.9% on those same 36000 training samples.\n",
    "\n",
    "The epoch size is the number of samples--counted as *word tokens*, not sentences--to\n",
    "process between model checkpoints.\n",
    "\n",
    "Once the training has completed (a little less than 2 minutes on a Titan-X or a Surface Book),\n",
    "you will see an output like this\n",
    "```\n",
    "(0.06193035719939996, 0.014038397514149373)\n",
    "```\n",
    "which is a tuple containing the loss (cross entropy) and the metric (classification error) averaged over the final epoch.\n",
    "\n",
    "On a CPU-only machine, it can be 4 or more times slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Model\n",
    "\n",
    "In the following, you will be given tasks to practice modifying CNTK configurations.\n",
    "The solutions are given at the end of this document... but please try without!\n",
    "\n",
    "### A Word About [`Sequential()`](https://www.cntk.ai/pythondocs/layerref.html#sequential)\n",
    "\n",
    "Before jumping to the tasks, let's have a look again at the model we just ran.\n",
    "The model is described in what we call *function-composition style*.\n",
    "```python\n",
    "        Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "```\n",
    "You may be familiar with the \"sequential\" notation from other neural-network toolkits.\n",
    "If not, [`Sequential()`](https://www.cntk.ai/pythondocs/layerref.html#sequential) is a powerful operation that,\n",
    "in a nutshell, allows to compactly express a very common situation in neural networks\n",
    "where an input is processed by propagating it through a progression of layers.\n",
    "`Sequential()` takes an list of functions as its argument,\n",
    "and returns a *new* function that invokes these functions in order,\n",
    "each time passing the output of one to the next.\n",
    "For example,\n",
    "```python\n",
    "\tFGH = Sequential ([F,G,H])\n",
    "    y = FGH (x)\n",
    "```\n",
    "means the same as\n",
    "```\n",
    "    y = H(G(F(x))) \n",
    "```\n",
    "This is known as [\"function composition\"](https://en.wikipedia.org/wiki/Function_composition),\n",
    "and is especially convenient for expressing neural networks, which often have this form:\n",
    "\n",
    "         +-------+   +-------+   +-------+\n",
    "    x -->|   F   |-->|   G   |-->|   H   |--> y\n",
    "         +-------+   +-------+   +-------+\n",
    "\n",
    "Coming back to our model at hand, the `Sequential` expression simply\n",
    "says that our model has this form:\n",
    "\n",
    "         +-----------+   +----------------+   +------------+\n",
    "    x -->| Embedding |-->| Recurrent LSTM |-->| DenseLayer |--> y\n",
    "         +-----------+   +----------------+   +------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Add Batch Normalization\n",
    "\n",
    "We now want to add new layers to the model, specifically batch normalization.\n",
    "\n",
    "Batch normalization is a popular technique for speeding up convergence.\n",
    "It is often used for image-processing setups, for example our other [hands-on lab on image\n",
    "recognition](./Hands-On-Labs-Image-Recognition).\n",
    "But could it work for recurrent models, too?\n",
    "  \n",
    "So your task will be to insert batch-normalization layers before and after the recurrent LSTM layer.\n",
    "If you have completed the [hands-on labs on image processing](https://github.com/Microsoft/CNTK/blob/master/bindings/python/tutorials/CNTK_201B_CIFAR-10_ImageHandsOn.ipynb),\n",
    "you may remember that the [batch-normalization layer](https://www.cntk.ai/pythondocs/layerref.html#batchnormalization-layernormalization-stabilizer) has this form:\n",
    "```\n",
    "    BatchNormalization()\n",
    "```\n",
    "So please go ahead and modify the configuration and see what happens.\n",
    "\n",
    "If everything went right, you will notice improved convergence speed (`loss` and `metric`)\n",
    "compared to the previous configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add batch normalization\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  # inject an option to mimic the BrainScript version identically; remove some day\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "#reader = create_reader(data_dir + \"/atis.train.ctf\", is_training=True)\n",
    "#model = create_model()\n",
    "#train(reader, model, max_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Add a Lookahead \n",
    "\n",
    "Our recurrent model suffers from a structural deficit:\n",
    "Since the recurrence runs from left to right, the decision for a slot label\n",
    "has no information about upcoming words. The model is a bit lopsided.\n",
    "Your task will be to modify the model such that\n",
    "the input to the recurrence consists not only of the current word, but also of the next one\n",
    "(lookahead).\n",
    "\n",
    "Your solution should be in function-composition style.\n",
    "Hence, you will need to write a Python function that does the following:\n",
    "\n",
    "* takes no input arguments\n",
    "* creates a placeholder sequence variable\n",
    "* computes the \"next value\" in this sequence using the `Delay()` layer (use this specific form: `Delay(T=-1)`); and\n",
    "* concatenate the current and the next value into a vector of twice the embedding dimension using `splice()`\n",
    "\n",
    "and then insert this function into `Sequential()`'s list between the embedding and the recurrent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add lookahead\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  # inject an option to mimic the BrainScript version identically; remove some day\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "#reader = create_reader(data_dir + \"/atis.train.ctf\", is_training=True)\n",
    "#model = create_model()\n",
    "#train(reader, model, max_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Bidirectional Recurrent Model\n",
    "\n",
    "Aha, knowledge of future words help. So instead of a one-word lookahead,\n",
    "why not look ahead until all the way to the end of the sentence, through a backward recurrence?\n",
    "Let us create a bidirectional model!\n",
    "\n",
    "Your task is to implement a new layer that\n",
    "performs both a forward and a backward recursion over the data, and\n",
    "concatenates the output vectors.\n",
    "\n",
    "Note, however, that this differs from the previous task in that\n",
    "the bidirectional layer contains learnable model parameters.\n",
    "In function-composition style,\n",
    "the pattern to implement a layer with model parameters is to write a *factory function*\n",
    "that creates a *function object*.\n",
    "\n",
    "A function object, also known as *functor*, is an object that is both a function and an object.\n",
    "Which means nothing else that it contains data yet still can be invoked as if it was a function.\n",
    "\n",
    "For example, `Dense(outDim)` is a factory function that returns a function object that contains\n",
    "a weight matrix `W`, a bias `b`, and another function to compute `W * input + b`.\n",
    "E.g. saying `Dense(1024)` will create this function object, which can then be used\n",
    "like any other function, also immediately: `Dense(1024)(x)`. \n",
    "\n",
    "Confused? Let's take an example: Let us implement a new layer that combines\n",
    "a linear layer with a subsequent batch normalization. \n",
    "To allow function composition, the layer needs to be realized as a factory function,\n",
    "which could look like this:\n",
    "\n",
    "```python\n",
    "def DenseLayerWithBN(dim):\n",
    "    F = Dense(dim)\n",
    "    G = BatchNormalization()\n",
    "    x = Placeholder()\n",
    "    apply_x = G(F(x))\n",
    "    return apply_x\n",
    "```\n",
    "\n",
    "Invoking this factory function will create `F`, `G`, `x`, and `apply_x`. In this example, `F` and `G` are function objects themselves, and `apply_x` is the function to be applied to the data.\n",
    "Thus, e.g. calling `DenseLayerWithBN(1024)` will\n",
    "create an object containing a linear-layer function object called `F`, a batch-normalization function object `G`,\n",
    "and `apply_x` which is the function that implements the actual operation of this layer\n",
    "using `F` and `G`. It will then return `apply_x`. To the outside, `apply_x` looks and behaves\n",
    "like a function. Under the hood, however, `apply_x` retains access to its specific instances of `F` and `G`.\n",
    "\n",
    "Now back to our task at hand. You will now need to create a factory function,\n",
    "very much like the example above.\n",
    "You shall create a factory function\n",
    "that creates two recurrent layer instances (one forward, one backward), and then defines an `apply_x` function\n",
    "which applies both layer instances to the same `x` and concatenate the two results.\n",
    "\n",
    "Allright, give it a try! To know how to realize a backward recursion in CNTK,\n",
    "please take a hint from how the forward recursion is done.\n",
    "Please also do the following:\n",
    "* remove the one-word lookahead you added in the previous task, which we aim to replace; and\n",
    "* change the `hidden_dim` parameter from 300 to 150, to keep the total number of model parameters limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add bidirectional recurrence\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  # inject an option to mimic the BrainScript version identically; remove some day\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "#reader = create_reader(data_dir + \"/atis.train.ctf\", is_training=True)\n",
    "#model = create_model()\n",
    "#train(reader, model, max_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works like a charm! This model achieves 1.83%, a tiny bit better than the lookahead model above.\n",
    "The bidirectional model has 40% less parameters than the lookahead one. However, if you go back and look closely\n",
    "at the complete log output (not shown on this web page), you may find that the lookahead one trained\n",
    "about 30% faster.\n",
    "This is because the lookahead model has both less horizontal dependencies (one instead of two\n",
    "recurrences) and larger matrix products, and can thus achieve higher parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Adding Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 722379 parameters in 10 parameter tensors.\n",
      "\n",
      " Minibatch[   1-   1]: loss = 5.717494 * 67, metric = 98.5% * 67\n",
      " Minibatch[   2-   2]: loss = 4.657044 * 63, metric = 73.0% * 63\n",
      " Minibatch[   3-   3]: loss = 3.498547 * 68, metric = 66.2% * 68\n",
      " Minibatch[   4-   4]: loss = 2.029608 * 70, metric = 32.9% * 70\n",
      " Minibatch[   5-   5]: loss = 2.782364 * 65, metric = 36.9% * 65\n",
      " Minibatch[   6-   6]: loss = 2.508770 * 62, metric = 27.4% * 62\n",
      " Minibatch[   7-   7]: loss = 1.439061 * 58, metric = 20.7% * 58\n",
      " Minibatch[   8-   8]: loss = 1.267150 * 70, metric = 14.3% * 70\n",
      " Minibatch[   9-   9]: loss = 0.748346 * 59, metric = 15.3% * 59\n",
      " Minibatch[  10-  10]: loss = 1.607091 * 64, metric = 21.9% * 64\n",
      " Minibatch[  11- 100]: loss = 0.562347 * 5654, metric = 9.4% * 5654\n",
      " Minibatch[ 101- 200]: loss = 0.276623 * 6329, metric = 5.2% * 6329\n",
      " Minibatch[ 201- 300]: loss = 0.189869 * 6259, metric = 4.5% * 6259\n",
      " Minibatch[ 301- 400]: loss = 0.173477 * 6229, metric = 3.8% * 6229\n",
      " Minibatch[ 401- 500]: loss = 0.164194 * 6289, metric = 3.4% * 6289\n",
      "Finished Epoch [1]: [Training] loss = 0.293336 * 36061, metric = 5.5% * 36061\n",
      " Minibatch[   1-   1]: loss = 0.138196 * 4665, metric = 2.8% * 4665\n",
      " Minibatch[   2-   2]: loss = 0.053027 * 60, metric = 1.7% * 60\n",
      " Minibatch[   3-   3]: loss = 0.014280 * 59, metric = 0.0% * 59\n",
      " Minibatch[   4-   4]: loss = 0.126720 * 70, metric = 4.3% * 70\n",
      " Minibatch[   5-   5]: loss = 0.059569 * 48, metric = 0.0% * 48\n",
      " Minibatch[   6-   6]: loss = 0.048835 * 53, metric = 1.9% * 53\n",
      " Minibatch[   7-   7]: loss = 0.054119 * 66, metric = 1.5% * 66\n",
      " Minibatch[   8-   8]: loss = 0.103247 * 52, metric = 1.9% * 52\n",
      " Minibatch[   9-   9]: loss = 0.845632 * 59, metric = 8.5% * 59\n",
      " Minibatch[  10-  10]: loss = 0.041902 * 59, metric = 1.7% * 59\n",
      " Minibatch[  11- 100]: loss = 0.111711 * 5616, metric = 2.6% * 5616\n",
      " Minibatch[ 101- 200]: loss = 0.129186 * 6370, metric = 2.7% * 6370\n",
      " Minibatch[ 201- 300]: loss = 0.091595 * 6374, metric = 1.9% * 6374\n",
      " Minibatch[ 301- 400]: loss = 0.082150 * 6280, metric = 1.8% * 6280\n",
      " Minibatch[ 401- 500]: loss = 0.094651 * 6240, metric = 2.1% * 6240\n",
      "Finished Epoch [2]: [Training] loss = 0.097639 * 36000, metric = 2.2% * 36000\n",
      " Minibatch[   1-   1]: loss = 0.061984 * 4595, metric = 1.9% * 4595\n",
      " Minibatch[   2-   2]: loss = 0.255457 * 60, metric = 5.0% * 60\n",
      " Minibatch[   3-   3]: loss = 0.031336 * 63, metric = 1.6% * 63\n",
      " Minibatch[   4-   4]: loss = 0.234299 * 64, metric = 3.1% * 64\n",
      " Minibatch[   5-   5]: loss = 0.002816 * 51, metric = 0.0% * 51\n",
      " Minibatch[   6-   6]: loss = 0.014195 * 62, metric = 0.0% * 62\n",
      " Minibatch[   7-   7]: loss = 0.004572 * 65, metric = 0.0% * 65\n",
      " Minibatch[   8-   8]: loss = 0.010527 * 68, metric = 0.0% * 68\n",
      " Minibatch[   9-   9]: loss = 0.002207 * 69, metric = 0.0% * 69\n",
      " Minibatch[  10-  10]: loss = 0.078587 * 58, metric = 1.7% * 58\n",
      " Minibatch[  11- 100]: loss = 0.053244 * 5684, metric = 1.2% * 5684\n",
      " Minibatch[ 101- 200]: loss = 0.039685 * 6285, metric = 1.1% * 6285\n",
      " Minibatch[ 201- 300]: loss = 0.043139 * 6305, metric = 1.2% * 6305\n",
      " Minibatch[ 301- 400]: loss = 0.055213 * 6386, metric = 1.5% * 6386\n",
      " Minibatch[ 401- 500]: loss = 0.035843 * 6332, metric = 0.8% * 6332\n",
      "Finished Epoch [3]: [Training] loss = 0.046091 * 35978, metric = 1.2% * 35978\n",
      " Minibatch[   1-   1]: loss = 0.048987 * 4441, metric = 1.4% * 4441\n",
      " Minibatch[   2-   2]: loss = 0.004870 * 64, metric = 0.0% * 64\n",
      " Minibatch[   3-   3]: loss = 0.055821 * 62, metric = 1.6% * 62\n",
      " Minibatch[   4-   4]: loss = 0.003356 * 66, metric = 0.0% * 66\n",
      " Minibatch[   5-   5]: loss = 0.006571 * 60, metric = 0.0% * 60\n",
      " Minibatch[   6-   6]: loss = 0.002904 * 65, metric = 0.0% * 65\n",
      " Minibatch[   7-   7]: loss = 0.008719 * 61, metric = 0.0% * 61\n",
      " Minibatch[   8-   8]: loss = 0.017485 * 65, metric = 0.0% * 65\n",
      " Minibatch[   9-   9]: loss = 0.070154 * 70, metric = 2.9% * 70\n",
      " Minibatch[  10-  10]: loss = 0.246939 * 61, metric = 6.6% * 61\n",
      " Minibatch[  11- 100]: loss = 0.051076 * 5720, metric = 1.4% * 5720\n",
      " Minibatch[ 101- 200]: loss = 0.037921 * 6275, metric = 1.1% * 6275\n",
      " Minibatch[ 201- 300]: loss = 0.031725 * 6250, metric = 0.9% * 6250\n",
      " Minibatch[ 301- 400]: loss = 0.037680 * 6344, metric = 1.1% * 6344\n",
      " Minibatch[ 401- 500]: loss = 0.024939 * 6191, metric = 0.7% * 6191\n",
      "Finished Epoch [4]: [Training] loss = 0.037509 * 36005, metric = 1.0% * 36005\n",
      " Minibatch[   1-   1]: loss = 0.043080 * 4642, metric = 1.1% * 4642\n",
      " Minibatch[   2-   2]: loss = 0.016881 * 59, metric = 0.0% * 59\n",
      " Minibatch[   3-   3]: loss = 0.084227 * 65, metric = 3.1% * 65\n",
      " Minibatch[   4-   4]: loss = 0.009417 * 70, metric = 0.0% * 70\n",
      " Minibatch[   5-   5]: loss = 0.000525 * 59, metric = 0.0% * 59\n",
      " Minibatch[   6-   6]: loss = 0.013439 * 64, metric = 0.0% * 64\n",
      " Minibatch[   7-   7]: loss = 0.070210 * 67, metric = 1.5% * 67\n",
      " Minibatch[   8-   8]: loss = 0.006239 * 70, metric = 0.0% * 70\n",
      " Minibatch[   9-   9]: loss = 0.000596 * 65, metric = 0.0% * 65\n",
      " Minibatch[  10-  10]: loss = 0.005079 * 66, metric = 0.0% * 66\n",
      " Minibatch[  11- 100]: loss = 0.016290 * 5653, metric = 0.6% * 5653\n",
      " Minibatch[ 101- 200]: loss = 0.025369 * 6271, metric = 0.8% * 6271\n",
      " Minibatch[ 201- 300]: loss = 0.028579 * 6274, metric = 0.8% * 6274\n",
      " Minibatch[ 301- 400]: loss = 0.023782 * 6305, metric = 0.7% * 6305\n",
      " Minibatch[ 401- 500]: loss = 0.025200 * 6334, metric = 0.7% * 6334\n",
      "Finished Epoch [5]: [Training] loss = 0.024493 * 35995, metric = 0.7% * 35995\n",
      " Minibatch[   1-   1]: loss = 0.027955 * 4576, metric = 0.7% * 4576\n",
      " Minibatch[   2-   2]: loss = 0.001341 * 69, metric = 0.0% * 69\n",
      " Minibatch[   3-   3]: loss = 0.000963 * 65, metric = 0.0% * 65\n",
      " Minibatch[   4-   4]: loss = 0.004432 * 70, metric = 0.0% * 70\n",
      " Minibatch[   5-   5]: loss = 0.000966 * 60, metric = 0.0% * 60\n",
      " Minibatch[   6-   6]: loss = 0.011763 * 59, metric = 0.0% * 59\n",
      " Minibatch[   7-   7]: loss = 0.118034 * 66, metric = 4.5% * 66\n",
      " Minibatch[   8-   8]: loss = 0.230119 * 56, metric = 7.1% * 56\n",
      " Minibatch[   9-   9]: loss = 0.004041 * 61, metric = 0.0% * 61\n",
      " Minibatch[  10-  10]: loss = 0.013028 * 69, metric = 1.4% * 69\n",
      " Minibatch[  11- 100]: loss = 0.021698 * 5692, metric = 0.7% * 5692\n",
      " Minibatch[ 101- 200]: loss = 0.022522 * 6341, metric = 0.7% * 6341\n",
      " Minibatch[ 201- 300]: loss = 0.023304 * 6323, metric = 0.7% * 6323\n",
      " Minibatch[ 301- 400]: loss = 0.016276 * 6250, metric = 0.5% * 6250\n",
      " Minibatch[ 401- 500]: loss = 0.017372 * 6276, metric = 0.4% * 6276\n",
      "Finished Epoch [6]: [Training] loss = 0.019248 * 36007, metric = 0.6% * 36007\n",
      " Minibatch[   1-   1]: loss = 0.010087 * 4547, metric = 0.4% * 4547\n",
      " Minibatch[   2-   2]: loss = 0.002613 * 63, metric = 0.0% * 63\n",
      " Minibatch[   3-   3]: loss = 0.002642 * 61, metric = 0.0% * 61\n",
      " Minibatch[   4-   4]: loss = 0.001505 * 68, metric = 0.0% * 68\n",
      " Minibatch[   5-   5]: loss = 0.002726 * 62, metric = 0.0% * 62\n",
      " Minibatch[   6-   6]: loss = 0.001234 * 62, metric = 0.0% * 62\n",
      " Minibatch[   7-   7]: loss = 0.002980 * 58, metric = 0.0% * 58\n",
      " Minibatch[   8-   8]: loss = 0.120411 * 66, metric = 6.1% * 66\n",
      " Minibatch[   9-   9]: loss = 0.002466 * 63, metric = 0.0% * 63\n",
      " Minibatch[  10-  10]: loss = 0.001008 * 66, metric = 0.0% * 66\n",
      " Minibatch[  11- 100]: loss = 0.013659 * 5753, metric = 0.5% * 5753\n",
      " Minibatch[ 101- 200]: loss = 0.021154 * 6207, metric = 0.8% * 6207\n",
      " Minibatch[ 201- 300]: loss = 0.016504 * 6388, metric = 0.4% * 6388\n",
      " Minibatch[ 301- 400]: loss = 0.014524 * 6355, metric = 0.5% * 6355\n",
      " Minibatch[ 401- 500]: loss = 0.019502 * 6378, metric = 0.6% * 6378\n",
      "Finished Epoch [7]: [Training] loss = 0.017421 * 35960, metric = 0.6% * 35960\n",
      " Minibatch[   1-   1]: loss = 0.019647 * 4336, metric = 0.7% * 4336\n",
      " Minibatch[   2-   2]: loss = 0.023011 * 61, metric = 1.6% * 61\n",
      " Minibatch[   3-   3]: loss = 0.001076 * 55, metric = 0.0% * 55\n",
      " Minibatch[   4-   4]: loss = 0.015785 * 69, metric = 1.4% * 69\n",
      " Minibatch[   5-   5]: loss = 0.001398 * 69, metric = 0.0% * 69\n",
      " Minibatch[   6-   6]: loss = 0.000863 * 69, metric = 0.0% * 69\n",
      " Minibatch[   7-   7]: loss = 0.002736 * 64, metric = 0.0% * 64\n",
      " Minibatch[   8-   8]: loss = 0.000751 * 66, metric = 0.0% * 66\n",
      " Minibatch[   9-   9]: loss = 0.000508 * 67, metric = 0.0% * 67\n",
      " Minibatch[  10-  10]: loss = 0.001891 * 70, metric = 0.0% * 70\n",
      " Minibatch[  11- 100]: loss = 0.021730 * 5595, metric = 0.6% * 5595\n",
      " Minibatch[ 101- 200]: loss = 0.019331 * 6299, metric = 0.6% * 6299\n",
      " Minibatch[ 201- 300]: loss = 0.012230 * 6295, metric = 0.3% * 6295\n",
      " Minibatch[ 301- 400]: loss = 0.011531 * 6337, metric = 0.3% * 6337\n",
      " Minibatch[ 401- 500]: loss = 0.011989 * 6253, metric = 0.5% * 6253\n",
      "Finished Epoch [8]: [Training] loss = 0.014827 * 36044, metric = 0.5% * 36044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.014827127369880783, 0.004660969925646432)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  # inject an option to mimic the BrainScript version identically; remove some day\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            BatchNormalization(),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            BatchNormalization(),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "reader = create_reader(data_dir + \"/atis.train.ctf\", is_training=True)\n",
    "model = create_model()\n",
    "train(reader, model, max_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Add a Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 902679 parameters in 10 parameter tensors.\n",
      "\n",
      " Minibatch[   1-   1]: loss = 5.907517 * 67, metric = 100.0% * 67\n",
      " Minibatch[   2-   2]: loss = 4.943243 * 63, metric = 84.1% * 63\n",
      " Minibatch[   3-   3]: loss = 3.374831 * 68, metric = 58.8% * 68\n",
      " Minibatch[   4-   4]: loss = 2.692569 * 70, metric = 38.6% * 70\n",
      " Minibatch[   5-   5]: loss = 2.942126 * 65, metric = 43.1% * 65\n",
      " Minibatch[   6-   6]: loss = 2.558049 * 62, metric = 32.3% * 62\n",
      " Minibatch[   7-   7]: loss = 1.299130 * 58, metric = 20.7% * 58\n",
      " Minibatch[   8-   8]: loss = 1.375856 * 70, metric = 15.7% * 70\n",
      " Minibatch[   9-   9]: loss = 1.242764 * 59, metric = 11.9% * 59\n",
      " Minibatch[  10-  10]: loss = 1.922263 * 64, metric = 25.0% * 64\n",
      " Minibatch[  11- 100]: loss = 0.588840 * 5654, metric = 10.0% * 5654\n",
      " Minibatch[ 101- 200]: loss = 0.275335 * 6329, metric = 5.8% * 6329\n",
      " Minibatch[ 201- 300]: loss = 0.191039 * 6259, metric = 4.3% * 6259\n",
      " Minibatch[ 301- 400]: loss = 0.183137 * 6229, metric = 3.9% * 6229\n",
      " Minibatch[ 401- 500]: loss = 0.160493 * 6289, metric = 3.5% * 6289\n",
      "Finished Epoch [1]: [Training] loss = 0.303192 * 36061, metric = 5.8% * 36061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.30319248410476657, 0.05801281162474696)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OneWordLookahead():\n",
    "    x = Placeholder()\n",
    "    apply_x = splice ([x, future_value(x)])\n",
    "    return apply_x\n",
    "\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  # inject an option to mimic the BrainScript version identically; remove some day\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            OneWordLookahead(),\n",
    "            BatchNormalization(),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            BatchNormalization(),\n",
    "            Dense(num_labels)        \n",
    "        ])\n",
    "\n",
    "reader = create_reader(data_dir + \"/atis.train.ctf\", is_training=True)\n",
    "model = create_model()\n",
    "train(reader, model, max_epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Bidirectional Recurrent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 761679 parameters in 10 parameter tensors.\n",
      "\n",
      " Minibatch[   1-   1]: loss = 5.669997 * 67, metric = 95.5% * 67\n",
      " Minibatch[   2-   2]: loss = 4.440025 * 63, metric = 74.6% * 63\n",
      " Minibatch[   3-   3]: loss = 3.123526 * 68, metric = 58.8% * 68\n",
      " Minibatch[   4-   4]: loss = 2.220408 * 70, metric = 40.0% * 70\n",
      " Minibatch[   5-   5]: loss = 2.585504 * 65, metric = 36.9% * 65\n",
      " Minibatch[   6-   6]: loss = 2.259114 * 62, metric = 29.0% * 62\n",
      " Minibatch[   7-   7]: loss = 1.162399 * 58, metric = 19.0% * 58\n",
      " Minibatch[   8-   8]: loss = 1.024514 * 70, metric = 17.1% * 70\n",
      " Minibatch[   9-   9]: loss = 0.870560 * 59, metric = 11.9% * 59\n",
      " Minibatch[  10-  10]: loss = 1.675982 * 64, metric = 26.6% * 64\n",
      " Minibatch[  11- 100]: loss = 0.642226 * 5654, metric = 9.9% * 5654\n",
      " Minibatch[ 101- 200]: loss = 0.274710 * 6329, metric = 5.5% * 6329\n",
      " Minibatch[ 201- 300]: loss = 0.217240 * 6259, metric = 4.5% * 6259\n",
      " Minibatch[ 301- 400]: loss = 0.186087 * 6229, metric = 3.7% * 6229\n",
      " Minibatch[ 401- 500]: loss = 0.161412 * 6289, metric = 3.6% * 6289\n",
      "Finished Epoch [1]: [Training] loss = 0.312397 * 36061, metric = 5.7% * 36061\n",
      " Minibatch[   1-   1]: loss = 0.158400 * 4665, metric = 2.9% * 4665\n",
      " Minibatch[   2-   2]: loss = 0.070518 * 60, metric = 3.3% * 60\n",
      " Minibatch[   3-   3]: loss = 0.014745 * 59, metric = 0.0% * 59\n",
      " Minibatch[   4-   4]: loss = 0.032572 * 70, metric = 1.4% * 70\n",
      " Minibatch[   5-   5]: loss = 0.073976 * 48, metric = 2.1% * 48\n",
      " Minibatch[   6-   6]: loss = 0.007707 * 53, metric = 0.0% * 53\n",
      " Minibatch[   7-   7]: loss = 0.184444 * 66, metric = 4.5% * 66\n",
      " Minibatch[   8-   8]: loss = 0.022854 * 52, metric = 0.0% * 52\n",
      " Minibatch[   9-   9]: loss = 0.869112 * 59, metric = 6.8% * 59\n",
      " Minibatch[  10-  10]: loss = 0.033964 * 59, metric = 1.7% * 59\n",
      " Minibatch[  11- 100]: loss = 0.101947 * 5616, metric = 2.2% * 5616\n",
      " Minibatch[ 101- 200]: loss = 0.126994 * 6370, metric = 2.7% * 6370\n",
      " Minibatch[ 201- 300]: loss = 0.095581 * 6374, metric = 2.0% * 6374\n",
      " Minibatch[ 301- 400]: loss = 0.084527 * 6280, metric = 1.8% * 6280\n",
      " Minibatch[ 401- 500]: loss = 0.081871 * 6240, metric = 1.8% * 6240\n",
      "Finished Epoch [2]: [Training] loss = 0.092263 * 36000, metric = 2.0% * 36000\n",
      " Minibatch[   1-   1]: loss = 0.043616 * 4595, metric = 1.0% * 4595\n",
      " Minibatch[   2-   2]: loss = 0.475264 * 60, metric = 6.7% * 60\n",
      " Minibatch[   3-   3]: loss = 0.005873 * 63, metric = 0.0% * 63\n",
      " Minibatch[   4-   4]: loss = 0.050036 * 64, metric = 3.1% * 64\n",
      " Minibatch[   5-   5]: loss = 0.000672 * 51, metric = 0.0% * 51\n",
      " Minibatch[   6-   6]: loss = 0.075018 * 62, metric = 4.8% * 62\n",
      " Minibatch[   7-   7]: loss = 0.012925 * 65, metric = 0.0% * 65\n",
      " Minibatch[   8-   8]: loss = 0.002258 * 68, metric = 0.0% * 68\n",
      " Minibatch[   9-   9]: loss = 0.000175 * 69, metric = 0.0% * 69\n",
      " Minibatch[  10-  10]: loss = 0.003313 * 58, metric = 0.0% * 58\n",
      " Minibatch[  11- 100]: loss = 0.043466 * 5684, metric = 1.0% * 5684\n",
      " Minibatch[ 101- 200]: loss = 0.027985 * 6285, metric = 0.7% * 6285\n",
      " Minibatch[ 201- 300]: loss = 0.034062 * 6305, metric = 1.0% * 6305\n",
      " Minibatch[ 301- 400]: loss = 0.044340 * 6386, metric = 1.0% * 6386\n",
      " Minibatch[ 401- 500]: loss = 0.029024 * 6332, metric = 0.6% * 6332\n",
      "Finished Epoch [3]: [Training] loss = 0.036863 * 35978, metric = 0.9% * 35978\n",
      " Minibatch[   1-   1]: loss = 0.041780 * 4441, metric = 0.9% * 4441\n",
      " Minibatch[   2-   2]: loss = 0.014298 * 64, metric = 0.0% * 64\n",
      " Minibatch[   3-   3]: loss = 0.003684 * 62, metric = 0.0% * 62\n",
      " Minibatch[   4-   4]: loss = 0.001711 * 66, metric = 0.0% * 66\n",
      " Minibatch[   5-   5]: loss = 0.000681 * 60, metric = 0.0% * 60\n",
      " Minibatch[   6-   6]: loss = 0.000851 * 65, metric = 0.0% * 65\n",
      " Minibatch[   7-   7]: loss = 0.005634 * 61, metric = 0.0% * 61\n",
      " Minibatch[   8-   8]: loss = 0.008039 * 65, metric = 0.0% * 65\n",
      " Minibatch[   9-   9]: loss = 0.080319 * 70, metric = 5.7% * 70\n",
      " Minibatch[  10-  10]: loss = 0.241711 * 61, metric = 4.9% * 61\n",
      " Minibatch[  11- 100]: loss = 0.046261 * 5720, metric = 1.1% * 5720\n",
      " Minibatch[ 101- 200]: loss = 0.030213 * 6275, metric = 0.7% * 6275\n",
      " Minibatch[ 201- 300]: loss = 0.026041 * 6250, metric = 0.7% * 6250\n",
      " Minibatch[ 301- 400]: loss = 0.027558 * 6344, metric = 0.8% * 6344\n",
      " Minibatch[ 401- 500]: loss = 0.013333 * 6191, metric = 0.3% * 6191\n",
      "Finished Epoch [4]: [Training] loss = 0.027729 * 36005, metric = 0.7% * 36005\n",
      " Minibatch[   1-   1]: loss = 0.021752 * 4642, metric = 0.8% * 4642\n",
      " Minibatch[   2-   2]: loss = 0.001042 * 59, metric = 0.0% * 59\n",
      " Minibatch[   3-   3]: loss = 0.018551 * 65, metric = 1.5% * 65\n",
      " Minibatch[   4-   4]: loss = 0.005392 * 70, metric = 0.0% * 70\n",
      " Minibatch[   5-   5]: loss = 0.000233 * 59, metric = 0.0% * 59\n",
      " Minibatch[   6-   6]: loss = 0.003344 * 64, metric = 0.0% * 64\n",
      " Minibatch[   7-   7]: loss = 0.004479 * 67, metric = 0.0% * 67\n",
      " Minibatch[   8-   8]: loss = 0.003765 * 70, metric = 0.0% * 70\n",
      " Minibatch[   9-   9]: loss = 0.000248 * 65, metric = 0.0% * 65\n",
      " Minibatch[  10-  10]: loss = 0.001614 * 66, metric = 0.0% * 66\n",
      " Minibatch[  11- 100]: loss = 0.008408 * 5653, metric = 0.3% * 5653\n",
      " Minibatch[ 101- 200]: loss = 0.013575 * 6271, metric = 0.4% * 6271\n",
      " Minibatch[ 201- 300]: loss = 0.015631 * 6274, metric = 0.4% * 6274\n",
      " Minibatch[ 301- 400]: loss = 0.012644 * 6305, metric = 0.4% * 6305\n",
      " Minibatch[ 401- 500]: loss = 0.011138 * 6334, metric = 0.4% * 6334\n",
      "Finished Epoch [5]: [Training] loss = 0.012446 * 35995, metric = 0.4% * 35995\n",
      " Minibatch[   1-   1]: loss = 0.013774 * 4576, metric = 0.3% * 4576\n",
      " Minibatch[   2-   2]: loss = 0.000140 * 69, metric = 0.0% * 69\n",
      " Minibatch[   3-   3]: loss = 0.000381 * 65, metric = 0.0% * 65\n",
      " Minibatch[   4-   4]: loss = 0.001700 * 70, metric = 0.0% * 70\n",
      " Minibatch[   5-   5]: loss = 0.000241 * 60, metric = 0.0% * 60\n",
      " Minibatch[   6-   6]: loss = 0.004363 * 59, metric = 0.0% * 59\n",
      " Minibatch[   7-   7]: loss = 0.068388 * 66, metric = 1.5% * 66\n",
      " Minibatch[   8-   8]: loss = 0.140129 * 56, metric = 5.4% * 56\n",
      " Minibatch[   9-   9]: loss = 0.001013 * 61, metric = 0.0% * 61\n",
      " Minibatch[  10-  10]: loss = 0.001622 * 69, metric = 0.0% * 69\n",
      " Minibatch[  11- 100]: loss = 0.016553 * 5692, metric = 0.4% * 5692\n",
      " Minibatch[ 101- 200]: loss = 0.018463 * 6341, metric = 0.4% * 6341\n",
      " Minibatch[ 201- 300]: loss = 0.019738 * 6323, metric = 0.6% * 6323\n",
      " Minibatch[ 301- 400]: loss = 0.008415 * 6250, metric = 0.3% * 6250\n",
      " Minibatch[ 401- 500]: loss = 0.008382 * 6276, metric = 0.3% * 6276\n",
      "Finished Epoch [6]: [Training] loss = 0.013598 * 36007, metric = 0.4% * 36007\n",
      " Minibatch[   1-   1]: loss = 0.007799 * 4547, metric = 0.2% * 4547\n",
      " Minibatch[   2-   2]: loss = 0.000037 * 63, metric = 0.0% * 63\n",
      " Minibatch[   3-   3]: loss = 0.004107 * 61, metric = 0.0% * 61\n",
      " Minibatch[   4-   4]: loss = 0.000611 * 68, metric = 0.0% * 68\n",
      " Minibatch[   5-   5]: loss = 0.000175 * 62, metric = 0.0% * 62\n",
      " Minibatch[   6-   6]: loss = 0.001210 * 62, metric = 0.0% * 62\n",
      " Minibatch[   7-   7]: loss = 0.000774 * 58, metric = 0.0% * 58\n",
      " Minibatch[   8-   8]: loss = 0.027437 * 66, metric = 1.5% * 66\n",
      " Minibatch[   9-   9]: loss = 0.018387 * 63, metric = 1.6% * 63\n",
      " Minibatch[  10-  10]: loss = 0.000199 * 66, metric = 0.0% * 66\n",
      " Minibatch[  11- 100]: loss = 0.006363 * 5753, metric = 0.2% * 5753\n",
      " Minibatch[ 101- 200]: loss = 0.010483 * 6207, metric = 0.4% * 6207\n",
      " Minibatch[ 201- 300]: loss = 0.006112 * 6388, metric = 0.2% * 6388\n",
      " Minibatch[ 301- 400]: loss = 0.006251 * 6355, metric = 0.2% * 6355\n",
      " Minibatch[ 401- 500]: loss = 0.012391 * 6378, metric = 0.4% * 6378\n",
      "Finished Epoch [7]: [Training] loss = 0.008183 * 35960, metric = 0.3% * 35960\n",
      " Minibatch[   1-   1]: loss = 0.007223 * 4336, metric = 0.2% * 4336\n",
      " Minibatch[   2-   2]: loss = 0.000453 * 61, metric = 0.0% * 61\n",
      " Minibatch[   3-   3]: loss = 0.002974 * 55, metric = 0.0% * 55\n",
      " Minibatch[   4-   4]: loss = 0.105590 * 69, metric = 2.9% * 69\n",
      " Minibatch[   5-   5]: loss = 0.001209 * 69, metric = 0.0% * 69\n",
      " Minibatch[   6-   6]: loss = 0.000125 * 69, metric = 0.0% * 69\n",
      " Minibatch[   7-   7]: loss = 0.031273 * 64, metric = 1.6% * 64\n",
      " Minibatch[   8-   8]: loss = 0.000092 * 66, metric = 0.0% * 66\n",
      " Minibatch[   9-   9]: loss = 0.000336 * 67, metric = 0.0% * 67\n",
      " Minibatch[  10-  10]: loss = 0.002490 * 70, metric = 0.0% * 70\n",
      " Minibatch[  11- 100]: loss = 0.005090 * 5595, metric = 0.1% * 5595\n",
      " Minibatch[ 101- 200]: loss = 0.005692 * 6299, metric = 0.1% * 6299\n",
      " Minibatch[ 201- 300]: loss = 0.006963 * 6295, metric = 0.2% * 6295\n",
      " Minibatch[ 301- 400]: loss = 0.002665 * 6337, metric = 0.0% * 6337\n",
      " Minibatch[ 401- 500]: loss = 0.003992 * 6253, metric = 0.1% * 6253\n",
      "Finished Epoch [8]: [Training] loss = 0.005204 * 36044, metric = 0.1% * 36044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.005203571479390112, 0.0013317056930418378)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def BiRecurrence(fwd, bwd):\n",
    "    F = Recurrence(fwd)\n",
    "    G = Recurrence(bwd, go_backwards=True)\n",
    "    x = Placeholder()\n",
    "    apply_x = splice ([F(x), G(x)])\n",
    "    return apply_x \n",
    "\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  # inject an option to mimic the BrainScript version identically; remove some day\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            BatchNormalization(),\n",
    "            BiRecurrence(LSTM(hidden_dim), LSTM(hidden_dim)),\n",
    "            BatchNormalization(),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "reader = create_reader(data_dir + \"/atis.train.ctf\", is_training=True)\n",
    "model = create_model()\n",
    "train(reader, model, max_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk-py34]",
   "language": "python",
   "name": "conda-env-cntk-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
