# configuration file for to build language understanding model with ATIS data
# LSTM model is built to tage each word in sentences with its semantic label

WorkDir = work
DataDir = data

modelPath = $WorkDir$/ATIS.slot.lstm
parallelTrain = true

#stderr = $WorkDir$/log

command = Train:Test

precision = "float"
deviceId = "auto"

wordCount = 944    # number of words
labelCount = 127   # number of labels 
Train = [
action = train

traceLevel = 1

    BrainScriptNetworkBuilder = [

        
        inputDim = $wordCount$
        labelDim = $labelCount$
        featDim = inputDim*3   # contextual words are used as features: previous word, current word, next word.
        embDim = 150
        hiddenDim = 300
        maxLayer = 1
        initScale = 6
        initBias = -1.0
        useStabilizer = true
        featuresPW = Input(inputDim)    # the previous word
        featuresCW = Input(inputDim)    # the current word
        featuresNW = Input(inputDim)    # the next word
        features = RowStack(featuresPW : featuresCW : featuresNW)
        
        labels = Input(labelDim, tag = "label")
        
        # embedding layer
        emb = LearnableParameter(embDim, featDim)
        featEmbedded = Times(emb, features)
        
        # build the LSTM stack of a single layer
        lstmDims[i:0..maxLayer] = hiddenDim
        NoAuxInputHook (input, lstmState) = BS.Constants.None
        lstmStack = BS.RNNs.RecurrentLSTMPStack (lstmDims, 
            cellDims=lstmDims,
            featEmbedded, 
            inputDim=embDim,
            previousHook=BS.RNNs.PreviousHC,
            augmentInputHook=BS.RNNs.NoAuxInputHook, 
            augmentInputDim=0,
            enableSelfStabilization=false)

        lstmOutputLayer = Length (lstmStack)-1
        LSTMoutput = lstmStack[lstmOutputLayer].h
    
        W = Parameter(labelDim, hiddenDim, init = "uniform", initValueScale=initScale)
        b = Parameter(labelDim, 1, init = "fixedValue", value=0)
        outputs = W * LSTMoutput + b
        
        cr = CrossEntropyWithSoftmax(labels, outputs)

        criterionNodes = (cr)
        evaluationNodes = (cr)
        outputNodes = (outputs)
    ]

    SGD = [
        # maximum number of epochs
        maxEpochs = 1   # set to 1 so this can be added to regression test. Increase to 20 get a good accuracy

        # for each epoch, maximum number of input samples(words) is set below
        epochSize = 36000   

        # minibatchSize should be larger than the maximum sentence length
        minibatchSize = 70

        learningRatesPerSample = 0.007*2:0.015*18
        gradUpdateType = "FSAdaGrad"

        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0

        # number of minibatches to report progress
        numMBsToShowResult = 100
        
        firstMBsToShowResult = 10 
        
        # if validation shows that the model has no improvement, then do back-up to the previously
        # estimated model and reduce learning rate
        loadBestModel = true

        parallelTrain = [
            parallelizationMethod = "DataParallelSGD"
            parallelizationStartEpoch = 2
            distributedMBReading = true
            dataParallelSGD = [
                gradientBits = 1
            ]
        ]
    ]

    reader = [
        readerType = "CNTKTextFormatReader" 
        file = "$DataDir$/ATIS.train.cntk.sparse" 

        miniBatchMode = "partial" 
        randomize = true
        input = [
            featuresPW = [ 
                alias = "PW"    # previous word
                dim = $wordCount$ 
                format = "sparse" 
            ] 
            featuresCW = [ 
                alias = "CW"    # current word
                dim = $wordCount$ 
                format = "sparse" 
            ]
            featuresNW = [ 
                alias = "NW"    # next word
                dim = $wordCount$ 
                format = "sparse" 
            ]
            
            labels = [ 
                alias = "L"     # label
                dim = $labelCount$
                format = "sparse" 
            ] 
        ]
    ]   
]

# Test the model built from Train
Test = [
    action = write

    traceLevel = 1
    epochSize = 0

    defaultHiddenActivity = 0.1
    BrainScriptNetworkBuilder = [
      modelAsTrained = BS.Network.Load ("$modelPath$")
      final = Softmax(modelAsTrained.outputs)
    ]
    
    outputPath = $WorkDir$/model.writeaction
    outputNodeNames = final
    
    reader = [
        readerType = "CNTKTextFormatReader" 
        file = "$DataDir$/ATIS.test.cntk.sparse" 

        miniBatchMode = "partial" 
        randomize = false
        input = [ 
            featuresPW = [ 
                alias = "PW"    # previous word
                dim = $wordCount$ 
                format = "sparse" 
            ] 
            featuresCW = [ 
                alias = "CW"    # current word
                dim = $wordCount$ 
                format = "sparse" 
            ]
            featuresNW = [ 
                alias = "NW"    # next word
                dim = $wordCount$ 
                format = "sparse" 
            ]            
            
            labels = [ 
                alias = "L"     # label
                dim = $labelCount$        
                format = "sparse" 
            ] 
        ]
    ]
]
