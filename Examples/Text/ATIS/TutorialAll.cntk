# The configuration file to build language understanding model with ATIS corpus.
# An LSTM model is built to tag each word in sentences with its semantic label.

makeMode = false ; traceLevel = 1 ; deviceId = -1

WorkDir = Work
DataDir = Data

modelPath = $WorkDir$/slot.model
parallelTrain = true

#stderr = $WorkDir$/log

command = TrainSlotTagger:RunSlotTagger:EvalSlotTagger

vocabSize = 943    # number of words
numLabels = 129    # number of slot labels
numIntents = 26    # number of intent labels

# The command to train the LSTM model
TrainSlotTagger = [
    action = "train"
    BrainScriptNetworkBuilder = [
        inputDim = $vocabSize$
        labelDim = $numLabels$
        embDim = 150
        hiddenDim = 300
        #hiddenDim = 150

        model = Sequential (
            Parallel ((DelayLayer{T=1} : Identity : DelayLayer{T=-1}), Splice) :  # 3-word window
            EmbeddingLayer {embDim} :                                             # embedding
            RecurrentLSTMLayer {hiddenDim} :              # LSTM
            #Parallel ((RecurrentLSTMLayer {hiddenDim} : RecurrentLSTMLayer {hiddenDim, goBackwards=true}), Splice) :  # bidirectional LSTM
            DenseLayer {labelDim, initValueScale=7}        # output layer
        )

        # features
        query      = Input {inputDim}
        slotLabels = Input {labelDim}

        # model application
        z = model (query)

        # loss and metric
        ce   = CrossEntropyWithSoftmax (slotLabels, z)
        errs = ErrorPrediction         (slotLabels, z)

        featureNodes    = (query)
        labelNodes      = (slotLabels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    ]
    # enable this one instead for intent classification
    Intent_BrainScriptNetworkBuilder = [
        inputDim = $vocabSize$
        labelDim = $numIntents$
        embDim = 150
        hiddenDim = 300

        model = Sequential (
            Parallel ((DelayLayer{T=1} : Identity : DelayLayer{T=-1}), Splice) :  # 3-word window
            EmbeddingLayer {embDim} :                                             # embedding
            RecurrentLSTMLayer {hiddenDim} :               # LSTM
            BS.Sequences.Last :                            # get last LSTM state
            DenseLayer {labelDim, initValueScale=7}        # output layer
        )

        # features
        t = DynamicAxis{}
        query        = Input {inputDim, dynamicAxis=t}
        intentLabels = Input {labelDim}

        # model application
        z = ReconcileDynamicAxis (model (query), intentLabels)

        # loss and metric
        ce   = CrossEntropyWithSoftmax (intentLabels, z)
        errs = ErrorPrediction         (intentLabels, z)

        featureNodes    = (query)
        labelNodes      = (intentLabels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    ]

    SGD = [
        # maximum number of epochs
        maxEpochs = 20   # set to 1 so this can be added to regression test. Increase to 20 get a good accuracy

        # for each epoch, maximum number of input samples(words) is set below
        epochSize = 36000

        # minibatchSize should be larger than the maximum sentence length
        minibatchSize = 70

        learningRatesPerSample = 0.01*2:0.005*12:0.001
        gradUpdateType = "FSAdaGrad"

        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 15.0

        # number of minibatches to report progress
        firstMBsToShowResult = 10
        numMBsToShowResult = 100

        # if validation shows that the model has no improvement, then do back-up to the previously
        # estimated model and reduce learning rate
        loadBestModel = true

        parallelTrain = [
            parallelizationMethod = "DataParallelSGD"
            parallelizationStartEpoch = 2
            distributedMBReading = true
            dataParallelSGD = [
                gradientBits = 1
            ]
        ]
    ]

    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.train.ctf"
        randomize = true
        input = [
            query        = [ alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" ]
            intentLabels = [ alias = "S1" ; dim = $numIntents$ ; format = "sparse" ]
            slotLabels   = [ alias = "S2" ; dim = $numLabels$ ;  format = "sparse" ]
        ]
    ]
]

# Run the model to predict slot labels
RunSlotTagger = [
    action = "write"
    BrainScriptNetworkBuilder = [
        modelAsTrained = BS.Network.Load ("$modelPath$")
        final = Hardmax (modelAsTrained.z)  # make a decision
        labels = Pass (modelAsTrained.slotLabels)
        # enable this for intent classification:
        #labels = Pass (modelAsTrained.intentLabels)
        #t = DynamicAxis()
    ]

    outputPath = $WorkDir$/model.writeaction
    outputNodeNames = slotLabels:final
    # enable this for intent classification:
    #outputNodeNames = intentLabels:final

    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.test.ctf"
        randomize = false
        input = [
            query        = [ alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" ]
            intentLabels = [ alias = "S1" ; dim = $numIntents$ ; format = "sparse" ]
            slotLabels   = [ alias = "S2" ; dim = $numLabels$ ;  format = "sparse" ]
        ]
    ]
]

# Evaluate the model's slot-tagging accuracy (as an error count)
EvalSlotTagger = [
    action = "eval"
    modelPath = $modelPath$  # from outside
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.test.ctf"
        randomize = false
        input = [
            query        = [ alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" ]
            intentLabels = [ alias = "S1" ; dim = $numIntents$ ; format = "sparse" ]
            slotLabels   = [ alias = "S2" ; dim = $numLabels$ ;  format = "sparse" ]
        ]
    ]
]
