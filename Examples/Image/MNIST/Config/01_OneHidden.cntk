# Parameters can be overwritten on the command line
# for example: cntk configFile=myConfigFile RootDir=../.. 
# For running from Visual Studio add
# currentDirectory=$(SolutionDir)/<path to corresponding data folder> 
rootDir = ".."

configDir = "$rootDir$/Config"
dataDir   = "$rootDir$/Data"
outputDir = "$rootDir$/Output"
modelDir  = "$outputDir$/Models"

deviceId = 0
imageLayout = "cudnn"
# override the above as follows when running on CPU:
# deviceId = -1

command = train:test

precision = "float"
modelPath = "$modelDir$/01_OneHidden"
ndlMacros = "$configDir$/Macros.ndl"

# uncomment the following line to write logs to a file 
# stderr = "$outputDir$/01_OneHidden_out"
traceLevel = 1
numMBsToShowResult = 500

# If set to true, always initialize the network on CPU, making initialization consistent across CPU and GPU targets (for testing).
initOnCPUOnly = true

#######################################
#  TRAINING CONFIG                    #
#######################################

train = [
    action = "train"

    BrainScriptNetworkBuilder = [

        # macros to include
        include "Shared.bs"

        featDim = 28 * 28                           # number of pixels
        labelDim = 10                               # number of distinct labels

        features = Input (featDim)
        #featScaled = Constant (1.0 / 256.0) .* features
        featScale = Constant (0.00390625) # TODO: 1/256?
        featScaled = Scale (featScale, features)  # TODO: ElementTimes
        labels = Input (labelDim)

        hiddenDim = 200

        # DNNSigmoidLayer and DNNLayer are defined in Shared.bs
        h1 = DNNSigmoidLayer (featDim,  hiddenDim, featScaled, 1)
        z  = DNNLayer        (hiddenDim, labelDim, h1,         1)

        ce   = CrossEntropyWithSoftmax (labels, z)
        errs = ErrorPrediction         (labels, z)

        # set top5errs as an evaluation node to compute the top-5 error rate (currently inactive since expensive)
        top5errs = ErrorPrediction (labels, z, topN=5)

        # declare special nodes
        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    ]

    #NDLNetworkBuilder = [
    #    networkDescription = "$ConfigDir$/01_OneHidden.ndl"
    #]
    
    SGD = [
        epochSize = 60000
        minibatchSize = 32
        #learningRatesPerSample = 0.003125
        #momentumAsTimeConstant = 0
        learningRatesPerMB = 0.1
        momentumPerMB = 0
        maxEpochs = 30
    ]

    reader = [
        readerType = "CNTKTextFormatReader"
        # See ../REAMDE.md for details on getting the data (Train-28x28_cntk_text.txt).
        file = "$DataDir$/Train-28x28_cntk_text.txt"
        input = [
            features = [
                dim = 784
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]   
]

#######################################
#  TEST CONFIG                        #
#######################################

test = [
    action = "test"
    minibatchSize = 16      # TODO: make this MUCH larger!!

    BrainScriptNetworkBuilder = (new ComputationNetwork [
        network = BS.Network.Load ("$modelPath$")
        errrs = Pass (network.errs, tag="evaluation")   # Pass() creates a new top-level name, so that it shows as "errs"
        #top5errs = Pass (network.top5errs, tag="evaluation")
        top5errrs = ErrorPrediction (network.labels, network.errs.inputs[1] /* network.z*/, topN=5, tag="evaluation")

        #evaluationNodes = (errs : top5errs)   # TODO: make this work as well
    ])

    reader = [
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/Test-28x28_cntk_text.txt"
        input = [
            features = [
                dim = 784
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]
]
