# Simple CIFAR-10 convnet, without and with BatchNormalization.

command = TrainConvNet:Eval
#command = TrainConvNetWithBN:Eval

makeMode = false ; traceLevel = 1 ; deviceId = 0

RootDir = "." ; DataDir  = "$RootDir$" ; ModelDir = "$RootDir$/Output/Models"

modelPath = "$ModelDir$/01_Convolution"

# Training without BN
TrainConvNet = [
    action = "train"

    BrainScriptNetworkBuilder = [
        imageShape = 32:32:3
        labelDim = 10

        Subtract128 (x) = x - Constant (128)

        model = Sequential (
            Subtract128 :
            ConvolutionalLayer {32, (5:5), activation = ReLU, init = "gaussian", initValueScale = 0.0043} :
              MaxPoolingLayer {(3:3), stride = (2:2)} :
            ConvolutionalLayer {32, (5:5), activation = ReLU, init = "gaussian", initValueScale = 1.414} :
              MaxPoolingLayer {(3:3), stride = (2:2)} :
            ConvolutionalLayer {64, (5:5), activation = ReLU, init = "gaussian", initValueScale = 1.414} :
              MaxPoolingLayer {(3:3), stride = (2:2)} :
            DenseLayer {64, activation = ReLU, init = "gaussian", initValueScale = 12} :
            LinearLayer {labelDim, init = "gaussian", initValueScale = 1.5}
        )

        # inputs
        features = Input {imageShape}
        labels   = Input {labelDim}

        # apply model to features
        z = model (features)

        # connect to system
        ce       = CrossEntropyWithSoftmax (labels, z)
        errs     = ErrorPrediction         (labels, z)
        top5Errs = ErrorPrediction         (labels, z, topN=5)  # only used in Eval action

        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)  # top5Errs only used in Eval
        outputNodes     = (z)
    ]

    SGD = [
        epochSize = 49984 ; minibatchSize = 64

        learningRatesPerSample = 0.00015625*10:0.000046875*10:0.000015625
        momentumAsTimeConstant = 600*20:6400
        maxEpochs = 30
        L2RegWeight = 0.03

        firstMBsToShowResult = 10 ; numMBsToShowResult = 500
    ]

    reader = [
        verbosity = 0
        randomize = true
        deserializers = ([
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$DataDir$/cifar-10-batches-py/train_map.txt"
            input = [
                features = [ transforms = ( [ type = "Crop" ; cropType = "random" ; cropRatio = 0.8 ; jitterType = "uniRatio" ] : [ type = "Scale" ; width = 32 ; height = 32 ; channels = 3 ; interpolations = "linear" ] : [ type = "Transpose" ] ) ]
                labels =   [ labelDim = 10 ]
            ]
        ])
    ]
]

# Training with BN
# Not working well on this small set. Overtrains.
TrainConvNetWithBN = [
    action = "train"

    BrainScriptNetworkBuilder = [
        imageShape = 32:32:3
        labelDim = 10

        Subtract128 (x) = x - Constant (128)

        model = Sequential (
            Subtract128 :
            ConvolutionalLayer {32, (5:5), bias = false, init = "gaussian", initValueScale = 0.0043} :
              BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096} : ReLU :
                MaxPoolingLayer {(3:3), stride = (2:2)} :
            ConvolutionalLayer {32, (5:5), bias = false, init = "gaussian", initValueScale = 1.414} :
              BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096} : ReLU :
                MaxPoolingLayer {(3:3), stride = (2:2)} :
            ConvolutionalLayer {64, (5:5), bias = false, init = "gaussian", initValueScale = 1.414} :
              BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096} : ReLU :
                MaxPoolingLayer {(3:3), stride = (2:2)} :
            LinearLayer {64, bias = false, init = "gaussian", initValueScale = 12} :
              BatchNormalizationLayer {normalizationTimeConstant = 4096} : ReLU :
            LinearLayer {labelDim, init = "gaussian", initValueScale = 1.5}
        )

        # inputs
        features = Input {imageShape}
        labels   = Input {labelDim}

        # apply model to features
        z = model (features)

        # connect to system
        ce       = CrossEntropyWithSoftmax (labels, z)
        errs     = ErrorPrediction         (labels, z)
        top5Errs = ErrorPrediction         (labels, z, topN=5)

        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    ]

    SGD = [
        epochSize = 49984 ; minibatchSize = 64

        learningRatesPerSample = 0.00046875*7:0.00015625
        momentumAsTimeConstant = 0
        maxEpochs = 10
        L2RegWeight = 0
        dropoutRate = 0

        firstMBsToShowResult = 10 ; numMBsToShowResult = 500
    ]

    reader = [
        verbosity = 0
        randomize = true
        deserializers = ([
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$DataDir$/cifar-10-batches-py/train_map.txt"
            input = [
                features = [ transforms = ( [ type = "Scale" ; width = 32 ; height = 32 ; channels = 3 ; interpolations = "linear" ] : [ type = "Transpose" ] ) ]
                labels =   [ labelDim = 10 ]
            ]
        ])
    ]
]

# Eval action
Eval = [
    action = "eval"
    minibatchSize = 16
    evalNodeNames = errs:top5Errs  # also test top-5 error rate
    reader = [
        verbosity = 0
        randomize = true
        deserializers = ([
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$DataDir$/cifar-10-batches-py/test_map.txt"
            input = [
                features = [ transforms = ( [ type = "Scale" ; width = 32 ; height = 32 ; channels = 3 ; interpolations = "linear" ] : [ type = "Transpose" ] ) ]
                labels =   [ labelDim = 10 ]
            ]
        ])
    ]
]
