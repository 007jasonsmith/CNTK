# Copyright (c) Microsoft. All rights reserved.
# Licensed under the MIT license. See LICENSE file in the project root for full license information.

############################################################################
# G2P.cntk                                                                 #
#                                                                          #
# Example for sequence-to-sequence modeling for letter-to-sound conversion #
# on the CMUDict                                                           #
############################################################################

# directory defaults (if not overridden)

RunRootDir = "../.."             # default if not overridden
DataDir    = "$RunRootDir$/Data"
CacheDir   = "$DataDir$/cache"   # (not used currently)
ExpRootDir = "$RunRootDir$"

# command to execute

command = train:test:write
#command = write
#command = dump
makeMode = false

# experiment id

# 30-1: dumping all sequences with a small model, to test reader
# 30-2: fixed #samples for momentum calculation
# 30-3: after 'aux' input to LSTMP, in prep for more correct stabiliziation
# 30-4: same as 30-5 but move to new folder and reenabled memsharing
# 30-5: same as 29-5 but rerun with logging of stabilizers
# 29-3: same as 29-5, repro test
# 29-5: same as 29-4 but right-to-left encoder
# 29-4: trying once more with new reader, MB size 70, LR changed to 0.0035,then 0.002; shared stab weights in LSTMP
# 29-2: switched to new reader again, emulating 69-dim outputs  --gives comparable 'ce', but not comparable convergence; SOME att weights are totally flat
# 29-1: same as 29-0 but also switched back to 128 MBSize  --also GLITCH??
# 29-0: switched back to old reader   --not quite the same :( what am I missing?
# 28-5: like 28-4 but using default axis for labels  --minor glitch, got worse
# 28-4: like 28-3 but with momentum changed from 2500 to 1250 (since applied to different #samples)  --GLITCH
# 28-3: like 28-2 but with randomization enabled
# 28-2: like 28-0 but after yet another reader fix   --GLITCH
# 28-1: like 28-0 but halving the MB size--CNTKTextFormatReader interprets the length differently
# 28-0: CNTK reader after data-format fix
# 27-5: trying CNTK reader again after Ryan's bug fix --data format bad
# 27-4: back to LMSequenceReader (regression test)
# 27-3: used </s> for sent end
# 27-2: some refactoring, went back to 26-4 from LMSequenceReader
# 27-1: fixed slicing
# 27-0: incorrect slicing, dropped first input
deviceId = 0
ExpId = 30-$deviceId$-g2p
#ExpId = 22-3-g2p # for decoding a different model
decodeModel = 21

hiddenDim = 512  #420 # 1024 # 200
maxLayer = 2
isBidirectional=false
#hiddenDim = 256 # 1024 # 200
#maxLayer = 2
#isBidirectional=true

# directories
ExpDir    = "$ExpRootDir$/$ExpId$"
ModelDir  = "$ExpDir$/Models"

stderr = $ExpDir$/G2P

precision  = "float"
traceLevel = 1
modelPath  = "$ModelDir$/G2P.dnn"

# decoding config
beamDepth = 1                    # 0=predict; 1=greedy; >1=beam
decodeModelPath = "$modelPath$.$decodeModel$" # epoch to decode appended
decodeOutputPath = "$decodeModelPath$.bl$beamDepth$"

dumpModelPath = "$modelPath$.2" # model to dump if needed

confClassSize = 0
maxLength = 20
isAutoEncoder=false
startSymbol = "<s>"

#useCNTKTextFormatReader = false
#inputVocabSize = 69
#labelVocabSize = 69
#shareEmbeddings = true
#fileExt = "joint"
#readerType = "LMSequenceReader"

useCNTKTextFormatReader = true
inputVocabSize = 29     # 26 letters plus start, end, apostrophe
labelVocabSize = 41     # 39 phonemes (~AX missing), plus start and end symbol (in index 0)
shareEmbeddings = false
fileExt = "ctf"
readerInputDef = [ rawInput = [ alias = "s" ; dim = $inputVocabSize$ ; format = "sparse" ] ; rawLabels = [ alias = "t" ;  dim = $labelVocabSize$ ;  format = "sparse" ] ]
readerType = "CNTKTextFormatReader"

# pathnames
trainFile = "g014b2b.train-dev-20-21.bsf.$fileExt$"
#trainFile = "g014b2b.train-dev-1-21.bsf.$fileExt$" # small one for debugging
validFile = "g014b2b.train-dev-1-21.bsf.$fileExt$"
testFile  = "g014b2b.test.bsf.$fileExt$"

#######################################
#  network definition                 #
#######################################

BrainScriptNetworkBuilder = (new ComputationNetwork [

    # TODO: remove these
    enableTracing = true
    traceFrequency = 100
    tracingLabelMappingFile = "$ModelDir$/vocab.wl"
    beamDepth=3 // for above Trace macros only, need to clean that up
    include "S2SLib.bs"

    # import general config options from outside config values
    useCNTKTextFormatReader = $useCNTKTextFormatReader$

    inputVocabDim = $inputVocabSize$
    labelVocabDim = $labelVocabSize$

    isAutoencoder = $isAutoEncoder$     # input is only one sequence, meant to reproduce itself
    attentionSpan = $maxLength$         # attention window, must be large ebnough for largest input sequence. 0 to disable. Exactly 20 is needed for the g2p CMUDict task
    useBidirectionalEncoder = $isBidirectional$ # bi-directional LSTM for encoder

    shareEmbeddings = $shareEmbeddings$
    hiddenDim       = $hiddenDim$
    attentionDim    = 128 # dim of attention  projection
    maxLayer        = $maxLayer$        # e.g. 2 for 3 hidden layers

    useStabilizer = true
    useEncoder    = true                # if false, this becomes a regular RNN
    useNYUStyle   = false               # if true use thought vector for all inputs, NYU-style

    # dimensions
    embeddingDim = 300
    inputEmbeddingDim = if inputVocabDim < 300 then inputVocabDim else embeddingDim
    labelEmbeddingDim = if labelVocabDim < 300 then labelVocabDim else embeddingDim

    encoderDims[i:0..maxLayer] = hiddenDim # this defines the number of hidden layers in each
    decoderDims[i:0..maxLayer] = hiddenDim # both are one LSTM layer only for now

    #############################################################
    # inputs
    #############################################################

    # inputs and axes must be defined on top-scope level in order to get a clean node name from BrainScript.
    inputAxis = DynamicAxis()
    rawInput  = if useCNTKTextFormatReader && !isAutoencoder
                then Input (inputVocabDim, dynamicAxis=inputAxis, tag='feature')
                else Input (inputVocabDim,                        tag='feature')
    rawLabels = if useCNTKTextFormatReader && !isAutoencoder
                then Input (labelVocabDim, tag='label')
                else rawInput

    # get out input and label data
    # Specifically, if the input and label is on a single line, we must split it in two.
    streams = [
        out = if isAutoencoder || useCNTKTextFormatReader then [
            input  = TraceSparse (rawInput, 'inp')
            labels = TraceSparse_all (rawLabels, 'lbl')
        ]
        else [
            separatorRow = 2                                                                             # row index of separator symbol 
            isSeparator = RowSlice (separatorRow, 1, rawInput)                                           # cut out the separator as a flag
            inInput  = BS.Boolean.Or (FutureValue (1, inInput , defaultHiddenActivation=0), isSeparator) # flag sequence: word is input...
            inLabels = BS.Boolean.Or (PastValue   (1, inLabels, defaultHiddenActivation=0), isSeparator) # ...or labels
            input  = BS.Sequences.Gather (inInput,  rawInput)                                            # use flags to split raw input into input and labels
            labels = BS.Sequences.Gather (inLabels, rawInput)                                            # (both have different lengths)
        ]
    ].out

    # inputs and labels are expected to be surrounded by sentence delimiters, e.g. <s> A B C </s>  ==>  <s> D E F </s>
    # The encoder uses all tokens of 'input', while for the target labels we must exclude the initial sentence start, which is only used as the LM history.

    inputSequence = Pass (streams.input)                             # e.g. <s> A   B   C    </s>
    labelSequence = Pass (Slice (1,  0, streams.labels,  axis=-1))   # e.g. D   E   F   </s>
    labelSentenceStart = Pass (BS.Sequences.First (streams.labels))  # e.g. <s>
    inputSequenceDim = inputVocabDim
    labelSequenceDim = labelVocabDim

    isFirstLabel = BS.Loop.IsFirst (labelSequence)

    #############################################################
    # embeddings  --as long as we cannot read multiple sequences, we got one embedding
    #############################################################

    # Note: Embeddings are linear. Should we use BatchNormalization?

    # note: this is assumed to be applied transposed, hence the swapped dimensions. Actually--why? Still needed?
    Einput  =                                     BS.Parameters.WeightParam (inputSequenceDim, inputEmbeddingDim)
    Elabels = if shareEmbeddings then Einput else BS.Parameters.WeightParam (labelSequenceDim, labelEmbeddingDim)
    EmbedInput (x)  = if inputSequenceDim == inputEmbeddingDim then x else TransposeTimes (Einput, x)
    EmbedLabels (x) = if labelSequenceDim == labelEmbeddingDim then x else TransposeTimes (Elabels, x)

    inputEmbedded  = EmbedInput  (inputSequence)
    labelsEmbedded = EmbedLabels (labelSequence)
    #labelSentenceStartEmbedded = EmbedLabels (labelSentenceStart)
    labelSentenceStartEmbedded = Pass (EmbedLabels (labelSentenceStart))  # TODO: remove Pass() if not actually needed in decoder
    labelSentenceStartEmbeddedScattered = BS.Sequences.Scatter (isFirstLabel, labelSentenceStartEmbedded) # unfortunately needed presently

    S(x) = BS.Parameters.Stabilize (x, enabled=useStabilizer)

    #############################################################
    # encoder (processes inputEmbedded)
    #############################################################

    # TODO: do not reverse our inputs; instead, if needed, use a backwards-running loop here

    encoderFunction = if useBidirectionalEncoder then BS.RNNs.RecurrentBirectionalLSTMPStack else BS.RNNs.RecurrentLSTMPStack
    encoder = encoderFunction (encoderDims, cellDims=encoderDims, S(inputEmbedded), inputDim=inputEmbeddingDim,
        previousHook=BS.RNNs.NextHC,    # TODO: change back to PreviousHC!!!
        enableSelfStabilization=useStabilizer)
    encoderOutput = encoder[Length (encoderDims)-1]

    # There are three ways of passing encoder state:
    #  1. as initial state for decoder (Google style)
    #  2. as side information for every decoder step (NYU style)
    #  3. attention

    # get the final encoder state for use as the initial state
    thoughtVector = [
        h = BS.Sequences.Last (encoderOutput.h)
        c = BS.Sequences.Last (encoderOutput.c)
        dim = encoderOutput.dim
    ]

    thoughtVectorBroadcast = [ # broadcast to all time steps of the target sequence
        h = BS.Sequences.BroadcastSequenceAs (labelsEmbedded, thoughtVector.h)
        c = BS.Sequences.BroadcastSequenceAs (labelsEmbedded, thoughtVector.c)
        dim = thoughtVector.dim
    ]

    #############################################################
    # decoder reordering hook: propagation of beam hypotheses
    #############################################################

    # we bake into the LSTMs to multiply h and c with the 'beamSearchReorderHook' matrix, which is
    # a dummy in training but will be patched through model editing for beam decoding.
    # Specifically, the decoder will replace this by a per-sample matrix that reorders hypotheses according to
    # how they propagate. E.g. the 2nd best in a frame may be the history of the 3rd best in the subsequent frame

    beamSearchReorderHook = Pass (BS.Constants.OnesTensor (1:1))

    # helper functions to delay h and c that apply beam-search reordering, if so configured

    PreviousHCWithReorderingHook (lstmState) = [
       h = BS.Loop.Previous (lstmState.h * beamSearchReorderHook)             // hidden state(t-1)
       c = BS.Loop.Previous (lstmState.c * beamSearchReorderHook)             // cell(t-1)
       dim = lstmState.dim
    ]

    PreviousHCFromThoughtVectorWithReorderingHook (lstmState) = [ # with both thought vector and beam-search hook
       isFirst = BS.Loop.IsFirst (initialState.h)
       # BUGBUG: Should be thoughtVector, but Scatter() can't expand from inside a loop
       h = BS.Boolean.If (isFirst, thoughtVectorBroadcast.h, BS.Loop.Previous (lstmState.h * beamSearchReorderHook)) # hidden state(t-1)
       c = BS.Boolean.If (isFirst, thoughtVectorBroadcast.c, BS.Loop.Previous (lstmState.c * beamSearchReorderHook)) # cell(t-1)
       dim = lstmState.dim
    ]

    #############################################################
    # decoder history hook: LM history, from ground truth vs. output
    #############################################################

    # these are the two choices for the input to the decoder network
    decoderHistoryFromGroundTruth = labelsEmbedded              # for training, decoder input is ground truth...
    decoderHistoryFromOutput = Pass (EmbedLabels (Hardmax (z))) # ...but for (greedy) decoding, the decoder's output is its previous input

    # during training, we use ground truth. For decoding, we will rewire decoderHistoryHook = decoderHistoryFromOutput
    decoderHistoryHook = Pass (decoderHistoryFromGroundTruth) # this gets redirected in decoding to feed back decoding output instead

    #############################################################
    # decoder
    #############################################################

    decoderInput    = Pass (BS.Boolean.If (isFirstLabel, labelSentenceStartEmbeddedScattered, BS.Loop.Previous (decoderHistoryHook)))
    decoderInputDim = labelEmbeddingDim

    decoderDynamicAxis = labelsEmbedded
    FixedWindowAttentionHook = BS.Seq2Seq.CreateAugmentWithFixedWindowAttentionHook (attentionDim, attentionSpan, decoderDynamicAxis, encoderOutput, enableSelfStabilization=useStabilizer)

    # TODO: collapse this into a single first-layer function; factor to lib; then merge with RecurrentLSTMPStack()
    # NYU style: The decoder starts with hidden state 0 and takes as input [thoughtVectorBroadcast.h; previous word].
    decoderOutputLayer = Length (decoderDims)-1
    decoder[i:0..decoderOutputLayer] =
        if i == 0
        then if useEncoder && useNYUStyle then BS.RNNs.RecurrentLSTMP (decoderDims[i], cellDim=decoderDims[i],
                                                                       RowStack (S(thoughtVectorBroadcast.h) : S(decoderInput)), inputDim=(thoughtVector.dim + decoderInputDim),
                                                                       previousHook=PreviousHCWithReorderingHook,
                                                                       enableSelfStabilization=useStabilizer)
             else if useEncoder && attentionSpan > 0 then BS.RNNs.RecurrentLSTMP (decoderDims[i], cellDim=decoderDims[i],
                                                                                  S(decoderInput), inputDim=decoderInputDim,
                                                                                  augmentInputHook=FixedWindowAttentionHook, augmentInputDim=encoderOutput.dim,
                                                                                  previousHook=PreviousHCWithReorderingHook,
                                                                                  enableSelfStabilization=useStabilizer)
             else BS.RNNs.RecurrentLSTMP (decoderDims[i], cellDim=decoderDims[i],
                                          S(decoderInput), inputDim=decoderInputDim,
                                          previousHook=PreviousHCFromThoughtVectorWithReorderingHook, # Previous() function with thought vector as initial state
                                          enableSelfStabilization=useStabilizer)
        else BS.RNNs.RecurrentLSTMP (decoderDims[i], cellDim=decoderDims[i],
                                     S(decoder[i-1].h), inputDim=/*decoderDims[i-1]*/ decoder[i-1].dim,
                                     previousHook=PreviousHCWithReorderingHook,
                                     enableSelfStabilization=useStabilizer)
    decoderOutput = decoder[decoderOutputLayer].h
    #decoderDim = decoderOutput.dim
    decoderDim = decoderDims[decoderOutputLayer]

    #############################################################
    # softmax output layer
    #############################################################

    W = BS.Parameters.WeightParam (labelSequenceDim, decoderDim)
    B = BS.Parameters.BiasParam (labelSequenceDim)

    z = W * S(decoderOutput) + B;  // top-level input to Softmax

    #############################################################
    # training criteria
    #############################################################

    ce   = Pass (ReduceLogSum (z) - ReduceSum (labelSequence .*          z ), tag='criterion')
    errs = Pass (BS.Constants.One - ReduceSum (labelSequence .* Hardmax (z)), tag='evaluation')

    #ce2 = Negate (ReduceSum (labelSequence .* LogSoftmax (z)), tag='evaluation')
    #ce1 = CrossEntropyWithSoftmax (labelSequence, z, tag='evaluation')   // this is the training objective
    #errs = ErrorPrediction         (labelSequence, z, tag='evaluation')  // this also gets tracked

    # score output for decoding
    scoreSequence = Pass (z)
])

#######################################
# shared reader definition            #
#######################################

reader = [
    readerType = "$readerType$"
    file = "$DataDir$/$trainFile$"
    randomize = "auto"

    # specific to CNTKTextFormatReader
    skipSequenceIds = "false"
    maxErrors = 100
    traceLevel = 2
    chunkSizeInBytes = 30000000 # large enough for entire data set

    # specific to LMSequenceReader
    mode = "softmax"                    # TODO: find out what this means
    nbruttsineachrecurrentiter = 0      # 0 means auto-fill given minibatch size
    cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once
 
    input = $readerInputDef$
    #[
    #    rawInput = [
    #        alias = "s"
    #        dim = $inputVocabSize$
    #        format = "sparse"
    #    ]
    #    rawLabels = [
    #        alias = "t"
    #        dim = $labelVocabSize$
    #        format = "sparse"
    #    ]
    #]
]

cvReader = [
    readerType = "$readerType$"
    file = "$DataDir$/$validFile$"
    randomize = "none"
    skipSequenceIds = "false"
    maxErrors = 100
    traceLevel = 2
 
    input = $readerInputDef$
    #input = [
    #    rawInput  = [
    #        alias = "s"
    #        dim = $inputVocabSize$
    #        format = "sparse"
    #    ]
    #    rawLabels = [
    #        alias = "t"
    #        dim = $labelVocabSize$     # 39 phonemes (~AX missing), start and end symbol (in index 0)
    #        format = "sparse"
    #    ]
    #]
]

#######################################
#  TRAINING CONFIG                    #
#######################################

train = [
    action = "train"
    traceLevel = 1
    epochSize = 0               # (for quick tests, this can be overridden with something small)

    # BrainScriptNetworkBuilder is defined in outer scope

    SGD = [
        minibatchSize = 128:128:256:512
        #minibatchSize = 64:64:128:256
        #minibatchSize = 70:70:70:140:280
        learningRatesPerSample = 0.0035*2:0.002 #0.01 #0.005 # 0.01
        momentumAsTimeConstant = 1000 #2500
        gradientClippingWithTruncation = true   # (as opposed to clipping the Frobenius norm of the matrix)
        clippingThresholdPerSample = 1   #15.0 # 1#visibly impacts objectives, but not final result, so keep it for safety
        maxEpochs = 50
        numMBsToShowResult = 100
        firstMBsToShowResult = 10
        gradUpdateType = "none" # FSAdaGrad?
        loadBestModel = false   # true # broken for some models (rereading overwrites something that got set by validation)

        # tracing (enable these for debugging)
        #traceNodeNamesReal = labelsEmbedded:decoderInput:"decoder[0].lstmState._privateInnards.ht":z.Plus_left.Times_right.result:z:ce
        #traceNodeNamesReal = labelsEmbedded:decoderInput:z:ce
        #traceNodeNamesCategory = inputSequence.out:labelSequence

        dropoutRate = 0.0

        # settings for Auto Adjust Learning Rate
        AutoAdjust = [
            autoAdjustLR = "adjustAfterEpoch"
            reduceLearnRateIfImproveLessThan = 0.001
            continueReduce = false
            increaseLearnRateIfImproveMoreThan = 1000000000
            learnRateDecreaseFactor = 0.5
            learnRateIncreaseFactor = 1.382
            numMiniBatch4LRSearch = 100
            numPrevLearnRates = 5
            numBestSearchEpoch = 1
        ]
    ]
]

#######################################
#  DUMP CONFIG                        #
#######################################

# dumps the model, specifically the learnable parameters

dump = [
    action = "dumpnode"
    modelPath = "$dumpModelPath$"
    outputFile = "$dumpModelPath$.txt"
]

#######################################
#  TEST CONFIG                        #
#######################################

test = [
    action = "eval"

    # correspond to the number of words/characteres to train in a minibatch
    minibatchSize = 8192                # choose as large as memory allows for maximum GPU concurrency
    # need to be small since models are updated for each minibatch
    traceLevel = 1
    epochSize = 0

    reader = [
        file = "$DataDir$/$testFile$"
        #randomize = "none" # gets ignored
    
        # everything below here is duplicated from 'reader'
        readerType = LMSequenceReader
        mode = "softmax"
        nbruttsineachrecurrentiter = 0      # 0 means auto-fill given minibatch size
        cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once
    
        # word class info
        wordclass = "$ModelDir$/vocab.txt"
    
        #### write definition
        # if writerType is set, we will cache to a binary file
        # if the binary file exists, we will use it instead of parsing this file
        #writerType = BinaryReader
        wfile = $CacheDir$\sequenceSentence.bin
        # if calculated size would be bigger, that is used instead
        wsize = 256
        #wrecords - number of records we should allocate space for in the file
        # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
        wrecords = 1000
        #windowSize - number of records we should include in BinaryWriter window
        windowSize = 10000
    
        # additional features sections
        # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
        input = [
            dim = 0     # no (explicit) labels   ...labelDim correct??
            ### write definition
            sectionType = "data"
        ]
        # labels sections
        # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
        # labels sections  --this is required, but our labels are extracted from the inLabels
        inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
            dim = 1
    
            # vocabulary size
            labelType = "category"
            labelDim = "$inputVocabSize$"
            labelMappingFile = "$ModelDir$/vocab.wl"
            beginSequence = "$startSymbol$" # "</s>"
            endSequence   = "</s>"
    
            #### Write definition ####
            # sizeof(unsigned) which is the label index type
            elementSize=4
            sectionType=labels
            mapping = [
              #redefine number of records for this section, since we don't need to save it for each data record
              wrecords=11
              #variable size so use an average string size
              elementSize=10
              sectionType=labelMapping
            ]
            category = [
              dim=11
              #elementSize=sizeof(ElemType) is default
              sectionType=categoryLabels
            ]
        ]
        outputDummy = [
            labelType = "none"
        ]
    ]
]

#######################################
#  WRITE CONFIG                       #
#######################################

# This will decode the test set. The beamDepth parameter specifies the decoding mode:
#  beamDepth = 0: word prediction given ground truth history (only useful for perplexity measurement)
#  beamDepth = 1: greedy decoding: At each time step, choose a word greedily
#  beamDepth > 1: beam decoder. Keep 'beamDepth' best hypotheses, and output their globally best at the end.

write = [
    action = "write"

    # select the decoder
    BrainScriptNetworkBuilder = (
        # beamDepth = 0 will decode with the unmodified model.
        # beamDepth = 1 will modify the model to use the decoding output as the decoder's input.
        # beamDepth > 1 will modify the model to track multiple hypotheses and select the globally best at the end.
        if      $beamDepth$ == 0 then BS.Network.Load ("$decodeModelPath$")
        else if $beamDepth$ == 1 then BS.Seq2Seq.GreedySequenceDecoderFrom (BS.Network.Load ("$decodeModelPath$"))
        else                          BS.Seq2Seq.BeamSearchSequenceDecoderFrom (BS.Network.Load ("$decodeModelPath$"), $beamDepth$)
    )

    outputPath = $decodeOutputPath$
    #outputPath = "-"                    # "-" will write to stdout; useful for debugging

    outputNodeNames = inputsOut:labelsOut:decodeOut:network.beamDecodingModel.inputsOut:network.beamDecodingModel.labelsOut:network.beamDecodingModel.decodeOut

    # output format
    # We configure the output to emit a flat sequence of token strings.
    format = [
        type = "category"
        transpose = false
        labelMappingFile = "$ModelDir$/vocab.wl"
    ]

    minibatchSize = 8192                # choose this to be big enough for the longest sentence
    traceLevel = 1
    epochSize = 0

    reader = [
        file = "$DataDir$/$testFile$"
        #randomize = "none" # gets ignored
    
        # everything below here is duplicated from 'reader'
        readerType = LMSequenceReader
        mode = "softmax"
        nbruttsineachrecurrentiter = 1      # 1 means one per minibatch
        cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once

        # word class info
        wordclass = "$ModelDir$/vocab.txt"

        #### write definition
        # if writerType is set, we will cache to a binary file
        # if the binary file exists, we will use it instead of parsing this file
        #writerType = BinaryReader
        wfile = $CacheDir$\sequenceSentence.bin
        # if calculated size would be bigger, that is used instead
        wsize = 256
        #wrecords - number of records we should allocate space for in the file
        # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
        wrecords = 1000
        #windowSize - number of records we should include in BinaryWriter window
        windowSize = 10000
    
        # additional features sections
        # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
        input = [
            dim = 0     # no (explicit) labels   ...labelDim correct??
            ### write definition
            sectionType = "data"
        ]
        # labels sections
        # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
        # labels sections  --this is required, but our labels are extracted from the inLabels
        inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
            dim = 1

            # vocabulary size
            labelType = "category"
            labelDim = "$inputVocabSize$"
            labelMappingFile = "$ModelDir$/vocab.wl"
            beginSequence = "$startSymbol$" # "</s>"
            endSequence   = "</s>"

            #### Write definition ####
            # sizeof(unsigned) which is the label index type
            elementSize=4
            sectionType=labels
            mapping = [
              #redefine number of records for this section, since we don't need to save it for each data record
              wrecords=11
              #variable size so use an average string size
              elementSize=10
              sectionType=labelMapping
            ]
            category = [
              dim=11
              #elementSize=sizeof(ElemType) is default
              sectionType=categoryLabels
            ]
        ]
        outputDummy = [
            labelType = "none"
        ]
    ]
]


# some outdated things we should remove:

lmreader = [
    file = "$DataDir$/$trainFile$"
    #randomize = "auto" # gets ignored

    readerType = LMSequenceReader
    mode = "softmax"                    # TODO: find out what this means
    nbruttsineachrecurrentiter = 0      # 0 means auto-fill given minibatch size
    cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once

    # word class info
    wordclass = "$ModelDir$/vocab.txt"

    #### write definition
    # if writerType is set, we will cache to a binary file
    # if the binary file exists, we will use it instead of parsing this file
    #writerType = BinaryReader
    wfile = $CacheDir$\sequenceSentence.bin
    # if calculated size would be bigger, that is used instead
    wsize = 256
    #wrecords - number of records we should allocate space for in the file
    # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
    wrecords = 1000
    #windowSize - number of records we should include in BinaryWriter window
    windowSize = 10000

    # additional features sections
    # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
    input = [
        dim = 0     # no (explicit) labels   ...labelDim correct??
        ### write definition
        sectionType = "data"
    ]
    # labels sections
    # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
    # labels sections  --this is required, but our labels are extracted from the inLabels
    inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
        dim = 1

        # vocabulary size
        labelType = "category"
        labelDim = "$inputVocabSize$"
        labelMappingFile = "$ModelDir$/vocab.wl"
        beginSequence = "$startSymbol$" # "</s>"
        endSequence   = "</s>"

        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping = [
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=11
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category = [
          dim=11
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
    ]
    outputDummy = [
        labelType = "none"
    ]
]

lmcvReader = [
    file = "$DataDir$/$validFile$"
    #randomize = "none" # gets ignored

    # everything below here is duplicated from 'reader'
    readerType = LMSequenceReader
    mode = "softmax"
    nbruttsineachrecurrentiter = 0      # 0 means auto-fill given minibatch size
    cacheBlockSize = 100000000          # read block size. This value is large enough to load entire corpus at once

    # word class info
    wordclass = "$ModelDir$/vocab.txt"

    #### write definition
    # if writerType is set, we will cache to a binary file
    # if the binary file exists, we will use it instead of parsing this file
    #writerType = BinaryReader
    wfile = $CacheDir$\sequenceSentence.bin
    # if calculated size would be bigger, that is used instead
    wsize = 256
    #wrecords - number of records we should allocate space for in the file
    # files cannot be expanded, so this should be large enough. If known modify this element in config before creating file
    wrecords = 1000
    #windowSize - number of records we should include in BinaryWriter window
    windowSize = 10000

    # additional features sections
    # For input labels, we need both 'features' and the first labels section (called 'inputLabelsDef' below)
    input = [
        dim = 0     # no (explicit) labels   ...labelDim correct??
        ### write definition
        sectionType = "data"
    ]
    # labels sections
    # TODO: seems we must specify two labels (in and out), but labelType = "none" is allowed
    # labels sections  --this is required, but our labels are extracted from the inLabels
    inputLabelsDef = [ # BUGBUG: Make sure that this section name comes before the dummy output labels alphabetically
        dim = 1

        # vocabulary size
        labelType = "category"
        labelDim = "$inputVocabSize$"
        labelMappingFile = "$ModelDir$/vocab.wl"
        beginSequence = "</s>"
        endSequence   = "</s>"

        #### Write definition ####
        # sizeof(unsigned) which is the label index type
        elementSize=4
        sectionType=labels
        mapping = [
          #redefine number of records for this section, since we don't need to save it for each data record
          wrecords=11
          #variable size so use an average string size
          elementSize=10
          sectionType=labelMapping
        ]
        category = [
          dim=11
          #elementSize=sizeof(ElemType) is default
          sectionType=categoryLabels
        ]
    ]
    outputDummy = [
        labelType = "none"
    ]
]
