# Simple CIFAR-10 convnet, without and with BatchNormalization.

command = TrainConvNet:Eval
#command = TrainConvNetWithBN:Eval

makeMode = false ; traceLevel = 0 ; deviceId = "auto"

RootDir = "." ; DataDir  = "$RootDir$" ; ModelDir = "$RootDir$/Output/Models"

modelPath = "$ModelDir$/cifar10.cmf"

# Training without BN
TrainConvNet = {
    action = "train"

    BrainScriptNetworkBuilder = {
        imageShape = 32:32:3
        labelDim = 10

        # basic model
        model_1 (features) =
        {
            featNorm = features - Constant (128)
            l1 = ConvolutionalLayer {32, (5:5), pad = true, activation = ReLU,
                                     init = "gaussian", initValueScale = 0.0043} (featNorm)
            p1 = MaxPoolingLayer {(3:3), stride = (2:2)} (l1)
            l2 = ConvolutionalLayer {32, (5:5), pad = true, activation = ReLU,
                                     init = "gaussian", initValueScale = 1.414} (p1)
            p2 = MaxPoolingLayer {(3:3), stride = (2:2)} (l2)
            l3 = ConvolutionalLayer {64, (5:5), pad = true, activation = ReLU,
                                     init = "gaussian", initValueScale = 1.414} (p2)
            p3 = MaxPoolingLayer {(3:3), stride = (2:2)} (l3)
            d1 = DenseLayer {64, activation = ReLU, init = "gaussian", initValueScale = 12} (p3)
            z  = LinearLayer {10, init = "gaussian", initValueScale = 1.5} (d1)
        }.z

        # with self-defined layer
        MyLayer (x, dim, initValueScale) =
        {
            c = ConvolutionalLayer {dim, (5:5), pad = true, activation = ReLU, init = "gaussian", initValueScale = initValueScale} (x)
            p = MaxPoolingLayer {(3:3), stride = (2:2)} (c)
        }.p
        model_f (features) =
        {
            featNorm = features - Constant (128)
            p1 = MyLayer (featNorm, 32, 0.0043)
            p2 = MyLayer (p1,       32, 1.414)
            p3 = MyLayer (p2,       64, 1.414)
            d1 = DenseLayer {64, activation = ReLU, init = "gaussian", initValueScale = 12} (p3)
            d1_d = Dropout (d1)
            z  = LinearLayer {10, init = "gaussian", initValueScale = 1.5} (d1_d)
        }.z

        // --- with BatchNorm
        # with self-defined layer
        MyLayerWithBN (x, dim, initValueScale) =
        {
            c = ConvolutionalLayer {dim, (5:5), pad = true, init = "gaussian", initValueScale = initValueScale} (x)
            b = BatchNormalizationLayer {spatialRank = 2} (c)
            r = ReLU (b)
            p = MaxPoolingLayer {(3:3), stride = (2:2)} (r)
        }.p
        model_bn (features) =
        {
            featNorm = features - Constant (128)
            p1 = MyLayerWithBN (featNorm, 32, 0.0043)
            p2 = MyLayerWithBN (p1,       32, 1.414)
            p3 = MyLayerWithBN (p2,       64, 1.414)
            d1 = DenseLayer {64, init = "gaussian", initValueScale = 12} (p3)
            d1_bnr = ReLU (BatchNormalizationLayer {} (d1))
            d1_d = Dropout (d1_bnr)
            z  = LinearLayer {10, init = "gaussian", initValueScale = 1.5} (d1_d)
        }.z

        // --- ResNet
        MyConvBN (x, dim, initValueScale, stride) =  # TO BE WRITTEN BY PARTICIPANT
        {
            c = ConvolutionalLayer {dim, (3:3), pad = true, stride = (stride:stride), bias = false, init = "gaussian", initValueScale = initValueScale} (x)
            b = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096} (c)
        }.b
        MyConvBNReLU (x, dim, initValueScale, stride) =
        {
            c = ConvolutionalLayer {dim, (3:3), pad = true, stride = (stride:stride), bias = false, init = "gaussian", initValueScale = initValueScale} (x)
            b = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096} (c)
            r = ReLU (b)
        }.r
        ResNetNode (x, dim) =
        {
            c1 = MyConvBNReLU (x,  dim, 7.07, 1)
            X2 = MyConvBNReLU (c1, dim, 7.07, 1)     # wrong
            c2 = MyConvBN     (c1, dim, 7.07, 1)
            r = ReLU (x + c2)
        }.r   # change to X2
        ResNetResample (x, dim) =
        {
            x2 = MaxPoolingLayer {(1:1), stride = (2:2)} (x)  # sub-sample by 2
            pad = ConstantTensor (0, (1:1:dim/2))             # pad with zeroes
            p = Splice ((x2 : pad), axis = 3)
        }.p
        ResNetIncNode (x, dim) =
        {
            c1 = MyConvBNReLU (x,  dim, 7.07, 2)
            c2 = MyConvBN     (c1, dim, 7.07, 1)

            px = ResNetResample (x, dim)
            b = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096} (px)

            r = ReLU (b + c2)  # ReLU between C1 and C2 and after summation
        }.r

        # these are the ones the participants are given upfront
        ResNetNode1 (x, dim) =
        {
            c1 = MyConvBNReLU (x,  dim, 7.07, 1)
            c2 = MyConvBNReLU (c1, dim, 7.07, 1)
        }.c2
        ResNetIncNode1 (x, dim) =
        {
            px = ResNetResample (x, dim)  # sub-sample but double the dims
            b = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096} (px)
            r = ReLU (b)
        }.r

        # this must be written
        ResNetNodeStack (x, dim, L) =
            if L == 0 then x
            else ResNetNode (ResNetNodeStack (x, dim, L-1), dim)

        model (features) =
        {
            conv1 = MyConvBNReLU (features, 16, 0.26, 1)
            #rn1 = ResNetNode1 (ResNetNode1 (ResNetNode1 (conv1, 16), 16), 16)
            rn1   = ResNetNodeStack (conv1, 16, 3)  # 3 means 3 such nodes

            rn2_1 = ResNetIncNode1 (rn1, 32)
            #rn2 = ResNetNode1 (ResNetNode1 (rn2_1, 32), 32)
            rn2   = ResNetNodeStack (rn2_1, 32, 2)

            rn3_1 = ResNetIncNode1 (rn2, 64)
            #rn3 = ResNetNode1 (ResNetNode1 (rn3_1, 64), 64)
            rn3   = ResNetNodeStack (rn3_1, 64, 2)

            pool = AveragePoolingLayer {(8:8)} (rn3)

            z = LinearLayer {labelDim, init = "gaussian", initValueScale = 0.4} (pool)
        }.z

        // --- ResNet, functional style
        MyConvBNLayer {dim, initValueScale, stride} =
        {
            # note: (3:3), while the macro above is (5:5)
            C = ConvolutionalLayer {dim, (3:3), pad = true, stride = (stride:stride), bias = false, init = "gaussian", initValueScale = initValueScale}
            B = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096}
            apply (x) = B(C(x))
        }.apply
        ResNetLayer {dim, initValueScale} =
        {
            C1 = MyConvBNLayer {dim, initValueScale, 1}  # first convolution layer
            C2 = MyConvBNLayer {dim, initValueScale, 1}  # second convolution layer
            #B = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096}
            # ^^ Note: Adding an exra BN to 'x' trains slightly better.
            apply (x) = ReLU (x + C2(ReLU(C1(x))))  # ReLU between C1 and C2 and after summation
        }.apply
        ResNetIncLayer {dim, initValueScale} =
        {
            # first branch. This doubles the #channels but halves the image size
            C1 = MyConvBNLayer {dim, initValueScale, 2}  # first convolution layer, stride = 2
            C2 = MyConvBNLayer {dim, initValueScale, 1}  # second convolution layer

            # second branch:
            # sub-sample spatially by a factor of 2
            DownSamplingLayer {stride} = MaxPoolingLayer {(1:1), stride = stride}
            # append dim/2 zero output channels
            pad = ConstantTensor (0, (1:1:dim/2))  # the 1s will broadcast to image size
            P(x) = Splice ((DownSamplingLayer {(2:2)} (x) : pad), axis = 3)
            B = BatchNormalizationLayer {spatialRank = 2, normalizationTimeConstant = 4096}

            # layer sums both branches and rectifies the result
            apply (x) = ReLU (B(P(x)) + C2(ReLU(C1(x))))  # ReLU between C1 and C2 and after summation
        }.apply
        model_resNet (features) =
        {
            conv1 = MyConvBNLayer {16, 0.26, 1} (features)
            rl1   = ReLU (conv1)
            rn1   = LayerStack {3, _ => ResNetLayer {16, 7.07}} (rl1)

            rn2_1 = ResNetIncLayer {32, 7.07} (rn1)
            rn2   = LayerStack {2, _ => ResNetLayer {32, 7.07}} (rn2_1)

            rn3_1 = ResNetIncLayer {64, 7.07} (rn2)
            rn3   = LayerStack {2, _ => ResNetLayer {64, 7.07}} (rn3_1)

            pool = AveragePoolingLayer {(8:8)} (rn3)

            z = LinearLayer {labelDim, init = "gaussian", initValueScale = 0.4} (pool)
        }.z

        # inputs
        features = Input {imageShape}
        labels   = Input {labelDim}

        # apply model to features
        z = model (features)

        # connect to system
        ce       = CrossEntropyWithSoftmax (labels, z)
        errs     = ErrorPrediction         (labels, z)
        top5Errs = ErrorPrediction         (labels, z, topN=5)  # only used in Eval action

        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)  # top5Errs only used in Eval
        outputNodes     = (z)
    }

    SGD = {
        epochSize = 50000

        # without BatchNormalization:
        #maxEpochs = 30 ; minibatchSize = 64
        #learningRatesPerSample = 0.00015625*10:0.000046875*10:0.000015625
        #momentumAsTimeConstant = 600*20:6400
        #L2RegWeight = 0.03
        #dropoutRate = 0*5:0.5   ##### added

        # with BatchNormalization:
        #maxEpochs = 30 ; minibatchSize = 64
        #learningRatesPerSample = 0.00046875*7:0.00015625*10:0.000046875*10:0.000015625
        #momentumAsTimeConstant = 0
        #L2RegWeight = 0
        #dropoutRate = 0*5:0.5   ##### added

        # ResNet
        maxEpochs = 160 ; minibatchSize = 128
        learningRatesPerSample = 0.0078125*80:0.00078125*40:0.000078125
        momentumAsTimeConstant = 1200
        L2RegWeight = 0.0001

        firstMBsToShowResult = 10 ; numMBsToShowResult = 500
    }

    reader = {
        verbosity = 0
        randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$DataDir$/cifar-10-batches-py/train_map.txt"
            input = {
                features = { transforms = (
                    { type = "Crop" ; cropType = "random" ; cropRatio = 0.8 ; jitterType = "uniRatio" } :
                    { type = "Scale" ; width = 32 ; height = 32 ; channels = 3 ; interpolations = "linear" } :
                    { type = "Transpose" }
                )}
                labels =   { labelDim = 10 }
            }
        })
    }
}

# Eval action
Eval = {
    action = "eval"
    minibatchSize = 16
    evalNodeNames = errs:top5Errs  # also test top-5 error rate
    reader = {
        verbosity = 0
        randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$DataDir$/cifar-10-batches-py/test_map.txt"
            input = {
                features = { transforms = (
                   { type = "Scale" ; width = 32 ; height = 32 ; channels = 3 ; interpolations = "linear" } :
                   { type = "Transpose" }
                )}
                labels =   { labelDim = 10 }
            }
        })
    }
}
