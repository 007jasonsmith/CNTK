# CNTK Configuration File for creating a slot tagger and an intent tagger.

command = TrainTagger:TestTagger

makeMode = false ; traceLevel = 0 ; deviceId = "auto"

rootDir = "." ; dataDir  = "$rootDir$" ; modelDir = "$rootDir$/Models"

modelPath = "$modelDir$/slu.cmf"

vocabSize = 943 ; numLabels = 129 ; numIntents = 26    # number of words in vocab, slot labels, and intent labels

# The command to train the LSTM model
TrainTagger = {
    action = "train"
    BrainScriptNetworkBuilder = {
        inputDim = $vocabSize$
        labelDim = $numLabels$
        embDim = 150
        hiddenDim = 300
        #hiddenDim = 150

        TrigramLayer{} = {
            Left  = DelayLayer{T=-2}
            Right = DelayLayer{T=-1}
            apply (x) = Splice (/*Left (x) :*/ x : Right(x))
        }.apply

        BiRecurrentLSTMLayer {hiddenDim} =
        {
            L2R = RecurrentLSTMLayer {hiddenDim, goBackwards=false}
            R2L = RecurrentLSTMLayer {hiddenDim, goBackwards=true}
            apply (x) = Splice (L2R (x) : R2L (x))
        }.apply

        model = Sequential (
            TrigramLayer{} :
            EmbeddingLayer {embDim} :                                             # embedding
            BatchNormalizationLayer {normalizationTimeConstant=2048} :
            RecurrentLSTMLayer {hiddenDim, goBackwards=false} :              # LSTM
            #BiRecurrentLSTMLayer {hiddenDim} : 
            #Parallel ((RecurrentLSTMLayer {hiddenDim} : RecurrentLSTMLayer {hiddenDim, goBackwards=true}), Splice) :  # bidirectional LSTM
            #Parallel ((RecurrentLSTMLayer {hiddenDim} : RecurrentLSTMLayer {hiddenDim, goBackwards=true}), Splice) :  # bidirectional LSTM
            BatchNormalizationLayer {normalizationTimeConstant=2048} :
            DenseLayer {labelDim, initValueScale=7}        # output layer
        )

        # features
        query      = Input {inputDim}
        slotLabels = Input {labelDim}

        # model application
        z = model (query)

        # loss and metric
        ce   = CrossEntropyWithSoftmax (slotLabels, z)
        errs = ErrorPrediction         (slotLabels, z)

        featureNodes    = (query)
        labelNodes      = (slotLabels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    }
    # rename this to BrainScriptNetworkBuilder to switch to intent-classification task
    Intent_BrainScriptNetworkBuilder = {
        inputDim = $vocabSize$
        labelDim = $numIntents$
        embDim = 150
        #hiddenDim = 300
        hiddenDim = 150

        model = Sequential (
            Parallel ((DelayLayer{T=1} : Identity : DelayLayer{T=-1}), Splice) :  # 3-word window
            EmbeddingLayer {embDim} :                                             # embedding
            RecurrentLSTMLayer {hiddenDim} : BS.Sequences.Last :                  # LSTM state, final state
            #Parallel ((Sequential (RecurrentLSTMLayer {hiddenDim}                   : BS.Sequences.Last):
            #           Sequential (RecurrentLSTMLayer {hiddenDim, goBackwards=true} : BS.Sequences.First)), Splice) :  # bidirectional LSTM
            DenseLayer {labelDim, initValueScale=7}        # output layer
        )

        # features
        t = DynamicAxis{}
        query        = Input {inputDim, dynamicAxis=t}
        intentLabels = Input {labelDim}

        # model application
        z = model (query)

        # loss and metric
        ce   = CrossEntropyWithSoftmax (intentLabels, z)
        errs = ErrorPrediction         (intentLabels, z)

        featureNodes    = (query)
        labelNodes      = (intentLabels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (z)
    }

    SGD = {
        maxEpochs = 8 ; epochSize = 36000

        minibatchSize = 70

        learningRatesPerSample = 0.01*2:0.005*12:0.001
        gradUpdateType = "FSAdaGrad"
        gradientClippingWithTruncation = true ; clippingThresholdPerSample = 15.0

        firstMBsToShowResult = 10 ; numMBsToShowResult = 100

        parallelTrain = {
            parallelizationMethod = "DataParallelSGD"
            parallelizationStartEpoch = 2
            distributedMBReading = true
            dataParallelSGD = { gradientBits = 1 }
        }
    }

    reader = {
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.train.ctf"
        randomize = true
        input = {
            query        = { alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" }
            intentLabels = { alias = "S1" ; dim = $numIntents$ ; format = "sparse" }
            slotLabels   = { alias = "S2" ; dim = $numLabels$ ;  format = "sparse" }
        }
    }
}

# Test the model's accuracy (as an error count)
TestTagger = {
    action = "eval"
    modelPath = $modelPath$
    reader = {
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/atis.test.ctf"
        randomize = false
        input = {
            query        = { alias = "S0" ; dim = $vocabSize$ ;  format = "sparse" }
            intentLabels = { alias = "S1" ; dim = $numIntents$ ; format = "sparse" }
            slotLabels   = { alias = "S2" ; dim = $numLabels$ ;  format = "sparse" }
        }
    }
}
